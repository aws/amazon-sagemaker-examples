{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a485b55-4b06-4fa9-8148-0bb37603eb6a",
   "metadata": {},
   "source": [
    "# Build Predictor (XGBoost) Model\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "We demonstrate building a ML inference application to predict the rings of Abalone.\n",
    "\n",
    "After the model is hosted for inference, the payload will be sent as a raw (untransformed) csv string to a real-time endpoint.\n",
    "The raw payload is first received by the preprocessor container. The raw payload is then transformed (feature-engineering) by the preprocessor, and the transformed record (float values) are returned as a csv string by the preprocessor container.\n",
    "\n",
    "The transformed record is then passed to the predictor container (XGBoost model). The predictor then converts the transformed record into [`XGBoost DMatrix`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix) format, loads the model, calls `booster.predict(input_data)` and returns the predictions (Rings) in a JSON format.\n",
    "\n",
    "![Abalone Predictor](../images/byoc-predictor.png)\n",
    "\n",
    "We use [nginx](https://nginx.org/) as the reverse proxy, [gunicorn](https://gunicorn.org/#deployment) as the web server gateway interface and the inference code as python [\"Flask\"](https://flask.palletsprojects.com/en/2.3.x/tutorial/factory/) app.\n",
    "\n",
    "## Dataset and model\n",
    "\n",
    "For this example, we use a pre-trained [XGBoost](https://xgboost.readthedocs.io) model on [UCI Abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone).\n",
    "\n",
    "The pre-trained model accepts input in `text/csv` format and returns prediction results in `application/json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d5400-7949-4a1b-b228-bbc772c75191",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Upgrade the below packages to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b997c6-f0ea-4be5-b2a0-d3bd1b726f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli boto3 sagemaker watermark scikit-learn tqdm --quiet\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -p awscli,boto3,sagemaker,scikit-learn,tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ecbf3-e4a5-494b-92da-cc3ded85f973",
   "metadata": {},
   "source": [
    "### Inference script for a predictor (XGBoost) model\n",
    "\n",
    "- In this example, we download a pre-trained XGBoost model on the UCI abalone dataset from s3.\n",
    "- The inference code is implemented in [`code/inference.py`](./code/inference.py). The [Flask](https://flask.palletsprojects.com/) app implementation is as follows:\n",
    "  - Implement routes for `/ping` and `/invocations`\n",
    "  - Implement functions to handle preprocessing, model loading and prediction\n",
    "  - Predictions will be returned from `/invocations` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b04af-6aa0-4b53-b37c-68ace4c870ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "from pathlib import Path\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "sm_session = session.Session()\n",
    "region = sm_session._region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "prefix = \"sagemaker/abalone/models/byoc\"\n",
    "default_bucket_prefix = sm_session.default_bucket_prefix\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    prefix = f\"{default_bucket_prefix}/{prefix}\"\n",
    "\n",
    "predictor_image_name = \"abalone/predictor\"\n",
    "\n",
    "pretrained_xgboost_model_s3uri = (\n",
    "    f\"s3://sagemaker-example-files-prod-{region}/models/xgb-abalone/xgboost-model\"\n",
    ")\n",
    "\n",
    "base_dir = Path(\"../data\").resolve()\n",
    "predictor_model_dir = Path(\"./models\").absolute()\n",
    "\n",
    "if not predictor_model_dir.exists():\n",
    "    predictor_model_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9c2d1-c7f7-4523-abeb-51efc1b1c85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download pretrained xgboost model\n",
    "!aws s3 cp $pretrained_xgboost_model_s3uri $predictor_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137679d4-58bb-464d-9acd-e3df30405266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838fec2-f74a-4186-95b6-6b486afe1a7e",
   "metadata": {},
   "source": [
    "### Build and test custom inference image locally\n",
    "\n",
    " - [Dockerfile](./Dockerfile) implementation\n",
    "   - Set required LABEL `LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true`\n",
    "   - Installs required software and python packages\n",
    "   - Copies all files under code directory to `/opt/program`\n",
    "   - Sets `ENTRYPOINT` to `[\"python\"]`\n",
    "   - Sets `CMD` to `[\"serve\"]` (python script that launches nginx, gunicorn in the background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38730055-fd66-40ce-a520-dd44bcb16ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba8ab1-4e0e-41f5-ad4e-e647b2a13efc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build image locally\n",
    "!docker build -t $predictor_image_name ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd93d6-e00a-44ce-a742-79e1bc5b4076",
   "metadata": {},
   "source": [
    "### Launch and test custom Inference container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef0220-ae37-4d72-b230-a292498c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a terminal and cd into the predictor directory (the location where predictor.ipynb is located) run the following command\n",
    "# run this command to launch container locally\n",
    "# mounts the [models](./models) directory to `/opt/ml/model` directory inside the container and maps container port `8080` to host port `8080`\n",
    "# docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 abalone/predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c4680-48ab-4ce2-aaaf-168da80bc0b0",
   "metadata": {},
   "source": [
    "#### Check container health by invoking `/ping`\n",
    "\n",
    "If the `/ping` was successful you should see a response similar to `\"GET /ping HTTP/1.1\" 200 1` in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd99bed-c9a6-4194-b41f-54501aba6816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ping local inference endpoint\n",
    "# !curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893e277-20ea-40e5-86a9-e5b5809ac892",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify container logs locally (using docker logs)\n",
    "\n",
    "- To inspect a running container to view container config values or IP address we use `docker inspect <CONTAINER_ID_OR_NAME>`\n",
    "- To view and tail logs generated in the container we use `docker logs --follow <NUM_OF_LINES> <CONTAINER_ID_OR_NAME>`\n",
    "- SageMaker publishes container logs to CloudWatch. CloudWatch logs for a given endpoint are published to the following log stream path\n",
    "`/aws/sagemaker/Endpoints/ENDPOINT_NAME/VARIANT_NAME/CONTAINER_NAME`\n",
    "\n",
    "**NOTE:** \n",
    "1. Run this command in a terminal as running this inside a cell would hang execution.\n",
    "1. the below command assumes there is only one running container. If you have more, then use command with container name `docker inspect <CONTAINER_ID_OR_NAME>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292186ed-22a4-4f4f-a23c-acfb138e2ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN THE BELOW IN A SEPARATE NEW TERMINAL\n",
    "# docker ps --format \"{{.Names}}\" | xargs -n1 -I{} docker logs --follow --tail 50 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79be964-d4dd-4cff-ac62-ef57c6dd0627",
   "metadata": {},
   "source": [
    "### Troubleshooting container locally (using logs)\n",
    "\n",
    "`!docker logs abalone/featurizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902de0e4-03da-4592-82c0-461d1ee17b51",
   "metadata": {},
   "source": [
    "#### Test records for inference\n",
    "\n",
    "Grab a test record from [abalone_test_predictor.csv](../featurizer/data/abalone_test_predictor.csv), generated by [`featurizer.ipynb`](../featurizer/featurizer.ipynb), format it as a CSV record, and send it as raw data to the endpoint `http://localhost:8080/invocations` path\n",
    "\n",
    "Uncomment below cells to test inference on local container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365d9de-51fe-46dc-9e5e-a82b05685bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "# !curl --data-raw '-1.3317586042173168,-1.1425409076053987,-1.0579488602777858,-1.177706547272754,-1.130662184748842,-1.1493955859050584,-1.139968767909096,0.0,1.0,0.0' \\\n",
    "# -H 'Content-Type: text/csv; charset=utf-8' \\\n",
    "# -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86286333-a056-4619-be76-2b65d492cd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "# !curl --data-raw '0.7995425613971686,0.877965470587042,1.326659055767273,1.398563012556441,0.9896192483949702,1.509166873607132,2.01650402614155,0.0,0.0,1.0' \\\n",
    "# -H 'Content-Type: text/csv; charset=utf-8' \\\n",
    "# -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad32979-3897-42e7-94f4-8537e12c720a",
   "metadata": {},
   "source": [
    "### Tag and push the local image to private ECR\n",
    "\n",
    "- Tag the `abalone/predictor` local image to `{account_id}.dkr.ecr.{region}.amazonaws.com/{imagename}:{tag}` format\n",
    "- Run [./build_n_push.sh](./build_n_push.sh) shell script with image name `nginx` as parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863efc3-4dfd-4ecc-8206-39f7b51ea690",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./build_n_push.sh\n",
    "\n",
    "!./build_n_push.sh abalone/predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf68801-7562-49b2-a9a1-c435ce2bf093",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Test predictor inference image by deploying to a real-time endpoint\n",
    "\n",
    "- **Step 1:** Compress your model in `./models/xgboost-model` to `model.tar.gz` format and upload to s3\n",
    "- **Step 2:** Create Model object with your custom inference image and deploy model to endpoint\n",
    "- **Step 3:** Send test inference request to deployed endpoint\n",
    "- **Step 4:** Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81d80-32e8-4f47-8bda-46946544c2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "os.chdir(predictor_model_dir)\n",
    "\n",
    "model_s3uri = s3_path_join(f\"s3://{bucket}/{prefix}\", \"predictor\")\n",
    "\n",
    "predictor_model_path = predictor_model_dir.joinpath(\"model.tar.gz\")\n",
    "\n",
    "if predictor_model_path.exists():\n",
    "    predictor_model_path.unlink()\n",
    "\n",
    "# SageMaker expects model artifacts to be compressed to `model.tar.gz`\n",
    "tar_cmd = \"tar -czvf model.tar.gz xgboost-model ../code/\"\n",
    "result = subprocess.run(tar_cmd, shell=True, capture_output=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"{predictor_model_path} archive created successfully!\")\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    os.chdir(predictor_model_dir)\n",
    "    print(\"An error occurred:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e751f1-97c1-401f-a1b6-717dc5d0824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload compressed model artifact to S3 using S3Uploader utility class\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=predictor_model_path.absolute(),\n",
    "    desired_s3_uri=model_s3uri,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "print(f\"Uploaded predictor model.tar.gz to {model_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f555547-7107-45b2-9a9b-abab4390142b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Step 2:** Create and deploy model with predictor inference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbb51f-d719-4ada-a507-0fbc2ef84a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from sagemaker.model import Model\n",
    "\n",
    "ecr_image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{predictor_image_name}:latest\"\n",
    "\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "\n",
    "model_name = f\"AbaloneXGB-predictor-{suffix}\"\n",
    "predictor_model = Model(\n",
    "    image_uri=ecr_image,\n",
    "    name=model_name,\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "endpoint_name = f\"Abalone-nginx-ep-{suffix}\"\n",
    "print(f\"Deploying model {model_name} to endpoint: {endpoint_name}\")\n",
    "predictor = predictor_model.deploy(\n",
    "    endpoint_name=endpoint_name, initial_instance_count=1, instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb816a27-6cb2-46d0-a77b-5433949a95bf",
   "metadata": {},
   "source": [
    "### **Step 3:** Send test inference requests to deployed endpoint\n",
    "\n",
    "We use sample transformed records from featurizer [abalone_featurizer_predictions.csv](./abalone_featurizer_predictions.csv) for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bff209-0303-44d8-a6ac-75c024ccfe28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "test_records = Path(\"./abalone_featurizer_predictions.csv\")\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sm_session)\n",
    "predictor.content_type = \"text/csv; charset=utf-8\"\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.accept = \"application/json\"\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "# Send 50 records for inference\n",
    "limit = 10\n",
    "i = 0\n",
    "\n",
    "with open(test_records, \"r\") as _f:\n",
    "    lines = _f.readlines()\n",
    "    for row in lines:\n",
    "        if i <= limit:\n",
    "            prediction = None\n",
    "            print(f\"input: {row}\")\n",
    "            try:\n",
    "                response = predictor.predict(row)\n",
    "                print(response)\n",
    "                i += 1\n",
    "                sleep(0.15)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a703e58-8f8f-4cf5-af43-ac6ee5774f16",
   "metadata": {},
   "source": [
    "### View logs emitted by the endpoint in CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28c6c2-c7d1-41d2-aac4-27a84dc9e449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "logs_client = boto3.client(\"logs\")\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(minutes=15)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "log_stream_name = log_streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "\n",
    "# Retrieve the logs\n",
    "logs = logs_client.get_log_events(\n",
    "    logGroupName=log_group_name,\n",
    "    logStreamName=log_stream_name,\n",
    "    startTime=int(start_time.timestamp() * 1000),\n",
    "    endTime=int(end_time.timestamp() * 1000),\n",
    ")\n",
    "\n",
    "# Print the logs\n",
    "for event in logs[\"events\"]:\n",
    "    print(f\"{datetime.fromtimestamp(event['timestamp'] // 1000)}: {event['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083067a1-e209-4448-8ed2-99c91db20d74",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Cleanup resources. Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b70a7b-be35-4fe5-99b2-8cf6c6d57d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete model\n",
    "try:\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting Model: {model_name}\\n{e}\")\n",
    "    pass\n",
    "\n",
    "# Delete endpoint\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting EP: {endpoint_name}\\n{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00ead3-970d-4b1a-9fc8-3a464842f615",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
