{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Model Inference Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "This notebook is a **blueprint** for real-time [large model inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference.html) testing in SageMaker. This notebook uses [Deep Java Library (djl) Large Model Inference (LMI) engines](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html) to deploy the models in SageMaker.\n",
    "\n",
    "This notebook is driven by [LMI configuration YAML files](#lmi-configuration-file). The `test` object in LMI configuration file defines the testing module interface. The testing module provides a prompt generator class that yields prompts for synchronous request-response tests. The prompt used in each test, the generated text, the request latency (secs), the number of output tokens, and tokens per second are recorded in a multi-line JSON output file. \n",
    "\n",
    "When comparing latency across different instance types, keep in mind that the LMI engines maybe different. Ensure consistency in LMI engine configurations, as best as possible, so that latency comparison is meaningful. Inconsistencies among LMI configurations can render comparisons across instance types largely meaningless. Different LMI engines will produce different results for the same model and prompt. Therefore, in addition to comparing latency, compare tokens-per-second.\n",
    "\n",
    "In addition to baseline single request-response latency tesitng, this notebook supports throughput testing using open-source [Locust](https://locust.io/).  \n",
    "\n",
    "\n",
    "## Hardware Requirements for Running Notebook\n",
    "\n",
    "Since we are working with large model inference, we will need to download HuggingFace model snapshots, and upload them to S3 bucket. Configure at least 1000 GB volume with the SageMaker Notebook instance. For SageMaker notebook instance type, `ml.m5.2xlarge`, or larger, is recommended..\n",
    "\n",
    "## Manually Validate Model Outputs\n",
    "\n",
    "After deploying a new model, do a trial run and manually validate that generated text is not gibberish. If the model is producing gibberish, try deleting and redeploying the model.\n",
    "\n",
    "## LMI Configuration File\n",
    "\n",
    "To deploy a model, you need to define a LMI configuration (YAML) file. Below, we show an example LMI configuration file:\n",
    "\n",
    "```\n",
    "huggingface:\n",
    "  model: \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "  revision: \"41b61a33a2483885c981aa79e0df6b32407ed873\"\n",
    "  download: true\n",
    "djl:\n",
    "  engine: \"MPI\"\n",
    "  option.entryPoint: \"djl_python.deepspeed\"\n",
    "  option.tensor_parallel_degree: 4\n",
    "  option.model_loading_timeout: 1800\n",
    "  option.dtype: \"fp16\"\n",
    "  option.max_tokens: 2048\n",
    "  option.task: \"text-generation\"\n",
    "sagemaker:\n",
    "  model:\n",
    "    name: \"mistral-7b-instruct-v0-2-deepspeed\"\n",
    "    container: \"containers/deepspeed\"\n",
    "    env: \n",
    "      HUGGINGFACE_HUB_CACHE: \"/tmp\"\n",
    "      TRANSFORMERS_CACHE: \"/tmp\"\n",
    "  endpoint:\n",
    "    name: \"mistral-7b-instruct-v0-2-deepspeed\"\n",
    "    instance_type: \"ml.g5.12xlarge\"\n",
    "    initial_instance_count: 1\n",
    "    variant_name: \"test\"\n",
    "    model_data_download_timeout_secs: 1800\n",
    "    container_startup_health_check_timeout_secs: 1200\n",
    "test:\n",
    "  module_name: \"prompt_generator\"\n",
    "  module_dir: \"modules/inst-semeval2017\"\n",
    "  prompt_generator: \"PromptGenerator\"\n",
    "  params: { \"do_sample\": true, \"max_new_tokens\": 1024, \"top_k\": 50 }\n",
    "  warmup_iters: 1\n",
    "  max_iters: 10\n",
    "  output_dir: \"output/mistral-7b-instruct-v0.2/deepspeed\"\n",
    "```\n",
    "\n",
    "Below, we provide a brief explanation for the LMI configuration file:\n",
    "\n",
    "* The `huggingface` object is optional: \n",
    "    * If you specify `huggingface` object, the `huggingface.name` field is required\n",
    "    * The `huggingface.revision` is required if `huggingface.download` is `true`.\n",
    "    * if `huggingface` is specified, `djl` object must be specified\n",
    "* The `djl` object is optional and if specified contains the content for [DJL serving.properties](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) for the selected [LMI engine](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/index.html)\n",
    "    * If you specify `huggingface` object, `djl.option\\.model_id` is computed automatically.\n",
    "* The `sagemaker.model` object is required \n",
    "    * The field `sagemaker.model.image` is optional: Alternatively, you can specify `sagemaker.model.container`  as relative path (w.r.t. to this notebook) to the container build script directory.\n",
    "    * The field `sagemaker.model.env` is optional\n",
    "* The `sagemaker.endpoint` object is required\n",
    "    * In `sagemaker.endpoint` object, only `name` and `instance_type` are required.\n",
    "* The `test` object is optional, and defines the test interface \n",
    "    * The `test.module_dir` is relative path. This path is added to `sys.path`. If there is a `requirements.txt` in this directory, it is installed.\n",
    "    * The `test.module_name` must be a Python module in `test.module_dir`. This module is dynamically loaded.\n",
    "    * The `test.prompt_generator` is the name of a class in the `test.module_name` module. An object of this class is dyamically created, and the `__call__` method on the object is called to get the prompt generator for testing.\n",
    "    * The `test.warmpup_iters` are used to warmup the deployed inference model.\n",
    "    * The `test.max_iters` limits the number of prompt requests, not including the `test.warmup_iters`.\n",
    "    * The `test.output_dir` is the relative path where `results.json` testing output file is written. \n",
    "    * Each line in `results-*.json` file is a json object with following fields: request `prompt`, request output `text`, and request `latency` in seconds.\n",
    "### Custom Model Handler Code\n",
    "\n",
    "You can optionally add a [custom model handler](https://github.com/deepjavalibrary/djl-serving/blob/bc7fdfdcbb66b982522e6bc809b0044fabde69e0/serving/docs/streaming_config.md#custom-modelpy-handler) in the `code` sub-folder, collocated with the LMI configuration file, and it is added to the SageMaker model package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session\n",
    "\n",
    "Let us specify the `s3_bucket` and `s3_prefix` that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "s3_bucket  =  None # specify bucket, or use default sagemaker bucket, if it exists\n",
    "s3_prefix = 'lmi-djl' # Large model inference with deep java library\n",
    "\n",
    "role = get_execution_role() # you may provide a pre-existing role ARN here\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "try:\n",
    "    if s3_bucket is None:\n",
    "        s3_bucket = sagemaker_session.default_bucket()\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_bucket_location(Bucket=s3_bucket)\n",
    "    bucket_region = response['LocationConstraint']\n",
    "    bucket_region = 'us-east-1' if bucket_region is None else bucket_region\n",
    "    \n",
    "    print(f\"Bucket region: {bucket_region}\")\n",
    "    \n",
    "    try:\n",
    "        s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "    except:\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "\n",
    "    print(f\"Using S3 folder: s3://{s3_bucket}/{s3_prefix}/ in this notebook\")\n",
    "except:\n",
    "    print(f\"Access Error: Check if '{s3_bucket}' S3 bucket is in '{aws_region}' region, and {s3_prefix} path exists\")\n",
    "\n",
    "sts = boto3.client(\"sts\")\n",
    "aws_account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"AWS Account Id: {aws_account_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify SageMaker LMI configuration\n",
    "\n",
    "We specify a *SageMaker LMI configuration* YAML file for the model we wish to deploy. For example, we specify [vLLM LMI configuration file for mistral-7b-instruct-v0.2](./examples/vllm/mistral-7b-instruct-v0.2/config.yaml), below. You can specify whatever example file you wish to deploy.\n",
    "\n",
    "**Tip:** \n",
    "You may wish to make copies of this notebook, if you would like to work with multiple examples concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "\n",
    "import pathlib\n",
    "print(f\"Current working directory: {pathlib.Path().resolve()}\")\n",
    "\n",
    "config_path=\"examples/vllm/mistral-7b-instruct-v0.2/config.yaml\"\n",
    "with open(config_path, \"r\") as mf:\n",
    "    model_config=yaml.safe_load(mf)\n",
    "\n",
    "print(\"\\nmodel_config:\\n\")\n",
    "print(json.dumps(model_config, indent=2))\n",
    "\n",
    "assert model_config.get('huggingface', None) is None \\\n",
    "    or model_config.get('djl', None) is not None, \"'djl' must be specified if 'huggingface' is specified\"\n",
    "\n",
    "assert model_config.get('sagemaker', None), \"'sagemaker' object is required\"\n",
    "assert model_config['sagemaker'].get('model',None), \"'sagemaker.model' is required\"\n",
    "assert model_config['sagemaker'].get('endpoint', None), \"'sagemaker.endpoint' is required\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Build and Push Model Inference Container Image to ECR\n",
    "\n",
    "Next, if `sagemaker.model.container` is specified, we build and push the container image to Amazon ECR. After building and pushing the image to ECR, we update the `sagemaker.model.image` with the ECR URI for the image. Either `container` or `image` must be specified in `sagemaker.model`.\n",
    "\n",
    "**Tip:** After building and pushing the container image to ECR, you may want to update `sagemaker.model.image` field in the LMI configuration file with the ECR URI, and delete the image from your local docker repository. This will free up space to build additional docker images, or you are likely to run of of local disk space. Docker, by default, uses the root partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, subprocess, stat, re\n",
    "\n",
    "sm_model_config = model_config['sagemaker']['model']\n",
    "container_path = sm_model_config.get(\"container\", None)\n",
    "\n",
    "if container_path is not None:\n",
    "    with open(os.path.join(container_path, \"build.log\"), \"w\") as logfile:\n",
    "        print(f\"Building and pushing {container_path} to ECR; see log file: {container_path}/build.log\")\n",
    "        container_build_script = os.path.join(container_path, \"build_tools\", \"build_and_push.sh\")\n",
    "\n",
    "        st = os.stat(container_build_script)\n",
    "        os.chmod(container_build_script, st.st_mode | stat.S_IEXEC)\n",
    "        subprocess.check_call([container_build_script, aws_region], stdout=logfile, stderr=subprocess.STDOUT)\n",
    "\n",
    "        image_tag = !cat {container_path}/build_tools/set_env.sh \\\n",
    "            | grep 'IMAGE_TAG' | sed 's/.*IMAGE_TAG=\\(.*\\)/\\1/'\n",
    "\n",
    "        image_name = !cat {container_path}/build_tools/set_env.sh \\\n",
    "            | grep 'IMAGE_NAME' | sed 's/.*IMAGE_NAME=\\(.*\\)/\\1/'\n",
    "\n",
    "        ecr_image_uri=f\"{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com/{image_name[0]}:{image_tag[0]}\"\n",
    "\n",
    "        sm_model_config['image'] = ecr_image_uri\n",
    "else:\n",
    "    ecr_image_uri=sm_model_config.get('image', None)\n",
    "    assert ecr_image_uri is not None, \"'sagemaker.model.image' or 'sagemaker.model.container' is required\"\n",
    "    pattern=\"\\.dkr\\.ecr\\.[a-z0-9-]+\\.\"\n",
    "    replace=f\".dkr.ecr.{aws_region}.\"\n",
    "    sm_model_config['image']=re.sub(pattern,replace,ecr_image_uri)\n",
    "\n",
    "print(f\"Model serving image: {sm_model_config['image']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Download HuggingFace Model Snapshot\n",
    "\n",
    "If `huggingface` object is specified, only `huggingface.model` is required. If `huggingface.download` is `true`, the HuggingFace model snapshot with revision `huggingface.revision` is downloaded and uploaded to the `s3_bucket`, and `djl.option\\.model_id` is set to the S3 URI of the uploaded HuggingFace model snapshot. \n",
    "\n",
    "**Note: You must specify `hf_token` below if the Hugging Face model requires a Hugging Face token for downloading the model from the HuggingFace hub. This is true whether or not you specify the `huggingface` object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_token = None # Specify HuggingFace token, if required to access model\n",
    "hf_spec = model_config.get('huggingface', None)\n",
    "\n",
    "if hf_spec:\n",
    "    hf_model = hf_spec.get('model', None)\n",
    "    download = hf_spec.get('download', None)\n",
    "    if download:\n",
    "        revision = hf_spec.get('revision', None)\n",
    "        assert revision, \"'huggingface.revision' is required if 'download' is 'true'\"\n",
    "        \n",
    "        s3_model_prefix = f\"{s3_prefix}/huggingface/models/{hf_model}/{revision}\"  # folder where model checkpoint will go\n",
    "        print(f\"s3_model_prefix: {s3_model_prefix}\")\n",
    "\n",
    "        try:\n",
    "            s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_model_prefix}/config.json\")\n",
    "            print(f\"Skipping download; HuggingFace model already exists at s3://{s3_bucket}/{s3_model_prefix}/\")\n",
    "        except:\n",
    "            subprocess.check_output(f\"pip install huggingface-hub\", shell=True, stderr=subprocess.STDOUT)\n",
    "            from huggingface_hub import snapshot_download\n",
    "            from tempfile import TemporaryDirectory\n",
    "            from pathlib import Path\n",
    "\n",
    "            print(f\"Downloading HuggingFace model snapshot: {hf_model}, revision: {revision}\")\n",
    "            with TemporaryDirectory(suffix=\"model\", prefix=\"hf\", dir=\".\") as cache_dir:\n",
    "                ignore_patterns = [\"*.msgpack\", \"*.h5\"]\n",
    "                snapshot_download(repo_id=hf_model, \n",
    "                    revision=revision, \n",
    "                    cache_dir=cache_dir,\n",
    "                    ignore_patterns=ignore_patterns,\n",
    "                    token=hf_token)\n",
    "\n",
    "                local_model_path = Path(cache_dir)\n",
    "                model_snapshot_path = str(list(local_model_path.glob(f\"**/snapshots/{revision}\"))[0])\n",
    "                print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "\n",
    "                for root, dirs, files in os.walk(model_snapshot_path):\n",
    "                    for file in files:\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        with open(full_path, 'rb') as data:\n",
    "                            key = f\"{s3_model_prefix}/{full_path[len(model_snapshot_path)+1:]}\"\n",
    "                            s3_client.upload_fileobj(data, s3_bucket, key)\n",
    "\n",
    "        model_s3_url = f\"s3://{s3_bucket}/{s3_model_prefix}/\"\n",
    "        model_config['djl']['option.model_id'] = model_s3_url\n",
    "    else:\n",
    "        model_config['djl']['option.model_id'] = hf_model\n",
    "    \n",
    "    print(json.dumps(model_config, indent=2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe Create DJL Model Package\n",
    "\n",
    "Next, we create the model package TAR ball, if `djl` is specified, and upload it to `s3_bucket`. The model package will be used to define the SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory, NamedTemporaryFile\n",
    "from pathlib import Path\n",
    "import glob \n",
    "import shutil\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "sm_model_name = sm_model_config.get('name', None)\n",
    "assert sm_model_name, \"'sagemaker.model.name' is required\"\n",
    "djl_spec = model_config.get('djl', None)\n",
    "\n",
    "model_pkg_key = None\n",
    "if djl_spec is not None:\n",
    "    with TemporaryDirectory(suffix=\"pkg\", prefix=\"model\", dir=\".\") as pkg_dir:\n",
    "\n",
    "        with open(os.path.join(pkg_dir, \"serving.properties\"), \"w\") as props_file:\n",
    "            for key, value in djl_spec.items():\n",
    "                props_file.write(f\"{key}={value}\\n\")\n",
    "\n",
    "        code_dir = os.path.join(os.path.dirname(config_path), \"code\")\n",
    "        if os.path.isdir(code_dir):\n",
    "            files = glob.glob(f\"{code_dir}/*\")\n",
    "\n",
    "            for file in files:\n",
    "                if os.path.isdir(file):\n",
    "                    shutil.copytree(file, os.path.join(pkg_dir, os.path.basename(file)))\n",
    "                else:\n",
    "                    shutil.copy2(file, pkg_dir)\n",
    "\n",
    "        with NamedTemporaryFile(prefix=\"model\", suffix=\".gz\") as gz_file:\n",
    "            with tarfile.open(gz_file.name, \"w:gz\") as tar:\n",
    "                tar.add(pkg_dir, arcname=\"\")\n",
    "\n",
    "            gz_file.seek(0)\n",
    "            model_pkg_key = f\"{s3_prefix}/sagemaker/code/{sm_model_name}/model.tar.gz\"\n",
    "            print(f\"Upload model package to s3://{s3_bucket}/{model_pkg_key}\")\n",
    "            s3_client.upload_fileobj(gz_file, s3_bucket, model_pkg_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Next, we create the SageMaker model using the model package we just uploaded tp `s3_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sm_model_image = sm_model_config.get(\"image\", None)\n",
    "assert sm_model_image, \"'sagemaker.model.image' is required\"\n",
    "\n",
    "sm_model_env = sm_model_config.get(\"env\", {})\n",
    "if sm_model_env.get(\"HF_MODEL_ID\", None) is not None and hf_token is not None:\n",
    "    sm_model_env['HF_TOKEN']=hf_token\n",
    "\n",
    "primary_container={\"Image\": sm_model_image,\"Environment\": sm_model_env}\n",
    "if model_pkg_key is not None:\n",
    "    primary_container[\"ModelDataUrl\"]=f\"s3://{s3_bucket}/{model_pkg_key}\"\n",
    "    \n",
    "try:\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName=sm_model_name,\n",
    "        ExecutionRoleArn=role,\n",
    "        PrimaryContainer=primary_container,\n",
    "    )\n",
    "    model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "    print(f\"Created Model: {model_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint Config\n",
    "\n",
    "Next we create endpoint config for the SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_endpoint_spec = model_config['sagemaker']['endpoint']\n",
    "endpoint_name = sm_endpoint_spec.get('name', None)\n",
    "assert endpoint_name, \"'sagemaker.endpoint.name' is required\"\n",
    "\n",
    "variant_name = sm_endpoint_spec.get('variant_name', 'test')\n",
    "\n",
    "instance_type=sm_endpoint_spec.get('instance_type', None)\n",
    "assert instance_type, \"'sagemaker.endpoint.instance_type' is required\"\n",
    "\n",
    "initial_instance_count=sm_endpoint_spec.get('initial_instance_count', 1)\n",
    "model_data_download_timeout_secs = sm_endpoint_spec.get('model_data_download_timeout_secs', 1200)\n",
    "container_startup_health_check_timeout_secs=sm_endpoint_spec.get('container_startup_health_check_timeout_secs', 1200)\n",
    "\n",
    "production_variant = {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": initial_instance_count,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_secs,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_secs,\n",
    "        }\n",
    "\n",
    "volume_size_gb = sm_endpoint_spec.get('volume_size_gb', None)\n",
    "if volume_size_gb:\n",
    "    production_variant['VolumeSizeInGB'] = volume_size_gb\n",
    "\n",
    "try:\n",
    "    endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_name,\n",
    "        ProductionVariants=[production_variant]\n",
    "    )\n",
    "    print(endpoint_config_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating endpoint config: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint\n",
    "\n",
    "Next, we create the SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, \n",
    "                                                     EndpointConfigName=endpoint_name)\n",
    "    print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating endpoint: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Testing\n",
    "\n",
    "Now, we are ready to run our tests, using the test interface. During this step, we install HuggingFace  `transformers` library, download model tokenizer configuration files, and create a tokenizer. Next, we load the test interface Python module, create a prompt generator class object, and use it to drive our test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "from test_task import test_task\n",
    "\n",
    "sm_runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "print(\"installing transformers package\")\n",
    "subprocess.check_output(f\"pip install transformers\", shell=True, stderr=subprocess.STDOUT)\n",
    "\n",
    "test_spec = model_config.get('test', None)\n",
    "if test_spec:\n",
    "    sm_endpoint_spec = model_config['sagemaker']['endpoint']\n",
    "    endpoint_name = sm_endpoint_spec.get('name', None)\n",
    "    assert endpoint_name, \"'sagemaker.endpoint.name' is required\"\n",
    "\n",
    "    instance_type=sm_endpoint_spec.get('instance_type', None)\n",
    "    assert instance_type, \"'sagemaker.endpoint.instance_type' is required\"\n",
    "\n",
    "    ts = strftime(\"%Y-%m-%d-%H-%M-%S-GMT\", gmtime())\n",
    "    output_dir = test_spec.get('output_dir', None)\n",
    "    assert output_dir, \"'test.output_dir' is required\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results_path = os.path.join(output_dir, f\"results-{instance_type}-{ts}.json\")\n",
    "\n",
    "    output_formatter = djl_spec.get(\"option.output_formatter\", None) if djl_spec else None\n",
    "    rolling_batch = djl_spec.get(\"option.rolling_batch\", None) if djl_spec else None\n",
    "    streaming_enabled = rolling_batch in [ \"auto\", \"deepspeed\", \"trtllm\" ] and output_formatter in [ \"jsonlines\"]                         \n",
    "    print(f\"Streaming enabled: {streaming_enabled}\")\n",
    "\n",
    "    test_task(s3_client=s3_client, \n",
    "              sm_runtime_client=sm_runtime_client, \n",
    "              model_id=djl_spec.get('option.model_id') if djl_spec else sm_model_env.get('HF_MODEL_ID'),\n",
    "              test_spec=test_spec,\n",
    "              endpoint_name=endpoint_name,\n",
    "              results_path=results_path,\n",
    "              streaming_enabled=streaming_enabled,\n",
    "              hf_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install locust\n",
    "!which locust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locust Testing\n",
    "\n",
    "If the configuration file includes `test` object, we can do [Locust](https://locust.io/) based throughput testing, as shown below. Next cell will run until testing is complete. The duration of the testing is specified in  `os.environ[\"RUN_TIME\"]` below, which by default is 2 minutes. The `os.environ[\"SPAWN_RATE\"]` specifies ramp up rate for the Locust Users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if test_spec:\n",
    "    params = test_spec.get(\"params\", None)\n",
    "    os.environ[\"STREAMING_ENABLED\"] = str(streaming_enabled)\n",
    "    os.environ[\"PROMPT_MODULE_DIR\"] = test_spec.get('module_dir', None)\n",
    "    os.environ[\"PROMPT_MODULE_NAME\"] = test_spec.get('module_name', None)\n",
    "    os.environ[\"PROMPT_GENERATOR_NAME\"] = test_spec.get('prompt_generator', None)\n",
    "    os.environ[\"CONTENT_TYPE\"]=\"application/json\"\n",
    "    os.environ[\"MODEL_PARAMS\"] = json.dumps(params)\n",
    "    os.environ[\"ENDPOINT_NAME\"] = f\"https://runtime.sagemaker.{aws_region}.amazonaws.com/endpoints/{endpoint_name}/invocations\"\n",
    "    os.environ[\"USERS\"]=\"4\"\n",
    "    os.environ[\"WORKERS\"]=\"4\"\n",
    "    os.environ[\"RUN_TIME\"]=\"2m\"\n",
    "    os.environ[\"SPAWN_RATE\"]=\"5\"\n",
    "    os.environ[\"SCRIPT\"]=\"endpoint_user.py\"\n",
    "    os.environ[\"RESULTS_PREFIX\"]=f\"{output_dir}/locust_results_{instance_type}_{ts}\"\n",
    "    os.environ[\"TASK_NAME\"]=test_spec.get(\"task_name\", \"text-generation\")\n",
    "    os.environ[\"INPUT_TYPE\"]=test_spec.get(\"input_type\", \"list\")\n",
    "    \n",
    "    try:\n",
    "        with open(\"run_locust.log\", \"w\") as logfile:\n",
    "            print(f\"Start Locust testing; logfile: run_locust.log; results: {output_dir}\")\n",
    "            path = os.path.join(os.getcwd(), \"run_locust.sh\")\n",
    "            os.chmod(path, stat.S_IRUSR | stat.S_IEXEC)\n",
    "            process = subprocess.Popen(path, encoding=\"utf-8\", \n",
    "                                   shell=True,stdout=logfile,stderr=subprocess.STDOUT)\n",
    "            process.wait()\n",
    "            logfile.flush()\n",
    "            print(f\"Locust testing completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Locust Results\n",
    "\n",
    "Below we visualize the results of the Locust testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "if test_spec:\n",
    "    results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "    df = pd.read_csv(results_path)\n",
    "    df = df.replace(np.nan, '')\n",
    "    \n",
    "    top_n = 2\n",
    "    caption=f\"Endpoint: {endpoint_name}\".upper()\n",
    "    df = df.truncate(after=top_n - 1, axis=0)\n",
    "    df = df.style \\\n",
    "          .format(precision=6) \\\n",
    "            .set_properties(**{'text-align': 'left'}) \\\n",
    "            .set_caption(caption)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "This concludes the notebook. Below. we delete the **deployed** SageMaker endpoint, endpoint configuration, and the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Delete Endpoint response: {response}\")\n",
    "\n",
    "response = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "print(f\"Delete Endpoint Config response: {response}\")\n",
    "\n",
    "response = sm_client.delete_model(ModelName=sm_model_name)\n",
    "print(f\"Delete Model response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
