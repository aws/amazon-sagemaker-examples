{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Amazon Sagemaker Distributed Model Parallel to Launch a BERT Training Job with Model Parallelization\n",
    "\n",
    "SMP (Sagemaker Distributed Model Parallel) is a model parallelism library for training large deep learning models that were previously difficult to train due to GPU memory limitations. SMP automatically and efficiently splits a model across multiple GPUs and instances and coordinates model training, allowing you to increase prediction accuracy by creating larger models with more parameters.\n",
    "\n",
    "Use this notebook to configure SMP to train a model using PyTorch (version 1.6.0) and the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk).\n",
    "\n",
    "In this notebook, you will use a BERT example training script with SMP.\n",
    "The example script is based on [Nvidia Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT) and will require you to download the datasets and upload to s3 as provided in the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to understand how SageMaker uses Docker to train custom models.\n",
    "* To learn more about using Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "* To learn more about using Docker to train your own models with Amazon SageMaker, see [Example Notebooks: Use Your Own Algorithm or Model](https://docs.aws.amazon.com/sagemaker/latest/dg/adv-bring-own-examples.html).\n",
    "* To see other examples of distributed training using Amazon SageMaker and Pytorch, see [Distributed TensorFlow training using Amazon SageMaker\n",
    "](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/distributed_tensorflow_mask_rcnn).\n",
    "\n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "* A S3 bucket to store the input data to be used for training.\n",
    "* The input data you use for training must be in an Amazon S3 bucket in the same AWS Region as this notebook instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Initialize the notebook instance. Get the aws region, sagemaker execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare/Identify your Training Data in Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have the BERT dataset in a S3 bucket, please see the instructions in [Nvidia BERT Example](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md) to download the dataset and upload it to a s3 bucket. \n",
    "\n",
    "Uncomment and use the following cell to specify the Amazon S3 bucket and prefix that contains your training data. For example, if your training data is in s3://your-bucket/training, enter your-bucket for s3_bucket and training for prefix. Note that your output data will be stored in the same bucket, under the \"output\" prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = '<ADD BUCKET>'\n",
    "#prefix = '<ADD PREFIX>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Data Channels\n",
    "\n",
    "In this step, you define Amazon SageMaker training data channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3train = f's3://{s3_bucket}/{prefix}'\n",
    "train = sagemaker.session.TrainingInput(s3train, distribution='FullyReplicated', \n",
    "                                        s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required: Set your output data path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f's3://{s3_bucket}/output'\n",
    "print(f'your output data will be stored in: s3://{s3_bucket}/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Training Job\n",
    "\n",
    "Next, you will use SageMaker Estimator API to define a SageMaker Training Job. You will use a [`PyTorchEstimator`](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html) to define the number and type of EC2 instances Amazon SageMaker uses for training, as well as the size of the volume attached to those instances. \n",
    "\n",
    "You must update the following:\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "\n",
    "See the following sub-sections for more details. \n",
    "\n",
    "### Update the Type and Number of EC2 Instances Used\n",
    "\n",
    "The instance type and number of instances you specify in `instance_type` and `instance_count` respectively will determine the number of GPUs Amazon SageMaker uses during training. Explicitly, `instance_type` will determine the number of GPUs on a single instance and that number will be multiplied by `instance_count`. \n",
    "\n",
    "You must specify values for `instance_type` and `instance_count` so that the total number of GPUs available for training is equal to `partitions` in `config` of `smp.init` in your training script. \n",
    "\n",
    "If you set ddp to `True`, you must ensure that the total number of GPUs available is divisible by `partitions`. The result of the division is inferred to be the number of model replicas to be used for Horovod (data parallelism degree). \n",
    "\n",
    "See [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for SageMaker supported instances and cost information. To look up GPUs for each instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that an ml.p3.2xlarge has the same number of GPUs as an p3.2xlarge.\n",
    "\n",
    "### Update your Volume Size\n",
    "\n",
    "The volume size you specify in `volume_size` must be larger than your input data size.\n",
    "\n",
    "### Set your parameters dictionary for SMP and set custom mpioptions\n",
    "\n",
    "With the parameters dictionary you can configure: the number of microbatches, number of partitions, whether to use data parallelism with ddp, the pipelining strategy, the placement strategy and other BERT specific hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpioptions = \"-verbose --mca orte_base_help_aggregate 0 \"\n",
    "mpioptions += \"--mca btl_vader_single_copy_mechanism none\"\n",
    "parameters = {\"optimize\": \"speed\", \"microbatches\": 12, \"partitions\": 2, \"ddp\": True, \"pipeline\": \"interleaved\", \"overlapping_allreduce\": True, \"placement_strategy\": \"cluster\", \"memory_weight\": 0.3}\n",
    "timeout = 60 * 60\n",
    "metric_definitions = [{\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}]\n",
    "\n",
    "hyperparameters = {\"input_dir\": \"/opt/ml/input/data/train\",\n",
    "                   \"output_dir\": \"./checkpoints\", \n",
    "                   \"config_file\": \"bert_config.json\", \n",
    "                   \"bert_model\": \"bert-large-uncased\", \n",
    "                   \"train_batch_size\": 48, \n",
    "                   \"max_seq_length\": 128,\n",
    "                   \"max_predictions_per_seq\": 20,\n",
    "                   \"max_steps\": 7038,\n",
    "                   \"warmup_proportion\": 0.2843,\n",
    "                   \"num_steps_per_checkpoint\": 200,\n",
    "                   \"learning_rate\": 6e-3,\n",
    "                   \"seed\": 12439,\n",
    "                   \"steps_this_run\": 500,\n",
    "                   \"allreduce_post_accumulation\": 1,\n",
    "                   \"allreduce_post_accumulation_fp16\": 1,\n",
    "                   \"do_train\": 1,\n",
    "                   \"use_sequential\": 1,\n",
    "                   \"skip_checkpoint\": 1,\n",
    "                   \"smp\": 1,\n",
    "                   \"apply_optimizer\": 1,\n",
    "                   \"json-summary\": \"./dllogger.json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Pytorch Estimator with SMP enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\"sagemaker_smp_pretrain.py\",\n",
    "                            role=role,\n",
    "                            instance_type=\"ml.p3.16xlarge\",\n",
    "                            volume_size=200,\n",
    "                            instance_count=1,\n",
    "                            sagemaker_session=sagemaker_session,\n",
    "                            py_version=\"py3\",\n",
    "                            framework_version='1.6.0',\n",
    "                            distribution={\n",
    "                                \"smdistributed\": {\n",
    "                                    \"modelparallel\": {\n",
    "                                        \"enabled\": True,\n",
    "                                        \"parameters\": parameters\n",
    "                                    }\n",
    "                                },\n",
    "                                \"mpi\": {\n",
    "                                    \"enabled\": True,\n",
    "                                    \"process_per_host\": 8,\n",
    "                                    \"custom_mpi_options\": mpioptions,\n",
    "                                }\n",
    "                            },\n",
    "                            source_dir='bert_example',\n",
    "                            output_path=s3_output_location,\n",
    "                            max_run=timeout,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            metric_definitions=metric_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will use the estimator to launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator.fit(data_channels, logs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

