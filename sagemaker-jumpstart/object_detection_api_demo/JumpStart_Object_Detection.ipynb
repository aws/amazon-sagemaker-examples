{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2062d597",
   "metadata": {},
   "source": [
    "# Introduction to JumpStart - Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634722a6",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker JumpStart API](https://sagemaker.readthedocs.io/en/stable/doc_utils/jumpstart.html). \n",
    "\n",
    "In this demo notebook, we demonstrate how to use the JumpStart API for Object Detection.  Object Detection refers to localizing objects in an image with a bounding box and predicting the classes of the objects. It can be used for counting objects, determining their exact locations and applied to multiple areas including security, surveillance, automated vehicle systems and machine inspection.\n",
    "\n",
    "We demonstrate two use cases of object detection models:\n",
    "\n",
    "* How to use pre-trained models trained on COCO dataset to do object detection.\n",
    "* Use Jumpstart algorithm in transfer learning mode to train a Object Detection model on a custom dataset. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f7914",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Run inference on the pre-trained model](#2.-Run-inference-on-the-pre-trained-model)\n",
    "    * [Select a pre-trained model for inference](#2.1.-Select-a-pre-trained-model-for-inference)\n",
    "    * [Retrieve JumpStart Artifacts & Deploy an Endpoint](#2.2.-Retrieve-JumpStart-Artifacts-&-Deploy-an-Endpoint)\n",
    "    * [Download an example image for inference](#2.3.-Download-an-example-image-for-inference)\n",
    "    * [Query endpoint and parse response](#2.4.-Query-endpoint-and-parse-response)\n",
    "    * [Display model predictions](#2.5.-Display-model-predictions)\n",
    "    * [Clean up the endpoint](#2.6-Clean-up-the-endpoint)\n",
    "\n",
    "3. [Fine-tune the pre-trained model on a custom dataset](#3.-Fine-tune-the-pre-trained-model-on-a-custom-dataset)\n",
    "    * [Retrieve Training Artifacts](#3.1.-Retrieve-Training-Artifacts)\n",
    "    * [Set Training hyperparameters](#3.2.-Set-Training-hyperparameters)\n",
    "    * [Start the Training](#3.3.-Start-the-Training)\n",
    "    * [Deploy and run inference on the fine-tuned model](#3.4.-Deploy-and-run-inference-on-the-fine-tuned-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d3f82",
   "metadata": {},
   "source": [
    "Note: This notebook was tested in Amazon SageMaker Studio on ml.t3.medium instance with Python 3 (Data Science) kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26075701",
   "metadata": {},
   "source": [
    "## 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727aa48",
   "metadata": {},
   "source": [
    "---\n",
    "Before executing the notebook, there are some initial steps required for setup. This notebook requires latest version of sagemaker and ipywidgets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fe09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9337536",
   "metadata": {},
   "source": [
    "### Permissions and environment variables\n",
    "\n",
    "---\n",
    "To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role of the current notebook as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. \n",
    "\n",
    "\n",
    "* We need to identify the S3 bucket that you want to use for providing training and validation datasets. It will also be used to store the tranied model artifacts. In this notebook, we use a custom bucket. You could alternatively use a default bucket for the session. We use an object prefix to help organize the bucket content.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-api-od-training-example\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43a6a3",
   "metadata": {},
   "source": [
    "## 2. Run inference on the pre-trained model\n",
    "\n",
    "***\n",
    "\n",
    "Using JumpStart, we can perform inference on the pre-trained base-model, even without fine-tuning it first on a new dataset. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541273f",
   "metadata": {},
   "source": [
    "### 2.1. Select a pre-trained model for inference\n",
    "\n",
    "***\n",
    "Here, we download jumpstart model_manifest file from the jumpstart s3 bucket, and filter-out all the Object Detection models.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "\n",
    "# download JumpStart model_manifest file.\n",
    "boto3.client(\"s3\").download_file(\n",
    "    f\"jumpstart-cache-prod-{aws_region}\", \"models_manifest.json\", \"models_manifest.json\"\n",
    ")\n",
    "with open(\"models_manifest.json\", \"rb\") as json_file:\n",
    "    model_list = json.load(json_file)\n",
    "\n",
    "# filter-out all the Object Detection models from the manifest list.\n",
    "od_models = []\n",
    "for model in model_list:\n",
    "    model_id = model[\"model_id\"]\n",
    "    if (\"-od-\" in model_id or \"-od1-\" in model_id) and model_id not in od_models:\n",
    "        od_models.append(model_id)\n",
    "\n",
    "print(f\"Number of models available for inference: {len(od_models)}\")\n",
    "# display the model-ids in a dropdown, for user to select a model.\n",
    "infer_model_dropdown = Dropdown(\n",
    "    options=od_models,\n",
    "    value=\"pytorch-od-nvidia-ssd\",\n",
    "    description=\"Chose a model for inference:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89bd99",
   "metadata": {},
   "source": [
    "### 2.2. Retrieve JumpStart Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "We retrieve the base_model_uri for the pre-trained model, which is a model pre-trained on COCO 2017. We retrieve the deploy_image_uri, and the deploy_source_uri as well. To host the pre-trained base-model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388add0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# model_version=\"*\" fetches the latest version of the model\n",
    "infer_model_id, infer_model_version = infer_model_dropdown.value, \"*\"\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(\n",
    "    f\"jumpstart-api-infer-{infer_model_id}-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    ")\n",
    "\n",
    "inference_instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=infer_model_id,\n",
    "    model_version=infer_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=infer_model_id, model_version=infer_model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the base model uri\n",
    "base_model_uri = model_uris.retrieve(\n",
    "    model_id=infer_model_id, model_version=infer_model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=base_model_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9e493",
   "metadata": {},
   "source": [
    "### 2.3. Download an example image for inference\n",
    "---\n",
    "We download an example stored in an S3 bucket.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_s3_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "assets_key_prefix = \"pytorch-metadata/assets\"\n",
    "Naxos_Taverna = \"Naxos_Taverna.jpg\"\n",
    "\n",
    "boto3.client(\"s3\").download_file(\n",
    "    assets_s3_bucket, f\"{assets_key_prefix}/{Naxos_Taverna}\", Naxos_Taverna\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa584ea5",
   "metadata": {},
   "source": [
    "### 2.4. Query endpoint and parse response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182da008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def query(model_predictor, image_file_name):\n",
    "    with open(image_file_name, \"rb\") as file:\n",
    "        input_img_rb = file.read()\n",
    "\n",
    "    query_response = model_predictor.predict(\n",
    "        input_img_rb,\n",
    "        {\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"Accept\": \"application/json;verbose;n_predictions=3\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(model_predictions):\n",
    "    model_predictions = json.loads(query_response)\n",
    "    normalized_boxes, classes, scores, labels = (\n",
    "        model_predictions[\"normalized_boxes\"],\n",
    "        model_predictions[\"classes\"],\n",
    "        model_predictions[\"scores\"],\n",
    "        model_predictions[\"labels\"],\n",
    "    )\n",
    "    # Substitute the classes index with the classes name\n",
    "    class_names = [labels[int(idx)] for idx in classes]\n",
    "    return normalized_boxes, class_names, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query(base_model_predictor, Naxos_Taverna)\n",
    "\n",
    "normalized_boxes, classes_names, confidences = parse_response(query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f606eb9",
   "metadata": {},
   "source": [
    "### 2.5. Display model predictions\n",
    "---\n",
    "We display the bounding boxes overlaid on the original image\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae410d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def display_predictions(img_jpg, normalized_boxes, classes_names, confidences):\n",
    "    colors = list(ImageColor.colormap.values())\n",
    "    image_np = np.array(Image.open(img_jpg))\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    ax = plt.axes()\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    for idx in range(len(normalized_boxes)):\n",
    "        left, bot, right, top = normalized_boxes[idx]\n",
    "        x, w = [val * image_np.shape[1] for val in [left, right - left]]\n",
    "        y, h = [val * image_np.shape[0] for val in [bot, top - bot]]\n",
    "        color = colors[hash(classes_names[idx]) % len(colors)]\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor=color, facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x,\n",
    "            y,\n",
    "            \"{} {:.0f}%\".format(classes_names[idx], confidences[idx] * 100),\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(Naxos_Taverna, normalized_boxes, classes_names, confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d552e",
   "metadata": {},
   "source": [
    "### 2.6. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcd889",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the pre-trained model on a custom dataset\n",
    "\n",
    "Previously, we saw how to run inference on a pre-trained model. Next, we discuss how this model can be finetuned to a custom dataset with any number of classes. \n",
    "\n",
    "Transfer learning algorithm removes the object detection head of the pre-trained model and attaches a head with number of classes same as the custom dataset. The fine-tuning step fine-tunes the last layer parameters while keeping the parameters of the rest of the model frozen, and returns the fine-tuned model. The objective is to minimize box prediction error on the input data. \n",
    "\n",
    "Input must be a directory with sub-directoriey images and a file annotations.json. The input directory should look like below if the training data contains two images. Note the trailing `/` is required. The names of .png files\n",
    "can be anything. \n",
    "\n",
    "    input_directory\n",
    "        |--images\n",
    "            |--abc.png\n",
    "            |--def.png\n",
    "        |--annotations.json\n",
    "            \n",
    " \n",
    "The annotations.json file should should have information for bounding_boxes and their class labels. It should have a \n",
    "dictionary with keys \"images\" and \"annotations\". Value for the \"images\" key should be a list of entries, \n",
    "one for each image of the form {\"file_name\": image_name, \"height\": height, \"width\": width, \"id\": image_id}. Value of the 'annotations' key should be a list of entries, one for each bounding box of the form {\"image_id\": image_id, \"bbox\": \\[xmin, ymin, xmax, ymax\\], \"category_id\": bbox_label}.\n",
    "\n",
    "We provide pennfudanped dataset as a default dataset for fine-tuning the model.\n",
    "PennFudanPed comprises images of pedestrians. The dataset has been downloaded from [here](https://www.cis.upenn.edu/~jshi/ped_html/#pub1). \n",
    "\n",
    "Citation:\n",
    "<sub><sup>\n",
    "@ONLINE {pennfudanped,\n",
    "author = \"Liming Wang1, Jianbo Shi2, Gang Song2, and I-fan Shen1\",\n",
    "title = \"Penn-Fudan Database for Pedestrian Detection and Segmentation\",\n",
    "year = \"2007\",\n",
    "url = \"https://www.cis.upenn.edu/~jshi/ped_html/\" }\n",
    "</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77545c7d",
   "metadata": {},
   "source": [
    "### 3.1. Retrieve Training Artifacts\n",
    "\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained base model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that, model_version=\"*\" fetches the lates model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "# Currently, not all the object detection models in jumpstart support finetuning. Thus, we manually select a model\n",
    "# which supports finetuning.\n",
    "train_model_id, train_model_version, train_scope = (\n",
    "    \"mxnet-od-ssd-300-vgg16-atrous-coco\",\n",
    "    \"*\",\n",
    "    \"training\",\n",
    ")\n",
    "training_instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "default_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bd263",
   "metadata": {},
   "source": [
    "### 3.2. Set Training hyperparameters\n",
    "\n",
    "Now that we are done with all the setup that is needed, we are ready to train our Object Detection model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job.\n",
    "### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "* **Training instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training \n",
    "* **Output path**: This the s3 folder in which the training output is stored\n",
    "\n",
    "The second ones are algorithm specific training hyper-parameters. The can be looked into the `default_hyperparameters` dictionary, and overridden to custom values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0721b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "default_hyperparameters[\"epochs\"] = \"10\"\n",
    "print(default_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "training_job_name = name_from_base(\n",
    "    f\"jumpstart-api-{train_model_id}-transfer-learning-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    ")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "od_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=default_hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5adbb7",
   "metadata": {},
   "source": [
    "### 3.3. Start the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74690a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "training_data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "training_data_prefix = \"training-datasets/PennFudanPed_COCO_format/\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "\n",
    "# Launch a SageMaker Training job by passing s3 path of the training data\n",
    "od_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564397ed",
   "metadata": {},
   "source": [
    "### 3.4. Deploy and run inference on the fine-tuned model\n",
    "\n",
    "---\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the bounding boxes of an image. We follow the same steps as in [2. Run inference on the pre-trained model](#2.-Run-inference-on-the-pre-trained-model).\n",
    "\n",
    "We start by retrieving the jumpstart artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `od_estimator` that we fine-tuned.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(\n",
    "    f\"sagemaker-api-{train_model_id}-FT-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    ")\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = od_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eecf34f",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we download an example pedestrian image from the S3 bucket for inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ab5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_image_s3_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "pedestrian_image_key = \"training-datasets/PennFudanPed_COCO_format/images\"\n",
    "pedestrian_image_file_name = \"FudanPed00001.png\"\n",
    "\n",
    "boto3.client(\"s3\").download_file(\n",
    "    pedestrian_image_s3_bucket,\n",
    "    f\"{pedestrian_image_key}/{pedestrian_image_file_name}\",\n",
    "    pedestrian_image_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac32f3",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we query the finetuned model, parse the response and display the predictions. Funtions for these are implemented in sections [2.4. Query endpoint and parse response](#2.4.-Query-endpoint-and-parse-response) and [2.5. Display model predictions](#2.5.-Display-model-predictions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query(finetuned_predictor, pedestrian_image_file_name)\n",
    "\n",
    "normalized_boxes, classes_names, confidences = parse_response(query_response)\n",
    "display_predictions(pedestrian_image_file_name, normalized_boxes, classes_names, confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c246130",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we delete the endpoint corresponding to the finetuned model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662da33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
