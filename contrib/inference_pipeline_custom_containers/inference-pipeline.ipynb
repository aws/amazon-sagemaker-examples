{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline with Custom Containers and xgBoost\n",
    "Typically a Machine Learning (ML) process consists of few steps: data gathering with various ETL jobs, pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm. \n",
    "In many cases, when the trained model is used for processing real time or batch prediction requests, the model receives data in a format which needs to pre-processed (e.g. featurized) before it can be passed to the algorithm. In the following notebook, we will demonstrate how you can build your ML Pipeline leveraging the ability to create custom Sagemaker algorithms and the out of the box SageMaker xgBoost algorithm. After the model is trained we will deploy the ML Pipeline (data preprocessing, the xgBoost classifier, and data postprocessing) as an Inference Pipeline behind a single SageMaker Endpoint for real time inference. We will also use the preprocessor with batch transformation using Amazon SageMaker Batch Transform to prepare xgBoost training data.\n",
    "\n",
    "![Inference Diagram](./Inference_diagram.png)\n",
    "\n",
    "The toy problem that is being solved here is to match a set of keywords to a category of questions. From there we can match that category against a list of available agents who specialize in answering that category of question. The agents and their availability is stored externally in a DynamoDB database. The data transformations, matching against our model, and querying of the database are all done as part of the inference pipeline.\n",
    "\n",
    "The preprocessing step of the pipeline encodes a comma-separated list of words into a format that xgBoost understands using a CountVectorizer. It also trains a LabelEncoder, which is used to transform from the categories of questions to a set of integers - having the labels encoded as integers is also a requirement of the xgBoost multiclass classifer. \n",
    "\n",
    "The xgBoost model maps the encoded list of words to an integer, which represents the encoded class of questions that best matches those words.\n",
    "\n",
    "Finally, the postprocessing step of the pipeline uses the LabelEncoding model trained in the preprocessing step to map the number representing the classification of the question back to the text. It then takes the category and queries dynamodb for available agents that matches that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p returns_data\n",
    "!python3 generate-training-data.py --samples 100000 --filename returns_data/samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 load-ddb-data.py PipelineLookupTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'Custom-Pipeline-Inference-Example'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"upload_data\"></a>\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. We can use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'returns_data'\n",
    "\n",
    "train_input = sagemaker_session.upload_data(\n",
    "    path='{}/{}'.format(WORK_DIRECTORY, 'samples.csv'), \n",
    "    bucket=bucket,\n",
    "    key_prefix='{}/{}'.format(prefix, 'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a loader function\n",
    "\n",
    "The load_data function pulls in the CSV data into two columns: the first column of the CSV is mapped to the label, and every subsequent CSV column is loaded as a dictionary into the second Pandas column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "def load_data(raw, columns, skip_first_row=True):\n",
    "    recs = [(row[0], set(row[1:])) for row in csv.reader(raw)]\n",
    "    if skip_first_row:\n",
    "        return pd.DataFrame.from_records(recs[1:], columns=columns)\n",
    "    else:\n",
    "        return pd.DataFrame.from_records(recs, columns=columns)\n",
    "\n",
    "def load(files, columns, skip_first_row=True):\n",
    "    raw_data = []\n",
    "    for file in files:\n",
    "        raw_data.append(load_data(open(file), columns, skip_first_row))\n",
    "\n",
    "    return pd.concat(raw_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load([\"returns_data/samples.csv\"], [\"label\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category_medical</td>\n",
       "      <td>{covid, expense}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>category_itemization</td>\n",
       "      <td>{deduction, donation, itemization}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>category_investments</td>\n",
       "      <td>{capital}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_estate taxes</td>\n",
       "      <td>{inheritance, 403b, late}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category_deferments</td>\n",
       "      <td>{deferment, payment, late}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label                               words\n",
       "0       category_medical                    {covid, expense}\n",
       "1   category_itemization  {deduction, donation, itemization}\n",
       "2   category_investments                           {capital}\n",
       "3  category_estate taxes           {inheritance, 403b, late}\n",
       "4    category_deferments          {deferment, payment, late}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(df['label'].unique())\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words in our dataset\n",
    "\n",
    "Let's take a look at the set of words being used. We use a CountVectorizer with the set analyzer to encode the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['401k',\n",
       " '403b',\n",
       " 'capital',\n",
       " 'charitable',\n",
       " 'covid',\n",
       " 'deduction',\n",
       " 'deferment',\n",
       " 'delay',\n",
       " 'donation',\n",
       " 'estate',\n",
       " 'expense',\n",
       " 'gains',\n",
       " 'inheritance',\n",
       " 'investment',\n",
       " 'ira',\n",
       " 'itemization',\n",
       " 'late',\n",
       " 'local',\n",
       " 'losses',\n",
       " 'medical',\n",
       " 'mortgage',\n",
       " 'payment',\n",
       " 'properties',\n",
       " 'rental',\n",
       " 'state',\n",
       " 'tax']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer=set)\n",
    "count_res = vectorizer.fit_transform(df['words'])\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328296961357.dkr.ecr.us-west-2.amazonaws.com/custompipeline/preprocessor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "ecr_namespace = 'custompipeline/'\n",
    "prefix = 'preprocessor'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing step\n",
    "\n",
    "Even though in we are not training a Machine Learning model in this step, we are going to use SageMaker SDK methods that involve creating an Estimator and calling .fit() and .depoly() on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07 22:16:01 Starting - Starting the training job...\n",
      "2021-04-07 22:16:24 Starting - Launching requested ML instances......\n",
      "2021-04-07 22:17:30 Starting - Preparing the instances for training......\n",
      "2021-04-07 22:18:24 Downloading - Downloading input data\n",
      "2021-04-07 22:18:24 Training - Downloading the training image...\n",
      "2021-04-07 22:19:00 Uploading - Uploading generated training model.\u001b[34mStarting script\u001b[0m\n",
      "\u001b[34marguments: ['main.py', 'train']\u001b[0m\n",
      "\u001b[34mstarting training...\n",
      "\u001b[0m\n",
      "\u001b[34mHyperparameters configuration:\u001b[0m\n",
      "\u001b[34m{}\n",
      "\u001b[0m\n",
      "\u001b[34mInput data configuration:\u001b[0m\n",
      "\u001b[34m{'train': {'RecordWrapperType': 'None',\n",
      "           'S3DistributionType': 'FullyReplicated',\n",
      "           'TrainingInputMode': 'File'}}\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in train channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/samples.csv\n",
      "\u001b[0m\n",
      "\u001b[34mResource configuration:\u001b[0m\n",
      "\u001b[34m{'current_host': 'algo-1',\n",
      " 'hosts': ['algo-1'],\n",
      " 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m<class 'pandas.core.frame.DataFrame'>\u001b[0m\n",
      "\u001b[34mRangeIndex: 100000 entries, 0 to 99999\u001b[0m\n",
      "\u001b[34mData columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \u001b[0m\n",
      "\u001b[34m---  ------  --------------   ----- \n",
      " 0   label   100000 non-null  object\n",
      " 1   words   100000 non-null  object\u001b[0m\n",
      "\u001b[34mdtypes: object(2)\u001b[0m\n",
      "\u001b[34mmemory usage: 1.5+ MB\u001b[0m\n",
      "\u001b[34mNone\u001b[0m\n",
      "\u001b[34mfitting...\u001b[0m\n",
      "\u001b[34mfinished fitting...\u001b[0m\n",
      "\u001b[34m['401k', '403b', 'capital', 'charitable', 'covid', 'deduction', 'deferment', 'delay', 'donation', 'estate', 'expense', 'gains', 'inheritance', 'investment', 'ira', 'itemization', 'late', 'local', 'losses', 'medical', 'mortgage', 'payment', 'properties', 'rental', 'state', 'tax']\u001b[0m\n",
      "\u001b[34mle classes:  ['category_deferments' 'category_estate taxes' 'category_investments'\n",
      " 'category_itemization' 'category_medical' 'category_properties']\u001b[0m\n",
      "\u001b[34msaved model!\u001b[0m\n",
      "\n",
      "2021-04-07 22:19:24 Completed - Training job completed\n",
      "Training seconds: 63\n",
      "Billable seconds: 63\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "custom_preprocessor = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    #train_instance_type='local', # use local mode\n",
    "                                    instance_type='ml.m5.4xlarge',\n",
    "                                    base_job_name=prefix)\n",
    "\n",
    "train_config = sagemaker.session.TrainingInput('s3://{0}/{1}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "val_config = sagemaker.session.TrainingInput('s3://{0}/{1}/val/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "custom_preprocessor.fit({'train': train_input}, logs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_name = 'simple-ep-' + timestamp_prefix\n",
    "predictor = custom_preprocessor.deploy(initial_instance_count=1, instance_type='ml.c5.4xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us make sure that preprosessor returns correct output\n",
    "we will then delete the endpoint since it is no longer needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0\\n0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "payload = \"rental,peanut,butter\\ncovid\"\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=CSVSerializer())\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '010d90b4-f535-4d11-9ede-03cff2a16f23',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '010d90b4-f535-4d11-9ede-03cff2a16f23',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 07 Apr 2021 22:21:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform our training data <a class=\"anchor\" id=\"preprocess_train_data\"></a>\n",
    "Now that our proprocessor is properly fitted, let's go ahead and preprocess our training data. Let's use batch transform to directly preprocess the raw data and store right back into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = custom_preprocessor.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m4.xlarge',\n",
    "    assemble_with = 'Line',\n",
    "    accept = 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n",
      "\u001b[34mStarting script\u001b[0m\n",
      "\u001b[35mStarting script\u001b[0m\n",
      "\u001b[34marguments: ['main.py', 'serve']\u001b[0m\n",
      "\u001b[34mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[34musing port:  8080\u001b[0m\n",
      "\u001b[34mnginx.conf: worker_processes 1;\u001b[0m\n",
      "\u001b[34mdaemon off; #Prevent forking\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log /var/log/nginx/error.log;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  # defaults\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[35marguments: ['main.py', 'serve']\u001b[0m\n",
      "\u001b[35mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[35musing port:  8080\u001b[0m\n",
      "\u001b[35mnginx.conf: worker_processes 1;\u001b[0m\n",
      "\u001b[35mdaemon off; #Prevent forking\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log /var/log/nginx/error.log;\n",
      "\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  # defaults\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /var/log/nginx/access.log combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 5m;\n",
      "\n",
      "    keepalive_timeout 5;\n",
      "\n",
      "    location ~ ^/(ping|invocations) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [17] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:14 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:14 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:14 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /var/log/nginx/access.log combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 5m;\n",
      "\n",
      "    keepalive_timeout 5;\n",
      "\n",
      "    location ~ ^/(ping|invocations) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [17] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:14 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:14 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:14 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mdata:  b'label,words\\ncategory_medical,covid,expense\\ncategory_itemization,donation,deduction,itemization\\ncateg'\u001b[0m\n",
      "\u001b[34mcookies:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[34mheaders:  {'X-Forwarded-For': '169.254.255.130', 'Host': '169.254.255.131:8080', 'Connection': 'close', 'Content-Length': '4267490', 'User-Agent': 'Go-http-client/1.1', 'Accept': 'text/csv', 'Content-Type': 'text/csv', 'X-Amzn-Sagemaker-Input-Object': 'sagemaker-us-west-2-328296961357/Custom-Pipeline-Inference-Example/train/samples.csv', 'X-Amzn-Sagemaker-Input-Object-Base64': 'c2FnZW1ha2VyLXVzLXdlc3QtMi0zMjgyOTY5NjEzNTcvQ3VzdG9tLVBpcGVsaW5lLUluZmVyZW5jZS1FeGFtcGxlL3RyYWluL3NhbXBsZXMuY3N2', 'Accept-Encoding': 'gzip'}\u001b[0m\n",
      "\u001b[34margs:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[34mContent type text/csv\u001b[0m\n",
      "\u001b[34mAccept text/csv\u001b[0m\n",
      "\u001b[34mFirst entry is:  label\u001b[0m\n",
      "\u001b[34mLength indicates that label is included\u001b[0m\n",
      "\u001b[34mmerged df                    label                               words\u001b[0m\n",
      "\u001b[34m0       category_medical                    {covid, expense}\u001b[0m\n",
      "\u001b[34m1   category_itemization  {donation, deduction, itemization}\u001b[0m\n",
      "\u001b[34m2   category_investments                           {capital}\u001b[0m\n",
      "\u001b[34m3  category_estate taxes           {late, 403b, inheritance}\u001b[0m\n",
      "\u001b[34m4    category_deferments          {deferment, late, payment}\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mdata:  b'label,words\\ncategory_medical,covid,expense\\ncategory_itemization,donation,deduction,itemization\\ncateg'\u001b[0m\n",
      "\u001b[35mcookies:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[35mheaders:  {'X-Forwarded-For': '169.254.255.130', 'Host': '169.254.255.131:8080', 'Connection': 'close', 'Content-Length': '4267490', 'User-Agent': 'Go-http-client/1.1', 'Accept': 'text/csv', 'Content-Type': 'text/csv', 'X-Amzn-Sagemaker-Input-Object': 'sagemaker-us-west-2-328296961357/Custom-Pipeline-Inference-Example/train/samples.csv', 'X-Amzn-Sagemaker-Input-Object-Base64': 'c2FnZW1ha2VyLXVzLXdlc3QtMi0zMjgyOTY5NjEzNTcvQ3VzdG9tLVBpcGVsaW5lLUluZmVyZW5jZS1FeGFtcGxlL3RyYWluL3NhbXBsZXMuY3N2', 'Accept-Encoding': 'gzip'}\u001b[0m\n",
      "\u001b[35margs:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[35mContent type text/csv\u001b[0m\n",
      "\u001b[35mAccept text/csv\u001b[0m\n",
      "\u001b[35mFirst entry is:  label\u001b[0m\n",
      "\u001b[35mLength indicates that label is included\u001b[0m\n",
      "\u001b[35mmerged df                    label                               words\u001b[0m\n",
      "\u001b[35m0       category_medical                    {covid, expense}\u001b[0m\n",
      "\u001b[35m1   category_itemization  {donation, deduction, itemization}\u001b[0m\n",
      "\u001b[35m2   category_investments                           {capital}\u001b[0m\n",
      "\u001b[35m3  category_estate taxes           {late, 403b, inheritance}\u001b[0m\n",
      "\u001b[35m4    category_deferments          {deferment, late, payment}\u001b[0m\n",
      "\u001b[34mlabel_column in input_data\u001b[0m\n",
      "\u001b[35mlabel_column in input_data\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Apr/2021:22:26:22 +0000] \"POST /invocations HTTP/1.1\" 200 5400000 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Apr/2021:22:26:22 +0000] \"POST /invocations HTTP/1.1\" 200 5400000 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-04-07T22:26:19.340:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Waiting for transform job: preprocessor-2021-04-07-22-21-54-558\n",
      "\u001b[34mStarting script\u001b[0m\n",
      "\u001b[35mStarting script\u001b[0m\n",
      "\u001b[34marguments: ['main.py', 'serve']\u001b[0m\n",
      "\u001b[34mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[34musing port:  8080\u001b[0m\n",
      "\u001b[34mnginx.conf: worker_processes 1;\u001b[0m\n",
      "\u001b[34mdaemon off; #Prevent forking\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log /var/log/nginx/error.log;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  # defaults\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[35marguments: ['main.py', 'serve']\u001b[0m\n",
      "\u001b[35mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[35musing port:  8080\u001b[0m\n",
      "\u001b[35mnginx.conf: worker_processes 1;\u001b[0m\n",
      "\u001b[35mdaemon off; #Prevent forking\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log /var/log/nginx/error.log;\n",
      "\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  # defaults\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /var/log/nginx/access.log combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 5m;\n",
      "\n",
      "    keepalive_timeout 5;\n",
      "\n",
      "    location ~ ^/(ping|invocations) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [17] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:13 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:14 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:14 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2021-04-07 22:26:14 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /var/log/nginx/access.log combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 5m;\n",
      "\n",
      "    keepalive_timeout 5;\n",
      "\n",
      "    location ~ ^/(ping|invocations) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [17] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:13 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:14 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:14 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[35m[2021-04-07 22:26:14 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mdata:  b'label,words\\ncategory_medical,covid,expense\\ncategory_itemization,donation,deduction,itemization\\ncateg'\u001b[0m\n",
      "\u001b[34mcookies:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[34mheaders:  {'X-Forwarded-For': '169.254.255.130', 'Host': '169.254.255.131:8080', 'Connection': 'close', 'Content-Length': '4267490', 'User-Agent': 'Go-http-client/1.1', 'Accept': 'text/csv', 'Content-Type': 'text/csv', 'X-Amzn-Sagemaker-Input-Object': 'sagemaker-us-west-2-328296961357/Custom-Pipeline-Inference-Example/train/samples.csv', 'X-Amzn-Sagemaker-Input-Object-Base64': 'c2FnZW1ha2VyLXVzLXdlc3QtMi0zMjgyOTY5NjEzNTcvQ3VzdG9tLVBpcGVsaW5lLUluZmVyZW5jZS1FeGFtcGxlL3RyYWluL3NhbXBsZXMuY3N2', 'Accept-Encoding': 'gzip'}\u001b[0m\n",
      "\u001b[34margs:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[34mContent type text/csv\u001b[0m\n",
      "\u001b[34mAccept text/csv\u001b[0m\n",
      "\u001b[34mFirst entry is:  label\u001b[0m\n",
      "\u001b[34mLength indicates that label is included\u001b[0m\n",
      "\u001b[34mmerged df                    label                               words\u001b[0m\n",
      "\u001b[34m0       category_medical                    {covid, expense}\u001b[0m\n",
      "\u001b[34m1   category_itemization  {donation, deduction, itemization}\u001b[0m\n",
      "\u001b[34m2   category_investments                           {capital}\u001b[0m\n",
      "\u001b[34m3  category_estate taxes           {late, 403b, inheritance}\u001b[0m\n",
      "\u001b[34m4    category_deferments          {deferment, late, payment}\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Apr/2021:22:26:19 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mdata:  b'label,words\\ncategory_medical,covid,expense\\ncategory_itemization,donation,deduction,itemization\\ncateg'\u001b[0m\n",
      "\u001b[35mcookies:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[35mheaders:  {'X-Forwarded-For': '169.254.255.130', 'Host': '169.254.255.131:8080', 'Connection': 'close', 'Content-Length': '4267490', 'User-Agent': 'Go-http-client/1.1', 'Accept': 'text/csv', 'Content-Type': 'text/csv', 'X-Amzn-Sagemaker-Input-Object': 'sagemaker-us-west-2-328296961357/Custom-Pipeline-Inference-Example/train/samples.csv', 'X-Amzn-Sagemaker-Input-Object-Base64': 'c2FnZW1ha2VyLXVzLXdlc3QtMi0zMjgyOTY5NjEzNTcvQ3VzdG9tLVBpcGVsaW5lLUluZmVyZW5jZS1FeGFtcGxlL3RyYWluL3NhbXBsZXMuY3N2', 'Accept-Encoding': 'gzip'}\u001b[0m\n",
      "\u001b[35margs:  ImmutableMultiDict([])\u001b[0m\n",
      "\u001b[35mContent type text/csv\u001b[0m\n",
      "\u001b[35mAccept text/csv\u001b[0m\n",
      "\u001b[35mFirst entry is:  label\u001b[0m\n",
      "\u001b[35mLength indicates that label is included\u001b[0m\n",
      "\u001b[35mmerged df                    label                               words\u001b[0m\n",
      "\u001b[35m0       category_medical                    {covid, expense}\u001b[0m\n",
      "\u001b[35m1   category_itemization  {donation, deduction, itemization}\u001b[0m\n",
      "\u001b[35m2   category_investments                           {capital}\u001b[0m\n",
      "\u001b[35m3  category_estate taxes           {late, 403b, inheritance}\u001b[0m\n",
      "\u001b[35m4    category_deferments          {deferment, late, payment}\u001b[0m\n",
      "\u001b[34mlabel_column in input_data\u001b[0m\n",
      "\u001b[35mlabel_column in input_data\u001b[0m\n",
      "\u001b[32m2021-04-07T22:26:19.340:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Apr/2021:22:26:22 +0000] \"POST /invocations HTTP/1.1\" 200 5400000 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Apr/2021:22:26:22 +0000] \"POST /invocations HTTP/1.1\" 200 5400000 \"-\" \"Go-http-client/1.1\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type='text/csv', split_type='Line')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path\n",
    "#print(preprocessed_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit an XGBoost Model with the preprocessed data <a class=\"anchor\" id=\"training_model\"></a>\n",
    "Let's take the preprocessed training data and use it to train an XGBoost Model. Sagemaker provides pre-built algorithm containers that can be used with the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "#xgboost_container = get_image_uri(region=region, framework='xgboost')\n",
    "\n",
    "from sagemaker.image_uris import retrieve\n",
    "xgboost_container = retrieve(region=region, framework='xgboost', version ='latest')\n",
    "xgboost_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": num_labels,\n",
    "        \"num_round\": \"10\"\n",
    "    }\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'xgboost-pipeline-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=xgboost_hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.4xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_train_data = sagemaker.inputs.TrainingInput(\n",
    "    preprocessed_train, \n",
    "    distribution='FullyReplicated',\n",
    "    content_type='text/csv', \n",
    "    s3_data_type='S3Prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07 22:27:11 Starting - Starting the training job...\n",
      "2021-04-07 22:27:34 Starting - Launching requested ML instances......\n",
      "2021-04-07 22:28:35 Starting - Preparing the instances for training...\n",
      "2021-04-07 22:29:04 Downloading - Downloading input data...\n",
      "2021-04-07 22:29:39 Training - Training image download completed. Training in progress.\n",
      "2021-04-07 22:29:39 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-04-07:22:29:34:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-04-07:22:29:34:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2021-04-07:22:29:34:INFO] File size need to be processed in the node: 5.15mb. Available memory size in the node: 55423.07mb\u001b[0m\n",
      "\u001b[34m[2021-04-07:22:29:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[22:29:34] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[22:29:34] 100000x26 matrix with 2600000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-merror:0.05131\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-merror:0.03593\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-merror:0.0339\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-merror:0.03268\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-merror:0.03017\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-merror:0.02851\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-merror:0.02742\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-merror:0.02608\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-merror:0.02552\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22:29:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-merror:0.02465\u001b[0m\n",
      "\n",
      "2021-04-07 22:29:55 Completed - Training job completed\n",
      "Training seconds: 43\n",
      "Billable seconds: 43\n"
     ]
    }
   ],
   "source": [
    "# execute the XGBoost training job\n",
    "xgboost_estimator.fit({'train': xgboost_train_data}, logs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Inference Pipeline with the preprocessor, XGBoost classifier and postprocessor <a class=\"anchor\" id=\"serial_inference\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the inference pipeline <a class=\"anchor\" id=\"pipeline_setup\"></a>\n",
    "Setting up a Machine Learning pipeline can be done with the Pipeline Model. This sets up a list of models in a single endpoint; in this example, we configure our pipeline model with the fitted preprocessor model, the fitted xgBoost model, and the postprocessor (which uses the preprocessor model data). Deploying the model follows the same ```deploy``` pattern in the SDK.\n",
    "\n",
    "Notice that we pass in the trained model from the proprocessor into the postpostprocessor. The reason we do this is so that the postprocesser can access the LabelEncoder that was trained in the preprocessor to invert that operation. This allows the inference pipeline to return the actual name of the category instead of the category label (e.g. \"medical\" instead of 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'328296961357.dkr.ecr.us-west-2.amazonaws.com/custompipeline/postprocessor:latest'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_namespace = 'custompipeline/'\n",
    "prefix = 'postprocessor'\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "postprocessor_container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "postprocessor_container_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "#print(Model.__init__.__doc__)\n",
    "custom_postprocessor = Model(\n",
    "    image_uri=postprocessor_container_image_uri,\n",
    "    model_data=custom_preprocessor.model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    "    ,\n",
    "    env=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an inference pipeline made up of three containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "preprocessor_model = custom_preprocessor.create_model()\n",
    "classifier_model = xgboost_estimator.create_model()\n",
    "postprocessor_model = custom_postprocessor\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, \n",
    "    role=role, \n",
    "    models=[\n",
    "        preprocessor_model, \n",
    "        classifier_model,\n",
    "        postprocessor_model])\n",
    "\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a request to our pipeline endpoint <a class=\"anchor\" id=\"pipeline_inference_request\"></a>\n",
    "\n",
    "Here we just grab the first line from the test data (you'll notice that the inference python script is very particular about the ordering of the inference request data). The ```ContentType``` field configures the first container, while the ```Accept``` field configures the last container. You can also specify each container's ```Accept``` and ```ContentType``` values using environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "payload = \"rental,peanut,butter\\ncovid\"\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=CSVSerializer())\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b'0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint <a class=\"anchor\" id=\"delete_endpoint\"></a>\n",
    "Once we are finished with the endpoint, we clean up the resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd082dc9e-3d46-4c4e-86b5-1f8b545f8f86',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd082dc9e-3d46-4c4e-86b5-1f8b545f8f86',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 06 Apr 2021 01:41:47 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
