{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining text sentiment analysis using SageMaker Clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. [Overview](#Overview)\n",
    " 1. [Prerequisites and Data](#Prerequisites-and-Data)\n",
    "     1. [Initialize SageMaker](#Initialize-SageMaker)\n",
    "     1. [Loading the data: Women's Ecommerce clothing reviews Dataset](#Loading-the-data:-Women's-ecommerce-clothing-reviews-dataset) \n",
    "     1. [Data preparation for model training](#Data-preparation-for-model-training) \n",
    " 1. [Train and Deploy Hugging Face Model](#Train-and-Deploy-Hugging-Face-Model)\n",
    "     1. [Train model with Hugging Face estimator](#Train-model-with-Hugging-Face-estimator)\n",
    "     1. [Deploy Model to Endpoint](#Deploy-Model)\n",
    " 1. [Model Explainability with SageMaker Clarify for text features](#Model-Explainability-with-SageMaker-Clarify-for-text-features)\n",
    "     1. [Explaining Predictions](#Explaining-Predictions)\n",
    "     1. [Visualize local explanations](#Visualize-local-explanations)\n",
    "     1. [Clean Up](#Clean-Up)\n",
    "\n",
    "## Overview\n",
    "Amazon SageMaker Clarify helps improve your machine learning models by detecting potential bias and helping explain how these models make predictions. The fairness and explainability functionality provided by SageMaker Clarify takes a step towards enabling AWS customers to build trustworthy and understandable machine learning models. The product comes with the tools to help you with the following tasks.\n",
    "\n",
    "* Measure biases that can occur during each stage of the ML lifecycle (data collection, model training and tuning, and monitoring of ML models deployed for inference).\n",
    "* Generate model governance reports targeting risk and compliance teams and external regulators.\n",
    "* Provide explanations of the data, models, and monitoring used to assess predictions for input containing data of various modalities like numerical data, categorical data, text, and images. \n",
    "\n",
    "Learn more about SageMaker Clarify [here](https://aws.amazon.com/sagemaker/clarify/). This sample notebook walks you through:\n",
    "1. Key terms and concepts needed to understand SageMaker Clarify\n",
    "1. The incremental updates required to explain text features, along with other tabular features.\n",
    "1. Explaining the importance of the various new input features on the model's decision\n",
    "\n",
    "In doing so, the notebook will first train a [Hugging Face model](https://huggingface.co/models) using the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) in the SageMaker Python SDK using training dataset, then use SageMaker Clarify to analyze a testing dataset in CSV format, and then visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Data\n",
    "\n",
    "We require the following AWS resources to be able to successfully run this notebook.\n",
    "1. Kernel: Python 3 (Data Science) kernel on SageMaker Studio or `conda_python3` kernel on notebook instances\n",
    "2. Instance type: Any GPU instance. Here, we use `ml.g4dn.xlarge`\n",
    "3. [SageMaker Python SDK](https://pypi.org/project/sagemaker/) version 2.70.0 or greater\n",
    "4. [Transformers](https://pypi.org/project/transformers/) > 4.6.1\n",
    "5. [Datasets](https://pypi.org/project/datasets/) > 1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" \"captum\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the latest version of the SageMaker Python SDK, boto, and AWS CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sagemaker botocore boto3 awscli --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Initialize SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data loading and pre-processing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import botocore\n",
    "import sagemaker\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, clarify\n",
    "from captum.attr import visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# SageMaker session bucket is used to upload the dataset, model and model training logs\n",
    "sess = sagemaker.Session()\n",
    "sess = sagemaker.Session(default_bucket=sess.default_bucket())\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-sagemaker-clarify-text\"\n",
    "\n",
    "# Define the IAM role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# SageMaker Clarify model directory name\n",
    "model_path = \"model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you change the value of `model_path` variable above, please be sure to update the `model_path` in [`code/inference.py`](./code/inference.py) script as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data: Women's ecommerce clothing reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset\n",
    "Data Source: `https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/`\n",
    "The Women’s E-Commerce Clothing Reviews dataset has been made available under a Creative Commons Public Domain license. A copy of the dataset has been saved in a sample data Amazon S3 bucket. In the first section of the notebook, we’ll walk through how to download the data and get started with building the ML workflow as a SageMaker pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://sagemaker-sample-files.s3.amazonaws.com/datasets/tabular/womens_clothing_ecommerce/Womens_Clothing_E-Commerce_Reviews.csv > womens_clothing_reviews_dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"womens_clothing_reviews_dataset.csv\", index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "The Women’s Clothing E-Commerce dataset contains reviews written by customers. Because the dataset contains real commercial data, it has been anonymized, and any references to the company in the review text and body have been replaced with “retailer”.\n",
    "\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "The dataset contains 23486 rows and 10 columns. Each row corresponds to a customer review.\n",
    "\n",
    "The columns include:\n",
    "\n",
    "* Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n",
    "* Age: Positive Integer variable of the reviewer's age.\n",
    "* Title: String variable for the title of the review.\n",
    "* Review Text: String variable for the review body.\n",
    "* Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "* Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "* Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n",
    "* Division Name: Categorical name of the product high level division.\n",
    "* Department Name: Categorical name of the product department name.\n",
    "* Class Name: Categorical name of the product class name.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "To predict the sentiment of a review based on the text, and then explain the predictions using SageMaker Clarify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable Creation\n",
    "Since the dataset does not contain a column that indicates the sentiment of the customer reviews, lets create one. To do this, let's assume that reviews with a `Rating` of 4 or higher indicate positive sentiment and reviews with a `Rating` of 2 or lower indicate negative sentiment. Let's also assume that a `Rating` of 3 indicates neutral sentiment and exclude these rows from the dataset. Additionally, to predict the sentiment of a review, we are going to use the `Review Text` column; therefore let's remove rows that are empty in the `Review Text` column of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_column(df, min_positive_score, max_negative_score):\n",
    "    neutral_values = [i for i in range(max_negative_score + 1, min_positive_score)]\n",
    "    for neutral_value in neutral_values:\n",
    "        df = df[df[\"Rating\"] != neutral_value]\n",
    "    df[\"Sentiment\"] = df[\"Rating\"] >= min_positive_score\n",
    "    replace_dict = {True: 1, False: 0}\n",
    "    df[\"Sentiment\"] = df[\"Sentiment\"].map(replace_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = create_target_column(df, 4, 2)\n",
    "df = df[~df[\"Review Text\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common approach for model evaluation is using the train/validation/test split. Although this approach can be very effective in general, it can result in misleading results and potentially fail when used on classification problems with a severe class imbalance. Instead, the technique must be modified to stratify the sampling by the class label as below. Stratification ensures that all classes are well represented across the train, validation and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Sentiment\"\n",
    "cols = \"Review Text\"\n",
    "\n",
    "X = df[cols]\n",
    "y = df[target]\n",
    "\n",
    "# Data split: 11%(val) of the 90% (train and test) of the dataset ~ 10%; resulting in 80:10:10split\n",
    "test_dataset_size = 0.10\n",
    "val_dataset_size = 0.11\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Stratified train-val-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_dataset_size, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_dataset_size, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Dataset: train \",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    y_train.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: validation \",\n",
    "    X_val.shape,\n",
    "    y_val.shape,\n",
    "    y_val.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: test \",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    y_test.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "\n",
    "# Combine the independent columns with the label\n",
    "df_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "df_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "df_val = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the dataset into train, test, and validation datasets. We use the train and validation datasets during training process, and run Clarify on the test dataset.\n",
    "\n",
    "In the cell below, we convert the Pandas DataFrames into Hugging Face Datasets for downstream modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload prepared dataset to the S3\n",
    "Here, we upload the prepared datasets to S3 buckets so that we can train the model with the Hugging Face Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 key prefix for the datasets\n",
    "s3_prefix = \"samples/datasets/womens_clothing_ecommerce_reviews\"\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step of the workflow, we use the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to load the pre-trained `distilbert-base-uncased` model and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with Hugging Face estimator\n",
    "The hyperparameters defined below are parameters that are passed to the custom PyTorch code in [`scripts/train.py`](./scripts/train.py). The only required parameter is `model_name`. The other parameters like `epoch`, `train_batch_size` all have default values which can be overriden by setting their values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters passed into the training job\n",
    "hyperparameters = {\"epochs\": 1, \"model_name\": \"distilbert-base-uncased\"}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained model files for model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {huggingface_estimator.model_data} model.tar.gz\n",
    "! mkdir -p {model_path}\n",
    "! tar -xvf model.tar.gz -C  {model_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deploy Model\n",
    "We are going to use the trained model files along with the PyTorch Inference container to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"hf_model.tar.gz\", mode=\"w:gz\") as archive:\n",
    "    archive.add(model_path, recursive=True)\n",
    "    archive.add(\"code/\")\n",
    "prefix = s3_prefix.split(\"/\")[-1]\n",
    "zipped_model_path = sess.upload_data(path=\"hf_model.tar.gz\", key_prefix=prefix + \"/hf-model-sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"womens-ecommerce-reviews-model-{}\".format(\n",
    "    datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    ")\n",
    "endpoint_name = \"womens-ecommerce-reviews-endpoint-{}\".format(\n",
    "    datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    name=model_name,\n",
    "    model_data=zipped_model_path,\n",
    "    role=get_execution_role(),\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    ")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\", endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model endpoint\n",
    "Lets test the model endpoint to ensure that deployment was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence1 = \"A very versatile and cozy top. would look great dressed up or down for a casual comfy fall day. what a fun piece for my wardrobe!\"\n",
    "test_sentence2 = \"Love the color! very soft. unique look. can't wait to wear it this fall\"\n",
    "test_sentence3 = (\n",
    "    \"These leggings are loose fitting and the quality is just not there.. i am returning the item.\"\n",
    ")\n",
    "test_sentence4 = \"Very disappointed the back of this blouse is plain, not as displayed.\"\n",
    "\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, sess)\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.CSVDeserializer()\n",
    "predictor.predict([[test_sentence1], [test_sentence2], [test_sentence3], [test_sentence4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability with SageMaker Clarify for text features\n",
    "\n",
    "Now that the model is ready, and we are able to get predictions, we are ready to get explanations for text data from Clarify processing job. For a detailed example that showcases how to use the Clarify processing job, please refer to [this example](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-clarify/fairness_and_explainability/fairness_and_explainability.ipynb). This example shows how to get explanations for text data from Clarify.\n",
    "\n",
    "In the cell below, we create the CSV file to pass on to the Clarify dataset. We are using 10 samples here to make it fast, but we can use entire dataset at a time. We are also filtering out any reviews with less than 500 characters as long reviews provide better visualization with `sentence` level granularity (When granularity is `sentence`, each sentence is a feature, and we need a few sentences per review for good visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"clarify_data.csv\"\n",
    "num_examples = 10\n",
    "\n",
    "df_test[\"len\"] = df_test[\"Review Text\"].apply(lambda ele: len(ele))\n",
    "\n",
    "df_test_clarify = pd.DataFrame(\n",
    "    df_test[df_test[\"len\"] > 500].sample(n=num_examples, random_state=RANDOM_STATE),\n",
    "    columns=[\"Review Text\"],\n",
    ")\n",
    "df_test_clarify.to_csv(file_path, header=True, index=False)\n",
    "df_test_clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "There are expanding business needs and legislative regulations that require explanations of _why_ a model made the decision it did. SageMaker Clarify uses SHAP to explain the contribution that each input feature makes to the final decision.\n",
    "\n",
    "How does the Kernel SHAP algorithm work? Kernel SHAP algorithm is a local explanation method. That is, it explains each instance or row of the dataset at a time. To explain each instance, it perturbs the features values - that is, it changes the values of some features to a baseline (or non-informative) value, and then get predictions from the model for the perturbed samples. It does this for a number of times per instance (determined by the optional parameter `num_samples` in `SHAPConfig`), and computes the importance of each feature based on how the model prediction changed.\n",
    "\n",
    "We are now extending this functionality to text data. In order to be able to explain text, we need the `TextConfig`. The `TextConfig` is an optional parameter of `SHAPConfig`, which you need to provide if you need explanations for the text features in your dataset. `TextConfig` in turn requires three parameters:\n",
    "1. `granularity` (required): To explain text features, Clarify further breaks down text into smaller text units, and considers each such text unit as a feature. The parameter `granularity` informs the level to which Clarify will break down the text: `token`, `sentence`, or `paragraph` are the allowed values for `granularity`.\n",
    "2. `language` (required): the language of the text features. This is required to tokenize the text to break them down to their granular form.\n",
    "3. `max_top_tokens` (optional): the number of top token attributions that will be shown in the output (we need this because the size of vocabulary can be very big). This is an optional parameter, and defaults to 50.\n",
    "\n",
    "Kernel SHAP algorithm requires a baseline (also known as background dataset). In case of tabular features, the baseline value/s for a feature is ideally a non-informative or least informative value for that feature. However, for text feature, the baseline values must be the value you want to replace the individual text feature (token, sentence or paragraph) with. For instance, in the example below, we have chosen the baseline values for `review_text` as `<UNK>`, and `granularity` is `sentence`. Every time a sentence has to replaced in the perturbed inputs, we will replace it with `<UNK>`.\n",
    "\n",
    "\n",
    "If baseline is not provided, a baseline is calculated automatically by SageMaker Clarify using K-means or K-prototypes in the input dataset for tabular features. For text features, if baseline is not provided, the default replacement value will be the string `<PAD>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, instance_count=1, instance_type=\"ml.m5.xlarge\", sagemaker_session=sess\n",
    ")\n",
    "\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "explainability_output_path = \"s3://{}/{}/clarify-text-explainability\".format(bucket, prefix)\n",
    "explainability_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=file_path,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    headers=[\"Review Text\"],\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = clarify.SHAPConfig(\n",
    "    baseline=[[\"<UNK>\"]],\n",
    "    num_samples=1000,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    "    text_config=clarify.TextConfig(granularity=\"sentence\", language=\"english\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the clarify explainability job involves spinning up a processing job and a model endpoint which may take a few minutes.\n",
    "# After this you will see a progress bar for the SHAP computation.\n",
    "# The size of the dataset (num_examples) and the num_samples for shap will effect the running time.\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize local explanations\n",
    "\n",
    "We use Captum to visualize the feature importances computed by Clarify.\n",
    "First, lets load the local explanations. Local text explanations can be found in the analysis results folder in a file named `out.jsonl` in the `explanations_shap` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_feature_attributions_file = \"out.jsonl\"\n",
    "analysis_results = []\n",
    "analysis_result = sagemaker.s3.S3Downloader.download(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file,\n",
    "    local_path=\"./\",\n",
    ")\n",
    "\n",
    "shap_out = []\n",
    "file = sagemaker.s3.S3Downloader.read_file(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file\n",
    ")\n",
    "for line in file.split(\"\\n\"):\n",
    "    if line:\n",
    "        shap_out.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local explanations file is a JSON Lines file, that contains the explanation of one instance per row. Let's examine the output format of the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(shap_out[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the highest level of this JSON Line, there are two keys: `explanations`, `join_source_value` (Not present here as we have not included a `joinsource` column in the input dataset). `explanations` contains a list of attributions for each feature in the dataset. In this case, we have a single element, because the input dataset also had a single feature. It also contains details like `feature_name`, `data_type` of the features (indicating whether Clarify inferred the column as numerical, categorical or text). Each token attribution also contains a `description` field that contains the token itself, and the starting index of the token in original input. This allows you to reconstruct the original sentence from the output as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, we create a list of attributions and a list of tokens for use in visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_dataset = [\n",
    "    np.array([attr[\"attribution\"][0] for attr in expl[\"explanations\"][0][\"attributions\"]])\n",
    "    for expl in shap_out\n",
    "]\n",
    "tokens_dataset = [\n",
    "    np.array(\n",
    "        [attr[\"description\"][\"partial_text\"] for attr in expl[\"explanations\"][0][\"attributions\"]]\n",
    "    )\n",
    "    for expl in shap_out\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain predictions as well so that they can be displayed alongside the feature attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict([t for t in df_test_clarify.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is a wrapper around the captum that helps produce visualizations for local explanations. It will\n",
    "# visualize the attributions for the tokens with red or green colors for negative and positive attributions.\n",
    "def visualization_record(\n",
    "    attributions,  # list of attributions for the tokens\n",
    "    text,  # list of tokens\n",
    "    pred,  # the prediction value obtained from the endpoint\n",
    "    delta,\n",
    "    true_label,  # the true label from the dataset\n",
    "    normalize=True,  # normalizes the attributions so that the max absolute value is 1. Yields stronger colors.\n",
    "    max_frac_to_show=0.05,  # what fraction of tokens to highlight, set to 1 for all.\n",
    "    match_to_pred=False,  # whether to limit highlights to red for negative predictions and green for positive ones.\n",
    "    # By enabling `match_to_pred` you show what tokens contribute to a high/low prediction not those that oppose it.\n",
    "):\n",
    "    if normalize:\n",
    "        attributions = attributions / max(max(attributions), max(-attributions))\n",
    "    if max_frac_to_show is not None and max_frac_to_show < 1:\n",
    "        num_show = int(max_frac_to_show * attributions.shape[0])\n",
    "        sal = attributions\n",
    "        if pred < 0.5:\n",
    "            sal = -sal\n",
    "        if not match_to_pred:\n",
    "            sal = np.abs(sal)\n",
    "        top_idxs = np.argsort(-sal)[:num_show]\n",
    "        mask = np.zeros_like(attributions)\n",
    "        mask[top_idxs] = 1\n",
    "        attributions = attributions * mask\n",
    "    return visualization.VisualizationDataRecord(\n",
    "        attributions,\n",
    "        pred,\n",
    "        int(pred > 0.5),\n",
    "        true_label,\n",
    "        attributions.sum() > 0,\n",
    "        attributions.sum(),\n",
    "        text,\n",
    "        delta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can customize the following display settings\n",
    "normalize = True\n",
    "max_frac_to_show = 1\n",
    "match_to_pred = False\n",
    "labels = test_dataset[\"Sentiment\"][:num_examples]\n",
    "vis = []\n",
    "for attr, token, pred, label in zip(attributions_dataset, tokens_dataset, preds, labels):\n",
    "    vis.append(\n",
    "        visualization_record(\n",
    "            attr, token, float(pred[0]), 0.0, label, normalize, max_frac_to_show, match_to_pred\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we compiled the record we are finally ready to render the visualization.\n",
    "\n",
    "We see a row per review in the selected dataset. For each row we have the prediction, the label, and the highlighted text. Additionally, we show the total sum of attributions (as attribution score) and its label (as attribution label), which indicates whether it is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualization.visualize_text(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Finally, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
