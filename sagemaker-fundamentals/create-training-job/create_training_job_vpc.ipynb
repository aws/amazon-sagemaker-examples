{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Job in Internet-free Mode\n",
    "\n",
    "If you want to isolate your training data and training container from the rest of the Internet, then you should create the training job in a private subnet. A private subnet is a subnet in your VPC without a route to an Internet Gateway. This means, by default, no inbound calls to your container from the Internet is possible and your container cannot make outbound calls to the Internet. If you need the training container to access your S3 resource, you need to **explicitly** add a VPC endpoint and attach it to the route table of your private subnet to allow traffic to your S3 bucket. \n",
    "\n",
    "In this notebook, you will walk through an example of creating such a training job. you will\n",
    "\n",
    "- Build a simple training image\n",
    "- Set up a VPC\n",
    "- Set up a private subnet in the VPC\n",
    "- Set up a security group in the VPC\n",
    "- Create a training job in your private subnet && security group and watch it to fail (because it cannot access your S3 resource)\n",
    "- Add a VPC endpoint to allow traffic to S3\n",
    "- Create another training job in your private subnet and watch it to succeeed \n",
    "\n",
    "If you are not familiar with VPC security configuration, the following materials can help you\n",
    "- [Security in Amazon Virtual Private Cloud](https://docs.aws.amazon.com/vpc/latest/userguide/security.html)\n",
    "- [Training and Inference Containers in Internet-Free Mode](https://docs.aws.amazon.com/sagemaker/latest/dg/mkt-algo-model-internet-free.html)\n",
    "\n",
    "It's okay if you don't understand everything from the official docs above. The code samples you will see in this notebook will help you grasp those concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import pprint\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions\n",
    "\n",
    "If you are running this notebook on an EC2 instance with an IAM user (you) as the default profile, then you will need policies to allow you to create VPC / Subnet / Secruity group / VPC endpoint. Likewise, if you are running this notebook on a SageMaker notebook instance or Studio, the service role needs to have those permission as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a training image\n",
    "\n",
    "You will follow the same procedure for building a training image as in [this notebook](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create_training_job.ipynb). We will refer to this image as `example-image`. Please go through that notebook if you are not familiar with `CreateTrainingJob` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a repo in your ECR \n",
    "\n",
    "ecr = boto3.client('ecr')\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName='example-image')\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-image:latest .\n",
    "    \n",
    "# test the container\n",
    "python local_test/test_container.py\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-image:latest\n",
    "\n",
    "# Tag the image with the fullname\n",
    "docker tag example-image:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a VPC\n",
    "\n",
    "You can think of Amazon VPC as the traditional network in a data center in the cloud. \n",
    "\n",
    "The following are the key concepts for VPCs: \n",
    "* Virtual private cloud (VPC) — A virtual network dedicated to your AWS account.\n",
    "* Subnet — A range of IP addresses in your VPC.\n",
    "* Route table — A set of rules, called routes, that are used to determine where network traffic is directed.\n",
    "* Internet gateway — A gateway that you attach to your VPC to enable communication between resources in your VPC and the internet.\n",
    "* VPC endpoint — Enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. For more information, see AWS PrivateLink and VPC endpoints.\n",
    "* CIDR block —Classless Inter-Domain Routing. An internet protocol address allocation and route aggregation methodology. For more information, see [Classless Inter-Domain Routing](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation) in Wikipedia.\n",
    "\n",
    "All of these concepts are explained in the [official docs](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VPC in your default region\n",
    "\n",
    "ec2 = boto3.client('ec2')\n",
    "\n",
    "vpc_res = ec2.create_vpc(\n",
    "    CidrBlock='10.0.0.0/20', # 2^(32 - 20) = 4906 private ipv4 addrs\n",
    "    AmazonProvidedIpv6CidrBlock=False,\n",
    "    DryRun=False,\n",
    "    TagSpecifications=[\n",
    "        {\n",
    "            'ResourceType': 'vpc', \n",
    "            'Tags':[\n",
    "                {\n",
    "                    'Key': 'Name',\n",
    "                    'Value': 'hello-world'\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "pp.pprint(vpc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect this VPC in details\n",
    "\n",
    "vpc_des = ec2.describe_vpcs(\n",
    "    VpcIds=[vpc_res['Vpc']['VpcId']]\n",
    "    )\n",
    "pp.pprint(vpc_des['Vpcs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subnet\n",
    "The VPC you just created has the capacity to host 4906 compute instances. Think of the VPC as you just created as the entire data center for your organization. Of course, you did not spin up any instances yet, so you are not billed for 4906 instances (rest assured). Suppose you are running a real data center, part of your cluster might be pubic facing (for example, machines that host your frontend applications), part of your cluster might be insulated from the internet and is only accessible from other machines in your data center (for example, your backend or database servers). You can define the scope of your cluster (public / private) via **subnet**. Using subnet, you can define which part of your VPC (via its CIDR block) are public and which part are private. \n",
    "\n",
    "If want to run a SageMaker training job in network isolation mode, then you will need to pass a private subnet id to the `CreateTrainingJob` API. SageMaker service will then start instances in the private subnet that run your training container. \n",
    "\n",
    "So first off, let's create a private subnet. A subnet is defined within an availability zone, whereas a VPC is defined within a region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subnet and associate it with route table\n",
    "\n",
    "def get_first_availability_zone():\n",
    "    region_name = boto3.Session().region_name\n",
    "    avz_res = ec2.describe_availability_zones(\n",
    "        Filters=[\n",
    "            {\n",
    "                \"Name\": \"region-name\",\n",
    "                \"Values\": [region_name]\n",
    "            }\n",
    "        ],\n",
    "        AllAvailabilityZones=True,\n",
    "    )\n",
    "    \n",
    "    for az in avz_res['AvailabilityZones']:\n",
    "        if az['ZoneType']=='availability-zone':\n",
    "            return az\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def create_subnet(vpc_id, cidr_block, dry_run):\n",
    "    \"\"\"Create a subnet in the first availability zone in your current region\"\"\"\n",
    "    az = get_first_availability_zone()\n",
    "    if az is not None:\n",
    "        subnet_res = ec2.create_subnet(\n",
    "            AvailabilityZone = az['ZoneName'], \n",
    "            VpcId = vpc_id,                    \n",
    "            CidrBlock= cidr_block,\n",
    "            DryRun=dry_run                     \n",
    "        )\n",
    "        return subnet_res\n",
    "    else:\n",
    "        raise \"No availability zone\"\n",
    "        \n",
    "sn_res = create_subnet(\n",
    "    vpc_id=vpc_res['Vpc']['VpcId'],\n",
    "    cidr_block='10.0.0.0/28', # I want 2 ^ (32 - 28) private ipv4 in this subnet\n",
    "    dry_run=False)\n",
    "\n",
    "pp.pprint(sn_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a security group\n",
    "A [security group](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html) is another layer of security configuration for instances running in your VPC. It acts as a firewall for your instance that controls its inbound and outbound calls. You need a security group for a SageMaker training job, because in complicated training job that involves distributed training, you need a security group configuration that allows traffics between instances that runs the training job. For the purpose of this notebook, the default setting of a security group (deny all inbound traffic; allow all outbound traffic) is enough. For more complicated training job, you will need to configure the security group accordingly. This will be discussed in more advanced notebooks for `CreateTrainingJob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a security group\n",
    "\n",
    "sg_res = ec2.create_security_group(\n",
    "    Description='security group for SageMaker instances',\n",
    "    GroupName='sagemaker-private',\n",
    "    VpcId=vpc_res['Vpc']['VpcId'],\n",
    "    TagSpecifications=[\n",
    "        {\n",
    "            \"ResourceType\": \"security-group\",\n",
    "            \"Tags\" : [\n",
    "                {   \n",
    "                    \"Key\": \"Service\", # Tag the sec gp by service, this can be used to filter sec gps\n",
    "                    \"Value\": \"SageMaker\" \n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "pp.pprint(sg_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the security group in detail\n",
    "\n",
    "ec2.describe_security_groups(\n",
    "    GroupIds=[\n",
    "        sg_res['GroupId']\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat a training job\n",
    "Now let's create a training job within your private subnet you just created. First, let's download some helper functions for creating service role for SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=$(ls . | grep iam_helpers.py)\n",
    "\n",
    "if [ -f \"$file\" ]\n",
    "then\n",
    "    rm $file\n",
    "fi\n",
    "\n",
    "wget https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/sagemaker-fundamentals/sagemaker-fundamentals/execution-role/iam_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up service role for SageMaker\n",
    "from iam_helpers import create_execution_role\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "sts = boto3.client('sts')\n",
    "caller = sts.get_caller_identity()\n",
    "\n",
    "if ':user/' in caller['Arn']: # as IAM user\n",
    "    # either paste in a role_arn with or create a new one and attach \n",
    "    # AmazonSageMakerFullAccess\n",
    "    role_name = 'example-sm'\n",
    "    role_arn = create_execution_role(role_name=role_name)['Role']['Arn']\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "    )\n",
    "elif 'assumed-role' in caller['Arn']: # on SageMaker infra\n",
    "    role_arn = caller['Arn']\n",
    "else:\n",
    "    print(\"I assume you are on an EC2 instance launched with an IAM role\")\n",
    "    role_arn = caller['Arn']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpers\n",
    "def current_time():\n",
    "    ct = datetime.datetime.now() \n",
    "    return str(ct.now()).replace(\":\", \"-\").replace(\" \", \"-\")[:19]\n",
    "\n",
    "def account_id():\n",
    "    return boto3.client('sts').get_caller_identity()['Account']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this notebook self-contained, you will create a bucket and upload some data there to pass to training container as you did in the [basic create training job notebook](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create_training_job.ipynb). But you don't have to do so, if you already have a bucket that SageMaker service can access (i.e. a bucket with bucket name containing `sagemaker`, see `AmazonSageMakerFullAccessPolicy`), then you can use that bucket as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bucket for SageMaker in your region\n",
    "\n",
    "def create_bucket():\n",
    "    \"\"\"Create an S3 bucket that is intended to be used for short term\"\"\"\n",
    "    bucket = f\"sagemaker-{current_time()}\"\n",
    "    \n",
    "    region_name = boto3.Session().region_name\n",
    "    create_bucket_config = {}\n",
    "    if region_name != 'us-east-1': \n",
    "        # us-east-1 is the default region for S3 bucket\n",
    "        # specify LocationConstraint if your VPC is not\n",
    "        # in us-east-1\n",
    "        create_bucket_config['LocationConstraint'] = region_name\n",
    "    \n",
    "    boto3.client('s3').create_bucket(\n",
    "        Bucket=bucket,\n",
    "        CreateBucketConfiguration=create_bucket_config\n",
    "    )\n",
    "    return bucket\n",
    "\n",
    "# replace it with your own SageMaker-accessible bucket \n",
    "# if you don't want to create a new one\n",
    "\n",
    "bucket = create_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload some mock data to your bucket\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "input_prefix = 'input_data'\n",
    "\n",
    "for fname in os.listdir('data'):\n",
    "    with open(os.path.join('data', fname), 'rb') as f:\n",
    "        key = input_prefix + fname\n",
    "        s3.upload_fileobj(f, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will configure the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_cli = boto3.client('sagemaker')\n",
    "\n",
    "\n",
    "# name training job\n",
    "training_job_name = 'example-training-job-{}'.format(current_time())\n",
    "\n",
    "data_path = \"s3://\" + bucket + '/' + input_prefix\n",
    "\n",
    "# location that SageMaker saves the model artifacts\n",
    "output_prefix = 'output/'\n",
    "output_path = \"s3://\" + bucket + '/' + output_prefix\n",
    "\n",
    "# ECR URI of your image\n",
    "region = boto3.Session().region_name\n",
    "account = account_id()\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/example-image:latest\".format(account, region)\n",
    "\n",
    "algorithm_specification = {\n",
    "    'TrainingImage': image_uri,\n",
    "    'TrainingInputMode': 'File',\n",
    "}\n",
    "\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        'ChannelName': 'train',\n",
    "            'DataSource':{\n",
    "                'S3DataSource':{\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': data_path,\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "        }\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'ChannelName': 'test',\n",
    "        'DataSource':{\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': data_path,\n",
    "                'S3DataDistributionType': 'FullyReplicated',\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "    \n",
    "vpc_config = {\n",
    "    # security groups need to be configured to communicate \n",
    "    # with each other for distributed training job\n",
    "    'SecurityGroupIds' : [\n",
    "         sg_res['GroupId']\n",
    "    ],\n",
    "    'Subnets': [\n",
    "        sn_res['Subnet']['SubnetId']\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': output_path\n",
    "}\n",
    "\n",
    "resource_config = {\n",
    "    'InstanceType': 'ml.m5.large',\n",
    "    'InstanceCount':1,\n",
    "    'VolumeSizeInGB':5\n",
    "}\n",
    "\n",
    "stopping_condition={\n",
    "    'MaxRuntimeInSeconds':120,\n",
    "}\n",
    "\n",
    "enable_network_isolation=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_res = sm_cli.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    AlgorithmSpecification=algorithm_specification,\n",
    "    RoleArn=role_arn,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    VpcConfig=vpc_config,\n",
    "    ResourceConfig=resource_config,\n",
    "    StoppingCondition=stopping_condition,\n",
    "    EnableNetworkIsolation=enable_network_isolation,\n",
    "    EnableManagedSpotTraining=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job is expected to fail, because the subnet you created is isolated from the Internet and you have not created any mechanism for it to access your the data in your S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the training job to fail\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    tj_state = sm_cli.describe_training_job(TrainingJobName=training_job_name)\n",
    "    \n",
    "    if tj_state['TrainingJobStatus'] in ['Completed', 'Stopped', 'Failed']:\n",
    "        stopped=True\n",
    "    else:\n",
    "        print(\"Training in progress\")\n",
    "        time.sleep(30)\n",
    "\n",
    "if tj_state['TrainingJobStatus'] == 'Failed':\n",
    "    print(\"Training job failed \")\n",
    "    print(\"Failed Reason: {}\".format(tj_state['FailureReason']))\n",
    "else:\n",
    "    print(\"Training job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a VPC endpoint\n",
    "\n",
    "A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. **Traffic between your VPC and the other service does not leave the Amazon network**. For more information, see [AWS PrivateLink and VPC endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-services-overview.html). \n",
    "\n",
    "There are three types of VPC endpoints as of March 2021.\n",
    "\n",
    "A **Gateway** endpoint serves as a target for a route in your route table for traffic destined for the AWS service. You can specify an endpoint policy to attach to the endpoint, which will control access to the service from your VPC. You can also specify the VPC route tables that use the endpoint.\n",
    "\n",
    "An **Interface** endpoint is a network interface in your subnet that serves as an endpoint for communicating with the specified service. You can specify the subnets in which to create an endpoint, and the security groups to associate with the endpoint network interface.\n",
    "\n",
    "A **GatewayLoadBalancer** endpoint is a network interface in your subnet that serves an endpoint for communicating with a Gateway Load Balancer that you've configured as a VPC endpoint service.\n",
    "\n",
    "---\n",
    "Only Gateway endpoint is a viable option for SageMaker service. So you will add a Gateway endpoint here. A Gateway endpoint needs to be added to a route table, so you will need to create a route table and associated it with your subnet first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a route table\n",
    "rt_res = ec2.create_route_table(\n",
    "    VpcId=vpc_res['Vpc']['VpcId'],\n",
    "    TagSpecifications=[\n",
    "        {\n",
    "            \"ResourceType\": 'route-table',\n",
    "            'Tags': [\n",
    "                {\n",
    "                    'Key': 'Service',\n",
    "                    'Value': 'SageMaker'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "pp.pprint(rt_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the route table with the subnet\n",
    "\n",
    "ass_rt_res = ec2.associate_route_table(\n",
    "    RouteTableId=rt_res['RouteTable']['RouteTableId'],\n",
    "    SubnetId=sn_res['Subnet']['SubnetId']\n",
    ")\n",
    "\n",
    "pp.pprint(ass_rt_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check service name of S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out service name for S3\n",
    "services = ec2.describe_vpc_endpoint_services()\n",
    "for s in services['ServiceNames']:\n",
    "    if 's3' in s:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gateway endpoint \n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "\n",
    "iep_res = ec2.create_vpc_endpoint(\n",
    "    VpcEndpointType='Gateway',\n",
    "    VpcId=vpc_res['Vpc']['VpcId'],\n",
    "    ServiceName=f'com.amazonaws.{region_name}.s3', # return of previous cell\n",
    "    RouteTableIds=[rt_res['RouteTable']['RouteTableId']],\n",
    "    \n",
    "    # you don't need to add a tag, it is only\n",
    "    # used as a convenient way to filter through your \n",
    "    # endpoints in the future \n",
    "    TagSpecifications=[    \n",
    "        {\n",
    "            'ResourceType': 'vpc-endpoint',\n",
    "            'Tags' : [\n",
    "                {\n",
    "                    'Key' : 'Service',\n",
    "                    'Value': 'SageMaker'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "pp.pprint(iep_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have added a Gateway endpoint to the route table of the subnet. This endpoint allows the subnet to talk to your S3 bucket **privately**. The traffic between the subnet and your S3 bucket does not leave AWS network. Let's create another training job to verify that the training container can access the data in your S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = 'example-training-job-{}'.format(current_time())\n",
    "\n",
    "ct_res = sm_cli.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    AlgorithmSpecification=algorithm_specification,\n",
    "    RoleArn=role_arn,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    VpcConfig=vpc_config,\n",
    "    ResourceConfig=resource_config,\n",
    "    StoppingCondition=stopping_condition,\n",
    "    EnableNetworkIsolation=enable_network_isolation,\n",
    "    EnableManagedSpotTraining=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch to to succeed\n",
    "\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    tj_state = sm_cli.describe_training_job(TrainingJobName=training_job_name)\n",
    "    \n",
    "    if tj_state['TrainingJobStatus'] in ['Completed', 'Stopped', 'Failed']:\n",
    "        stopped=True\n",
    "    else:\n",
    "        print(\"Training in progress\")\n",
    "        time.sleep(30)\n",
    "\n",
    "if tj_state['TrainingJobStatus'] == 'Failed':\n",
    "    print(\"Training job failed \")\n",
    "    print(\"Failed Reason: {}\".format(tj_state['FailureReason']))\n",
    "else:\n",
    "    print(\"Training job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Let's review what you did in this notebook: you have created \n",
    "- a VPC\n",
    "- a subnet inside the VPC\n",
    "- a security group inside the VPC\n",
    "\n",
    "The VPC is isolated from the Internet, because you did not add an Internet Gateway to it. \n",
    "You created a training job in the subnet. The traffic in and out the SageMaker Instance running your training container is controlled by the security group permissions. You verified that this training job failed, because SageMaker cannot download data from your S3 bucket. \n",
    "\n",
    "Next, you added \n",
    "- a route table to your subnet\n",
    "- an S3 Gateway Endpoint to the route table\n",
    "\n",
    "Then you verified that once you added the S3 Gateway Endpoint to your VPC, the same training job can go through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical considerations\n",
    "If you are an ML practioner, then most likely you will not need to touch VPC, because the network admin in your organization should have configured the VPC, subnet, security group, route table and VPC endpoints for you. The reason we discussed VPC configuration in this notebook is to get you familiar with the basic concepts of network engineering, so that when something goes wrong, you can message your network admin with more precise questions or requests. \n",
    "\n",
    "One common situation is that your org owns a VPC has has both public and private subnet. You are configuring a SageMaker training job on an EC2 / Notebook Instance / Studio in the public subnet and you want the training job to be executed in the private subnet. In that case, all you need to to is to pass the subnet id and security group id to the `CreateTrainingJob` API and set the `EnableNetworkIsolation` flag to `True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up \n",
    "Now, let's tear down all resources you created in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the entire VPC and its associated resources \n",
    "# adapted from https://gist.github.com/alberto-morales/b6d7719763f483185db27289d51f8ec5\n",
    "\n",
    "def vpc_cleanup(vpcid):\n",
    "    \"\"\"Remove VPC from AWS\n",
    "    Set your region/access-key/secret-key from env variables or boto config.\n",
    "    :param vpcid: id of vpc to delete\n",
    "    \"\"\"\n",
    "    if not vpcid:\n",
    "        return\n",
    "    print('Removing VPC ({}) from AWS'.format(vpcid))\n",
    "    ec2 = boto3.resource('ec2')\n",
    "    ec2client = ec2.meta.client\n",
    "    vpc = ec2.Vpc(vpcid)\n",
    "    # detach default dhcp_options if associated with the vpc\n",
    "    dhcp_options_default = ec2.DhcpOptions('default')\n",
    "    if dhcp_options_default:\n",
    "        dhcp_options_default.associate_with_vpc(\n",
    "            VpcId=vpc.id\n",
    "        )\n",
    "    # detach and delete all gateways associated with the vpc\n",
    "    for gw in vpc.internet_gateways.all():\n",
    "        vpc.detach_internet_gateway(InternetGatewayId=gw.id)\n",
    "        gw.delete()\n",
    "\n",
    "    # delete any instances\n",
    "    for subnet in vpc.subnets.all():\n",
    "        for instance in subnet.instances.all():\n",
    "            instance.terminate()\n",
    "    \n",
    "    # delte all subnets\n",
    "    for subnet in vpc.subnets.all():\n",
    "        for interface in subnet.network_interfaces.all():\n",
    "            interface.delete()\n",
    "        subnet.delete()\n",
    "    \n",
    "    # delete all route table associations\n",
    "    for rt in vpc.route_tables.all():\n",
    "        for rta in rt.associations:\n",
    "            if not rta.main:\n",
    "                rta.delete()\n",
    "            \n",
    "        try:\n",
    "            rt.delete()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # delete our endpoints\n",
    "    for ep in ec2client.describe_vpc_endpoints(\n",
    "            Filters=[{\n",
    "                'Name': 'vpc-id',\n",
    "                'Values': [vpcid]\n",
    "            }])['VpcEndpoints']:\n",
    "        ec2client.delete_vpc_endpoints(VpcEndpointIds=[ep['VpcEndpointId']])\n",
    "    # delete our security groups\n",
    "    for sg in vpc.security_groups.all():\n",
    "        if sg.group_name != 'default':\n",
    "            sg.delete()\n",
    "    # delete any vpc peering connections\n",
    "    for vpcpeer in ec2client.describe_vpc_peering_connections(\n",
    "            Filters=[{\n",
    "                'Name': 'requester-vpc-info.vpc-id',\n",
    "                'Values': [vpcid]\n",
    "            }])['VpcPeeringConnections']:\n",
    "        ec2.VpcPeeringConnection(vpcpeer['VpcPeeringConnectionId']).delete()\n",
    "    # delete non-default network acls\n",
    "    for netacl in vpc.network_acls.all():\n",
    "        if not netacl.is_default:\n",
    "            netacl.delete()\n",
    " \n",
    "    # finally, delete the vpc\n",
    "    ec2client.delete_vpc(VpcId=vpcid)\n",
    "    return\n",
    "\n",
    "vpc_cleanup(vpc_res['Vpc']['VpcId'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
