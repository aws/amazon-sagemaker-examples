{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2003c7cd",
   "metadata": {},
   "source": [
    "# Benchmarking PyTorch NLP Model with NVIDIA Triton Inference Server and Inference Recommender on Amazon SageMaker\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "Use Amazon SageMaker notebook instance to execute this notebook, this will not work on Amazon SageMaker Studio notebooks\n",
    "</div>\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker Inference recommender to perform custom load testing in order to performance fine tune the NLP BERT Model serving using NVIDIA Triton Serving on SageMaker.\n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service for data science and machine learning workflows. It helps data scientists and developers to prepare, build, train, and deploy high-quality ML models quickly by bringing together a broad set of capabilities purpose-built for ML.\n",
    "\n",
    "Now, [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/) can be used to serve models for inference in Amazon SageMaker. Thanks to the new NVIDIA Triton container image, you can easily serve ML models and benefit from the performance optimizations, dynamic batching, and multi-framework support provided by NVIDIA Triton. Triton helps maximize the utilization of GPU and CPU, further lowering the cost of inference.\n",
    "\n",
    "SageMaker Inference Recommender is a new capability of SageMaker that reduces the time required to get machine learning (ML) models in production by automating performance benchmarking and load testing models across SageMaker ML instances. You can use Inference Recommender to deploy your model to a real-time inference endpoint that delivers the best performance at the lowest cost.\n",
    "\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `ml.g4dn.8xlarge` with 50 GB EBS volume.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Pricing </strong>\n",
    "Estimated cost to execute this notebook is < $15 (assuming notebook executes for 5 hours). This does not include the cost of instances thats provisioned using Amazon SageMaker Inference Recommender during custom load test. The cost is based on instance usage, there is no additional fee for SageMaker Inference Recommender\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca099a6",
   "metadata": {},
   "source": [
    "## Introduction to NVIDIA Triton Server\n",
    "\n",
    "[NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/) was developed specifically to enable scalable, cost-effective, and easy deployment of models in production. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance.\n",
    "\n",
    "Some key features of Triton are:\n",
    "* **Support for Multiple frameworks**: Triton can be used to deploy models from all major frameworks. Triton supports TensorFlow GraphDef, TensorFlow SavedModel, ONNX, PyTorch TorchScript, TensorRT, RAPIDS FIL for tree based models, and OpenVINO model formats. \n",
    "* **Model pipelines**: Triton model ensemble represents a pipeline of one or more models or pre/post processing logic and the connection of input and output tensors between them. A single inference request to an ensemble will trigger the execution of the entire pipeline.\n",
    "* **Concurrent model execution**: Multiple models (or multiple instances of the same model) can run simultaneously on the same GPU or on multiple GPUs for different model management needs.\n",
    "* **Dynamic batching**: For models that support batching, Triton has multiple built-in scheduling and batching algorithms that combine individual inference requests together to improve inference throughput. These scheduling and batching decisions are transparent to the client requesting inference.\n",
    "* **Diverse CPUs and GPUs**: The models can be executed on CPUs or GPUs for maximum flexibility and to support heterogeneous computing requirements.\n",
    "\n",
    "**Note**: This initial release of NVIDIA Triton on SageMaker will only support a single model. Future releases will have multi-model support. A minimal `config.pbtxt` configuration file is **required** in the model artifacts. This release doesn't support inferring the model config automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123dd20",
   "metadata": {},
   "source": [
    "#### Install packages\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fade3a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.24.38 which is incompatible.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[http] in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.17.0)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tritonclient[http]) (1.19.5)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tritonclient[http]) (1.5)\n",
      "Requirement already satisfied: geventhttpclient>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tritonclient[http]) (1.5.3)\n",
      "Requirement already satisfied: brotli in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (1.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (1.15.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (21.1.2)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (2021.5.30)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (49.6.0.post20210108)\n",
      "Requirement already satisfied: zope.event in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (4.5.0)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (5.2.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (0.4.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker transformers==4.9.1\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44721d48",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f83ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# triton\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# custom CloudWatch\n",
    "from cloudwatch import get_endpoint_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc1024",
   "metadata": {},
   "source": [
    "#### Set Variables\n",
    "\n",
    "We set SageMaker variables and other variables below, also define the IAM role that will give Amazon SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c69b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role: arn:aws:iam::917092859813:role/service-role/viridian-sagemaker-notebook-SageMakerExecutionRole-1KL5F03PT23NN\n",
      "Region Name: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/pt-triton-inference-recommender\"\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "cw_client = boto3.client(\"cloudwatch\", region)\n",
    "\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region Name: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf82dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Model Name: pt-triton-benchmark-model-2022-04-11-19-33-14\n",
      "SageMaker Mode Package Name: pt-triton-benchmark-model-group-2022-04-11-19-33-14\n",
      "SageMaker Advanced Job Name: pt-triton-benchmark-advanced-job-2022-04-11-19-33-14\n"
     ]
    }
   ],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = \"pt-triton-benchmark-model-\" + ts\n",
    "model_package_group_name = \"pt-triton-benchmark-model-group-\" + ts\n",
    "advanced_job = \"pt-triton-benchmark-advanced-job-\" + ts\n",
    "\n",
    "print(f\"SageMaker Model Name: {sm_model_name}\")\n",
    "print(f\"SageMaker Mode Package Name: {model_package_group_name}\")\n",
    "print(f\"SageMaker Advanced Job Name: {advanced_job}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81bdc86",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker Triton Inference Server Deep Learning Container Image\n",
    "\n",
    "Let's retrieve Amazon SageMaker NVIDIA Triton Inference server container image based on the account ID you are running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ee414",
   "metadata": {},
   "source": [
    "Set `triton_image_uri` based on the `account_id` and `region` information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7d7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton Inference server DLC image: 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:21.08-py3\n"
     ]
    }
   ],
   "source": [
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:21.08-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "\n",
    "print(f\"Triton Inference server DLC image: {triton_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e955c68",
   "metadata": {},
   "source": [
    "## NLP Use case\n",
    "\n",
    "Deploying and scaling NLP models in a production set up can be quite challenging. NLP models are often very large in size, containing millions of model parameters. Optimal model configurations are required to satisfy stringent performance and scalability of production grade NLP applications\n",
    "\n",
    "In this notebook, we will benchmark a NLP use case using SageMaker Triton inference server and recommend performance tuning optimizations for the below NLP profile. We will use a large pre-trained transformer based `bert-large-uncased` model which has about 336 million model parameters. The input sentence used for the binary classification model will be padded and truncated to a maximum input sequence length 512 tokens. The inference load test will simulate to achieve 500 TPS (30000 maximum invocations per minute) and model latency of < 0.5 seconds (500 milliseconds)\n",
    "\n",
    "## NVIDIA Triton Setup with Amazon SageMaker\n",
    "\n",
    "1. We will use this [generate_models.sh](./workspace/generate_models.sh) to generate the `bert-large-uncased` model to be used with NVIDIA Triton inference server.\n",
    "2. The script for loading the pre-trained `bert_large_uncased` model and saving it can be found in this [pt_exporter.py](./workspace/pt_exporter.py)\n",
    "3. Pre-trained model is loaded in torchscript format and model artifacts are jit traced with a dummy input and store in model.pt format\n",
    "4. After the model is serialized we package it into the format that Triton and SageMaker expect it to be.\n",
    "5. We used the pre-configured `config.pbtxt` file provided with this repo to specify model [configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load the model. \n",
    "6. We tar the model directory and upload it to s3 to later create a [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html).\n",
    "\n",
    "\n",
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bert\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.pt\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1887923",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> NOTE </strong>\n",
    "The below script uses docker and thus will not work on Amazon SageMaker Studio notebook. Please use Amazon SageMaker Notebook instance to execute this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3483cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "== PyTorch ==\n",
      "=============\n",
      "\n",
      "NVIDIA Release 21.08 (build 26011915)\n",
      "PyTorch Version 1.10.0a0+3fd9dcf\n",
      "\n",
      "Container image Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Copyright (c) 2014-2021 Facebook Inc.\n",
      "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n",
      "Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n",
      "Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n",
      "Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n",
      "Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n",
      "Copyright (c) 2015      Google Inc.\n",
      "Copyright (c) 2015      Yangqing Jia\n",
      "Copyright (c) 2013-2016 The Caffe contributors\n",
      "All rights reserved.\n",
      "\n",
      "NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: MOFED driver for multi-node communication was not detected.\n",
      "      Multi-node communication performance may be reduced.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --ipc=host ...\n",
      "\n",
      "Collecting transformers==4.9.1\n",
      "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.6 MB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (1.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (4.62.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (2021.8.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (2.26.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 107.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (0.0.45)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (3.0.12)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers==4.9.1) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers==4.9.1) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers==4.9.1) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.9.1) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.9.1) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.9.1) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.9.1) (1.26.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.9.1) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.9.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.9.1) (1.0.1)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.12 tokenizers-0.10.3 transformers-4.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Using cuda device\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00<00:00, 769kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [00:12<00:00, 105MB/s]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n",
      "Saved model.pt\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:21.08-py3 \\\n",
    "            /bin/bash generate_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae9a54",
   "metadata": {},
   "source": [
    "The script saves the model in this [workspace](./workspace/) directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa80d9",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "- Each model in a model repository must include a model configuration that provides required and optional information about the model. Typically, this configuration is provided in a `config.pbtxt` file specified as `ModelConfig protobuf`. \n",
    "- The model configuration name property is optional. If the name of the model is not specified in the configuration it is assumed to be the same as the model repository directory containing the model. If name is specified it must match the name of the model repository directory containing the model\n",
    "- The `max_batch_size` property indicates the maximum batch size that the model supports for the types of batching that can be exploited by Triton. If the model's batch dimension is the first dimension, and all inputs and outputs to the model have this batch dimension, then Triton can use its dynamic batcher or sequence batcher to automatically use batching with the model. In this case `max_batch_size` should be set to a value greater-or-equal-to 1 that indicates the maximum batch size that Triton should use with the model\n",
    "- Each model input and output must specify a name, datatype, and shape. The name specified for an input or output tensor must match the name expected by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0f9bb",
   "metadata": {},
   "source": [
    "The below is the baseline configuration for PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988beb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb52e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing triton-serve-pt/bert/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile triton-serve-pt/bert/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e44bf2",
   "metadata": {},
   "source": [
    "You can find other model configurations that are used in the benchmarking exercise in this [workspaces](./workspaces) directory.\n",
    "\n",
    "1. config-dg.pbtxt (Dynamic batching enabled)\n",
    "2. config-ig.pbtxt (Multiple instance group)\n",
    "3. config-db-ig.pbtxt (Dynamic batching, Multiple instance group enabled)\n",
    "\n",
    "To execute benchmarking load using Amazon SageMaker Inference recommender copy above files with `config.pbtxt` and reupload the model tar files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6561d71",
   "metadata": {},
   "source": [
    "We will copy the `model.pt` file and create model tar file to be uploaded to S3. The model archive tar file will be used by Triton Inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ff7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/bert/1/\n",
    "!cp workspace/model.pt triton-serve-pt/bert/1/\n",
    "!tar -C triton-serve-pt/ -czf model.tar.gz bert\n",
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-serve-pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fb436",
   "metadata": {},
   "source": [
    "### Create Amazon SageMaker Real Time Endpoint\n",
    "\n",
    "We start off by creating a [sagemaker model](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3**. This variable is optional in case of a single model. In case of ensemble models, this key **has to be** specified for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts.\n",
    "\n",
    "*Note*: The current release of Triton (21.08-py3) on SageMaker doesn't support running instances of different models on the same server, except in case of [ensembles](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models). Only multiple model instances of the same model are supported, which can be specified under the [instance-groups](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#instance-groups) section of the config.pbtxt file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8377b",
   "metadata": {},
   "source": [
    "###  Create payload \n",
    "\n",
    "Create payload in JSON format and upload it on S3. This will be used by Inference Recommender to run the custom load test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c91f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    enc = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e71b9",
   "metadata": {},
   "source": [
    "If you want to change the payload (Token Length), below are the changes -\n",
    "1. Change the JSON with shape reflecting the right token length below\n",
    "2. Change the tokenize_text method to reflect the token length\n",
    "3. Change the config.pbtxt the triton* folder to reflect the input id and attention mask length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdd1304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample payload to be used with Inference Recommender\n",
      "{'inputs': [{'name': 'INPUT__0', 'shape': [1, 512], 'datatype': 'INT32', 'data': [101, 3443, 18093, 1046, 3385, 1998, 2039, 11066, 2009, 2006, 1055, 2509, 1012, 2023, 2097, 2022, 2109, 2011, 28937, 16755, 2121, 2000, 2448, 1996, 7170, 3231, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'name': 'INPUT__1', 'shape': [1, 512], 'datatype': 'INT32', 'data': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]}\n",
      "Directory Created ./sample-payload/\n",
      "request.json\r\n"
     ]
    }
   ],
   "source": [
    "text_triton = \"\"\"\n",
    "                Create payload JSON and upload it on S3. \n",
    "                This will be used by Inference Recommender to run the load test.\n",
    "              \"\"\"\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(text_triton)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, 512], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, 512], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Sample payload to be used with Inference Recommender\")\n",
    "print(payload)\n",
    "\n",
    "payload_location = \"./sample-payload/\"\n",
    "\n",
    "if not os.path.exists(payload_location):\n",
    "    os.makedirs(payload_location)\n",
    "    print(f\"Directory Created {payload_location}\")\n",
    "else:\n",
    "    print(f\"Directory already exists {payload_location}\")\n",
    "\n",
    "payload_archive_name = \"payload.tar.gz\"\n",
    "\n",
    "f = open(payload_location + \"request.json\", \"w\")\n",
    "json.dump(payload, f)\n",
    "f.close()\n",
    "\n",
    "!cd ./sample-payload/ && tar czvf ../payload.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc3e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Payload location in S3:  s3://sagemaker-us-east-1-917092859813/sagemaker/pt-triton-inference-recommender/inference/payload.tar.gz\n",
      "Model archive location: s3://sagemaker-us-east-1-917092859813/sagemaker/pt-triton-inference-recommender/model/model.tar.gz\n",
      "CPU times: user 7.11 s, sys: 6.27 s, total: 13.4 s\n",
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_payload_url = sagemaker.Session().upload_data(\n",
    "    payload_archive_name, bucket=bucket, key_prefix=prefix + \"/inference\"\n",
    ")\n",
    "\n",
    "model_archive_name = \"model.tar.gz\"\n",
    "model_url = sagemaker.Session().upload_data(\n",
    "    model_archive_name, bucket=bucket, key_prefix=prefix + \"/model\"\n",
    ")\n",
    "\n",
    "print(f\"Sample Payload location in S3:  {sample_payload_url}\")\n",
    "print(f\"Model archive location: {model_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0dcab",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Inference Recommender set up\n",
    "\n",
    "Set the Domain, Task, Framework, version and Model for Inference Recommender Job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95be9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_domain = \"NATURAL_LANGUAGE_PROCESSING\"\n",
    "ml_task = \"FILL_MASK\"\n",
    "ml_framework = \"PYTORCH\"\n",
    "framework_version = \"1.6.0\"\n",
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd5cb30",
   "metadata": {},
   "source": [
    "Create the Triton Container Dictionary object and Model Package group for Inference recommender Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e20d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_url,\n",
    "    \"NearestModelName\": model,\n",
    "    \"Framework\": ml_framework,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"bert\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e41aaa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Registry package group: {'ModelPackageGroupArn': 'arn:aws:sagemaker:us-east-1:917092859813:model-package-group/pt-triton-benchmark-model-group-2022-04-11-19-33-14', 'ResponseMetadata': {'RequestId': 'c3ad3734-1fa9-49fb-a76d-00c8aa8fe624', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c3ad3734-1fa9-49fb-a76d-00c8aa8fe624', 'content-type': 'application/x-amz-json-1.1', 'content-length': '139', 'date': 'Mon, 11 Apr 2022 19:35:15 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageGroupDescription=\"BERT large uncased Model group for Triton Serving\",\n",
    ")\n",
    "print(f\"Model Registry package group: {model_pacakge_group_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8762c3a",
   "metadata": {},
   "source": [
    "Amazon SageMaker model registry model package with domain, task and Inference container specification information. Specify the list of supported inference instance types in `SupportedRealtimeInferenceInstanceTypes` parameter. Also, define the `ContentType` and MIME type information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0288c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_version_response = sm_client.create_model_package(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageDescription=\"BERT large uncased Model group for Triton Serving\",\n",
    "    Domain=ml_domain,\n",
    "    Task=ml_task,\n",
    "    SamplePayloadUrl=sample_payload_url,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [container],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\n",
    "            \"ml.g4dn.4xlarge\",\n",
    "            \"ml.g4dn.8xlarge\",\n",
    "            \"ml.g4dn.16xlarge\",\n",
    "            \"ml.g4dn.12xlarge\",\n",
    "            \"ml.g4dn.xlarge\",\n",
    "            \"ml.g4dn.2xlarge\",\n",
    "        ],\n",
    "        \"SupportedContentTypes\": [\"application/octet-stream\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"application/json\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a6466",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Inference Recommender Custom Load Test\n",
    "\n",
    "Create Custom Inference Recommender Job for Triton Container serving BERT Model with 512 Token length\n",
    "\n",
    "Use the `create_inference_recommendations_job` to create an Inference Recommender load test and specify below parameters\n",
    "- Specify Advanced for the JobType field and provide:\n",
    "- A job name for your load test (JobName). \n",
    "- The Amazon Resource Name (ARN) of an IAM role that enables Inference Recommender to perform tasks on your behalf.\n",
    "- A traffic pattern of the load test (TrafficPattern)\n",
    "    - Initial number of users = 2\n",
    "    - Spawn Rate = 3 (creates 3 new users every 3 minutes for a duration of 15 minutes)\n",
    "- An endpoint configuration dictionary (InputConfig) where you specify an AWS instance type against which to run benchmarks\n",
    "- StoppingConditions (Inference recommender would adjust the initial number of instances to satisfy below stopping conditions)\n",
    "    - MaxInvocations is set to 30000 \n",
    "    - ModelLatencyThresholds p95 threshold for 500 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8ecc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-1:917092859813:inference-recommendations-job/pt-triton-benchmark-advanced-job-2022-04-11-19-33-14', 'ResponseMetadata': {'RequestId': 'e4a192b0-d6c5-4f65-8c5d-427f9c6b2229', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e4a192b0-d6c5-4f65-8c5d-427f9c6b2229', 'content-type': 'application/x-amz-json-1.1', 'content-length': '136', 'date': 'Mon, 11 Apr 2022 19:35:15 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "advanced_response = sm_client.create_inference_recommendations_job(\n",
    "    JobName=advanced_job,\n",
    "    JobDescription=\"nlp triton Inference Advanced Recommender Job\",\n",
    "    JobType=\"Advanced\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        \"ModelPackageVersionArn\": model_package_version_response[\"ModelPackageArn\"],\n",
    "        \"JobDurationInSeconds\": 7200,\n",
    "        \"EndpointConfigurations\": [\n",
    "            {\"InstanceType\": \"ml.p3.8xlarge\"},\n",
    "            {\"InstanceType\": \"ml.p3.2xlarge\"},\n",
    "            {\"InstanceType\": \"ml.p3.16xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.8xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.4xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.2xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.16xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.12xlarge\"},\n",
    "        ],\n",
    "        \"TrafficPattern\": {\n",
    "            \"TrafficType\": \"PHASES\",\n",
    "            \"Phases\": [\n",
    "                {\n",
    "                    \"InitialNumberOfUsers\": 2,\n",
    "                    \"SpawnRate\": 3,\n",
    "                    \"DurationInSeconds\": 900,\n",
    "                },  # simulating 50 users, 2 initial and 3 new users every minute for 16 minutes\n",
    "            ],  # second phase, we will strt with 50 users, steady traffic for 5 minutes\n",
    "        },\n",
    "        \"ResourceLimit\": {\"MaxNumberOfTests\": 10, \"MaxParallelOfTests\": 5},\n",
    "    },\n",
    "    StoppingConditions={\n",
    "        \"MaxInvocations\": 30000,\n",
    "        \"ModelLatencyThresholds\": [{\"Percentile\": \"P95\", \"ValueInMilliseconds\": 500}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(advanced_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9d79a",
   "metadata": {},
   "source": [
    "Let's get the inference recommender job details using `describe_inference_recommendations_job` boto3 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5675477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ended = False\n",
    "while not ended:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(advanced_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        print(f\"Inference recommender job status: {inference_recommender_job['Status']} \")\n",
    "        ended = True\n",
    "    else:\n",
    "        print(\"Inference recommender job in progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".inference_recommender_job[\"FailedReason\"])\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e3a74",
   "metadata": {},
   "source": [
    "### Visualize CloudWatch Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5e10e",
   "metadata": {},
   "source": [
    "Use `get_endpoint_metrics` helper functions, visualize the CloudWatch metrics. This will provide detailed overview of resource usage during the load test. Metrics such as GPU Memory utilization, Invocations and Model Latency metrics will allow to tweak NVIDIA Triton model configuration to improve application performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = advanced_response[\"JobArn\"].split(\"/\")[-1]\n",
    "\n",
    "df_cw = get_endpoint_metrics(sm_client, cw_client, region, job_name, include_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.to_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec840a6b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides an overview of NVIDIA Triton support for PyTorch framework, steps to leverage Amazon SageMaker Inference recommender to execute custom load test and find the right configurations, instance types for large NLP workloads. You can use different Triton configurations, visualize resource consumptions and optimize GPU resource utilization during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677151b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
