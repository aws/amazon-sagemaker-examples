{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring data quality in third-party models from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview:**\n",
    "The process of implementing a third-party model generally starts with searching for a model that meets your business needs, then thoroughly evaluating and testing the model (using your own ground truth dataset), and finally deploying the model to production. If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was evaluated on, the model might begin to lose accuracy in its predictions. \n",
    "\n",
    "Amazon SageMaker's [Data quality monitoring](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html) monitors machine learning (ML) models in production and notifies you when data quality issues arise.  Amazon SageMaker Model Monitor uses rules to detect data drift and alerts you when it happens. These alerts will help you understand whether you would need to re-evalute the ML model to see if it is still providing you the correct outputs. If it is not, you would need to request the third-party seller to retrain and provide a new version of AWS Marketplace model. \n",
    "\n",
    "In this notebook, you will learn how to perform Data Quality monitoring on a pre-trained third-party model from AWS Marketplace.\n",
    "\n",
    "**Contents:**\n",
    "- Prerequisites\n",
    "- Step 1. Initial setup\n",
    "    - 1.1 [Import packages and modules](#section_1_1)\n",
    "    - 1.2 [Set global variables](#section_1_2)\n",
    "    - 1.3 [Uploading sample datasets to your S3 bucket](#section_1_3)\n",
    "- Step 2. Create and deploy the model endpoint with data capture\n",
    "    - 2.1 [Create the model](#section_2_1)\n",
    "    - 2.2 [Create the endpoint configuration with DataCapture](#section_2_2)\n",
    "- Step 3. Create a baselining job to suggest a set of baseline constraints\n",
    "    - 3.1 [Create baselining job](#section_3_1)\n",
    "- Step 4. Setup a monitoring schedule to monitor the data captured for the model's endpoint\n",
    "    - 4.1 [Create a monitoring schedule](#section_4_1)\n",
    "- Step 5. Invoking the inference endpoint with anomalous data\n",
    "    - 5.1 [Create a wrapper function for the Predictor](#section_5_1)\n",
    "    - 5.2 [Generate some data quality constraint violations](#section_5_2)\n",
    "    - 5.3 [Check the model monitoring processing job for completion](#section_5_3)\n",
    "- Step 6. Invoking the inference endpoint with anomalous data\n",
    "    - 6.1 [Visualize the model monitor results](#section_6_1)\n",
    "    - 6.2 [Import the SageMaker-Model-Monitor-Visualize.ipynb notebook](#section_6_2)\n",
    "    - 6.3 [Run the SageMaker-Model-Monitor-Visualize.ipynb notebook](#section_6_3)\n",
    "- [Conclusion](#section_conclusion)\n",
    "- [Cleanup Resources](#section_cleanup_resources)\n",
    "\n",
    "**Pre-requisites**\n",
    "\n",
    "This notebook requires a subscription to the [Propensity-Planning to Buy a House](https://aws.amazon.com/marketplace/pp/prodview-vzofptk4lnxii) model, a pre-trained machine learning model package from AWS Marketplace.\n",
    "\n",
    "1. Open the [Propensity-Planning to Buy a House](https://aws.amazon.com/marketplace/pp/prodview-vzofptk4lnxii) model in your browser. \n",
    "\n",
    "2. To subscribe to the model package, follow these steps: \n",
    "  1. Review the information available on the product details page including **Support Terms** .\n",
    "  1. Click on **\"Continue to Subscribe\"**. You will now see the **\"Subscribe to this software\"** page. \n",
    "  1. Review **End User License Agreement** and **Pricing Terms**.\n",
    "  1. **\"Accept Offer\"** button needs to be clicked if your organization agrees with EULA, pricing information and support terms.\n",
    "  1. Once you click on **Continue to configuration** button and then choose a region, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3.  Copy the Model Package ARN and replace its contents in following cell. \n",
    "  \n",
    " \n",
    "Note: \n",
    "Products with **Free Trials**, do not incur hourly software charges during free trial period, but AWS infrastructure charges still apply. Free Trials will automatically convert to a paid hourly subscription upon expiration. We have included steps below to cancel subscription at the end of this exercise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_1_1></a>\n",
    "#### 1.1 Import packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import io\n",
    "\n",
    "import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import session\n",
    "from sagemaker import ModelPackage\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.model_monitor import (\n",
    "    CronExpressionGenerator,\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    MonitoringExecution,\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "from src.model_package_arns import ModelPackageArnProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_1_2></a>\n",
    "#### 1.2 Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IAM role, SakeMaker session, and region\n",
    "role = get_execution_role()\n",
    "session = session.Session()\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Set service clients\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# S3\n",
    "BUCKET = session.default_bucket()  # Update as needed\n",
    "PREFIX = \"third-party-model-seller-name\"  # Update as needed\n",
    "\n",
    "S3_DATA_CAPTURE_URI = \"s3://{}/{}/datacapture\".format(BUCKET, PREFIX)\n",
    "S3_BASELINE_DATASET_URI = \"s3://{}/{}/train/{}\".format(BUCKET, PREFIX, \"baseline.csv\")\n",
    "S3_BASELINE_ANALYSIS_RESULTS_URI = \"s3://{}/{}/baselining\".format(BUCKET, PREFIX)\n",
    "S3_DATA_QUALITY_RPT_URI = \"s3://{}/{}/reports\".format(BUCKET, PREFIX)\n",
    "\n",
    "# Model\n",
    "MODEL_ENDPOINT = \"third-party-model-endpoint\"\n",
    "MODEL_BASELINE_JOB = \"third-party-model-baseline-job\"\n",
    "MODEL_MONITOR_SCHEDULE_NAME = \"third-party-model-data-quality-schedule\"\n",
    "MODEL_MONITOR_INSTANCE_TYPE = \"ml.m4.xlarge\"\n",
    "MODEL_INFERENCE_INSTANCE_TYPE = \"ml.m4.xlarge\"\n",
    "MODEL_INSTANCE_COUNT = 1\n",
    "MODEL_BASELINE_JOB_NAME = \"{}-{}\".format(\n",
    "    MODEL_BASELINE_JOB, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update as needed\n",
    "MODEL_PACKAGE_ARN = ModelPackageArnProvider.get_model_package_arn(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_1_3></a>\n",
    "#### 1.3 Uploading sample datasets to your S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [baseline.csv](./data/baseline.csv) and [data_quality_drift.csv](./data/data_quality_drift.csv) files are curated datasets I created by modifying the seller's [sample notebook](https://github.com/goprosper/prosper-sagemaker-basic-geo/blob/master/using_prosper_model_package_basic_geo.ipynb) to return high propensity and low propensity values (respectively) for generating a baseline_drift_check constraint violation.\n",
    "\n",
    "Additionally, the data_quality_drift.csv file contains some rows with anomalous data that are used to demonstrate data_type_check and completeness_check constraint violations.\n",
    "\n",
    "The baseline.csv file contains 29 columns and 301 rows with the first column as the target value (prediction)\n",
    "\n",
    "The data_quality_drift.csv file contains 28 columns and 200 rows and is designed to illustrate a degredation in data quality from the baseline dataset.\n",
    "\n",
    "For reference, here's the folder structure used for this demo if you're planning to upload the datasets and run the notebook:\n",
    "\n",
    "<pre>\n",
    "│   monitoring_data_quality_of_models.ipynb\n",
    "│\n",
    "├───data\n",
    "│       baseline.csv\n",
    "│       data_quality_drift.csv\n",
    "└───src\n",
    "        model_package_arns.py\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/baseline.csv\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, BUCKET, \"{}/train/{}\".format(PREFIX, \"baseline.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/data_quality_drift.csv\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, BUCKET, \"{}/train/{}\".format(PREFIX, \"data_quality_drift.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_2_1></a>\n",
    "#### 2.1 Create the model\n",
    "\n",
    "Creates a model in Amazon SageMaker from a model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployable model from the model package.\n",
    "model = ModelPackage(role=role, model_package_arn=MODEL_PACKAGE_ARN, sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_2_2></a>\n",
    "#### 2.2 Deploy model while enabling DataCapture\n",
    "\n",
    "Creates an endpoint configuration that Amazon SageMaker hosting services uses to deploy models. (Estimated cell execution time ~8 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set data capture configuration settings\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=S3_DATA_CAPTURE_URI,\n",
    "    csv_content_types=[\"text/csv\"],\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "model.deploy(\n",
    "    initial_instance_count=MODEL_INSTANCE_COUNT,\n",
    "    instance_type=MODEL_INFERENCE_INSTANCE_TYPE,\n",
    "    endpoint_name=MODEL_ENDPOINT,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_3_1></a>\n",
    "#### 3.1 Create a baselining job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline calculations of statistics and constraints are needed as a standard against which data drift and other data quality issues can be detected. This job generates baseline statistics and suggests baseline constraints for the dataset and writes them to the output_s3_uri location that you specify. (Estimated cell execution time ~6 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initializes a Monitor instance\n",
    "default_model_monitor = DefaultModelMonitor(\n",
    "    role=role, instance_count=MODEL_INSTANCE_COUNT, instance_type=MODEL_MONITOR_INSTANCE_TYPE\n",
    ")\n",
    "\n",
    "# Suggest baselines for use with Amazon SageMaker Model Monitoring Schedules\n",
    "job = default_model_monitor.suggest_baseline(\n",
    "    job_name=MODEL_BASELINE_JOB_NAME,\n",
    "    baseline_dataset=S3_BASELINE_DATASET_URI,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=S3_BASELINE_ANALYSIS_RESULTS_URI,\n",
    "    wait=True,\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_4_1></a>\n",
    "#### 4.1 Create a monitoring schedule\n",
    "\n",
    "In this step, you create a schedule that starts Amazon SageMaker Processing Jobs every hour to monitor the data captured for an Amazon SageMaker Endoint.\n",
    "\n",
    "**Note:** Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start anywhere between the first ~20 minutes after the hour boundary (i.e. 00:00 – 00:20). This is expected and done for load balancing on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creates a schedule that regularly starts a processing jobs to monitor the data captured for the model's endoint\n",
    "default_model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=MODEL_MONITOR_SCHEDULE_NAME,\n",
    "    endpoint_input=MODEL_ENDPOINT,\n",
    "    output_s3_uri=S3_DATA_QUALITY_RPT_URI,\n",
    "    statistics=default_model_monitor.baseline_statistics(),\n",
    "    constraints=default_model_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),  # Currently, SageMaker only supports hourly integer rates between 1 hour and 24 hours.\n",
    ")\n",
    "\n",
    "# Allow time for processing\n",
    "time.sleep(30)\n",
    "\n",
    "# Print the current status\n",
    "monitor_schedule_details = default_model_monitor.describe_schedule()[\"MonitoringScheduleStatus\"]\n",
    "print(\n",
    "    '>> The current status of monitoring schedule \"{0}\" is {1}'.format(\n",
    "        MODEL_MONITOR_SCHEDULE_NAME, monitor_schedule_details\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_5_1></a>\n",
    "#### 5.1 Create a wrapper function for the Predictor\n",
    "\n",
    "The Predictor object is used to invoke the model's real-time inference endpoint to make predictions based on sample data. \n",
    "This function adds some defensive coding, a time delay, and decodes the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor endpoint\n",
    "predictor = Predictor(\n",
    "    endpoint_name=MODEL_ENDPOINT, sagemaker_session=None, serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample, delay=0.5):\n",
    "    \"\"\"Return the inference from the model's endpoint.\n",
    "    Args:\n",
    "        sample (str): Sample data from client\n",
    "        delay (float): The number of seconds execution is suspended\n",
    "    Returns:\n",
    "        (string): inference data from the model's endpoint\n",
    "    \"\"\"\n",
    "    if len(sample) > 0:\n",
    "        time.sleep(delay)\n",
    "        return predictor.predict(data=sample).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_5_2></a>\n",
    "#### 5.2 Generate some data quality constraint violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we'll generate a couple of [data quality constraint violations](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html) (data_type_check, completeness_check, and baseline_drift_check) for Amazon SageMaker Model Monitor to detect.\n",
    "\n",
    "Here's the structure of the data_quality_drift.csv file:\n",
    "\n",
    "- Rows 1-10:  Samples with negative floating point values (instead of positive integers) in the 3rd and 4th columns\n",
    "- Rows 11-20: Samples with missing values in the 2nd column\n",
    "- Rows 20-200: Samples that yield a low propensity prediction that deviates from the baseline dataset\n",
    "\n",
    "We'll use array slicing for accessing the anomalous sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataframe\n",
    "df = pd.read_csv(\"./data/data_quality_drift.csv\", header=None, na_filter=False)\n",
    "\n",
    "# List values by column to maintain the column dtype\n",
    "samples = [df[x].values.tolist() for x in df.columns]\n",
    "\n",
    "# Use unpacking operator * to unzip the data\n",
    "samples = list(list(x) for x in zip(*samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll generate a data_type_check constraint violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Invoke real-time inference endpoint\n",
    "for index, sample in enumerate(samples[0:10]):\n",
    "\n",
    "    # Get inference response\n",
    "    response = predict(sample)\n",
    "\n",
    "    # Display the model's prediction probability\n",
    "    print(\"Sample {0} >> Input: {1}: >> Prediction: {2}\".format(index, sample, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate a completeness_check constraint violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Invoke real-time inference endpoint\n",
    "for index, sample in enumerate(samples[10:20]):\n",
    "\n",
    "    # Get inference response\n",
    "    response = predict(sample)\n",
    "\n",
    "    # Display the model's prediction probability\n",
    "    print(\"Sample {0} >> Input: {1}: >> Prediction: {2}\".format(index, sample, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll generate a baseline_drift_check constraint violation (Estimated cell execution time ~2.5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Invoke real-time inference endpoint\n",
    "for index, sample in enumerate(samples[20:]):\n",
    "\n",
    "    # Get inference response\n",
    "    response = predict(sample)\n",
    "\n",
    "    # Display the model's prediction probability\n",
    "    print(\"Sample {0} >> Input: {1}: >> Prediction: {2}\".format(index, sample, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_5_3></a>\n",
    "#### 5.3 Check the model monitoring processing job for completion\n",
    "\n",
    "Please wait for the first data drift monitoring job to complete. Execution may start anywhere between the first ~20 minutes after the hour boundary (i.e. 00:00 – 00:20). This is expected and done for load balancing on the backend. For hourly scheduled monitoring jobs, currently, SageMaker only supports hourly integer rates between 1 hour and 24 hours.\n",
    "\n",
    "**Note: Estimated cell execution time is dependent on model monitoring schedule creation time -- this cell could take up to 80 minutes to execute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first monitoring execution details\n",
    "monitoring_executions = sm_client.list_monitoring_executions(\n",
    "    MonitoringScheduleName=MODEL_MONITOR_SCHEDULE_NAME\n",
    ")\n",
    "\n",
    "# Check if monitoring job has started\n",
    "while not (monitoring_executions.get(\"MonitoringExecutionSummaries\")):\n",
    "\n",
    "    # Progress update\n",
    "    print(\"Waiting for the data drift model monitoring processing job to start...\")\n",
    "\n",
    "    # Pause for 60 seconds\n",
    "    time.sleep(60)\n",
    "\n",
    "    # Get the first monitoring execution details\n",
    "    monitoring_executions = sm_client.list_monitoring_executions(\n",
    "        MonitoringScheduleName=MODEL_MONITOR_SCHEDULE_NAME\n",
    "    )\n",
    "\n",
    "print(\"The data drift model monitoring processing job has started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodically check the job's status. Upon the job's successful completion, print the ProcessingJobArn for use in section 6.3. (Estimated cell execution time ~6 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Check for the first monitoring execution detail (update as needed for subsequent monitoring executions)\n",
    "if len(monitoring_executions[\"MonitoringExecutionSummaries\"]) == 1:\n",
    "\n",
    "    # Periodically check the processing job status\n",
    "    while True:\n",
    "\n",
    "        # Get the first monitoring execution details\n",
    "        monitoring_executions = sm_client.list_monitoring_executions(\n",
    "            MonitoringScheduleName=MODEL_MONITOR_SCHEDULE_NAME\n",
    "        )\n",
    "        monitoring_execution_details = monitoring_executions[\"MonitoringExecutionSummaries\"][0]\n",
    "\n",
    "        # Get the processing job status\n",
    "        processing_job_status = monitoring_execution_details.get(\"MonitoringExecutionStatus\")\n",
    "\n",
    "        # Periodically check job status\n",
    "        if processing_job_status != \"InProgress\" and processing_job_status != \"Pending\":\n",
    "\n",
    "            # Progress update\n",
    "            print(\"\\nProcessing Job Status: {}\".format(processing_job_status))\n",
    "            print(\n",
    "                \"Processing Job Arn: {}\".format(\n",
    "                    monitoring_execution_details.get(\"ProcessingJobArn\")\n",
    "                )\n",
    "            )\n",
    "            break\n",
    "\n",
    "        # Progress update\n",
    "        print(\"Processing Job Status: {}\".format(processing_job_status))\n",
    "\n",
    "        # Pause for 60 seconds\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_6_1></a>\n",
    "#### 6.1 Visualize the model monitor results\n",
    "\n",
    "After the data drift monitoring job has run, select Endpoints from the <u>Sagemaker Studio Components and registries console</u>, then double-click the endpoint name to view the Model Monitoring tab. \n",
    "\n",
    "The Monitoring status of ‘Issue found’ indicates that the monitor successfully detected one or more data quality constraint violations created by the data drift datasets. \n",
    "\n",
    "Double-click the monitoring job name to view the Model Job Details tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/s4-1-sm-studio-visualize-results-v2.png\"\n",
    "    alt=\"Amazon Sagemaker Data Drift Monitoring\"\n",
    "    style=\"float: left; margin-right: 10px;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/s4-2-monitoring-job-report-details.png\"\n",
    "    alt=\"Amazon Sagemaker Model Job Report\"\n",
    "    style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_6_2></a>\n",
    "#### 6.2 Import the SageMaker-Model-Monitor-Visualize.ipynb notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To graphically visualize the distribution and the distribution statistics for all features, SakeMaker includes a pre-built notebook for viewing feature statistics. (Importing this notebook does not create any additional AWS resources.)\n",
    "\n",
    "1. In the Model Job Details tab under Feature Statistics, select 'View Amazon SageMaker notebook'\n",
    "2. Select Import Notebook (in the upper-right section of the tab)\n",
    "3. Select Kernel: Python 3 (Data Science), then choose the Select button. (It may take a few minutes for the Kernel to start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_6_3></a>\n",
    "#### 6.3 Run the SageMaker-Model-Monitor-Visualize.ipynb notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After importing the SageMaker-Model-Monitor-Visualize.ipynb notebook, update the code cell that contains the variable processing_job_arn with the value from the Processing Job ARN from above.\n",
    "2. Run all cells in the notebook to review the execution and baseline details from the model monitoring processing job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/s4-3-mm_visualize_inf_vs_baseline_stats-v2.png\"\n",
    "    alt=\"SageMaker Model Monitor Visualize notebook - Numerical Features\"\n",
    "    style=\"float: center; margin-right: 10px;\" />\n",
    "\n",
    "Tabular view of some basic statistical details such as mean, sum, standard deviation, etc... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/s4-3-mm_visualize_inf_vs_baseline_stats_chart-v2.png\"\n",
    "    alt=\"SageMaker Model Monitor Visualize notebook - Inference feature statistics plots\"\n",
    "    style=\"float: center; margin-right: 10px;\" />\n",
    "\n",
    "Bar charts illustrating the deviation in statistical data quality between the baseline data and the data drift dataset.\n",
    "The x-axis shows the range of collected inference data versus the baseline data. The y-axis shows the frequency of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_conclusion></a>\n",
    "### Conclusion\n",
    "\n",
    "This notebook demonstrated how to subscribe to a pre-trained third-party model from AWS Marketplace and configure a Data Quality monitoring schedule using Amazon SageMaker Model Monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_cleanup_resources></a>\n",
    "### Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the monitoring schedule\n",
    "sm_client.stop_monitoring_schedule(MonitoringScheduleName=MODEL_MONITOR_SCHEDULE_NAME)\n",
    "time.sleep(30)  # allow time for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model monitoring schedule\n",
    "sm_client.delete_monitoring_schedule(MonitoringScheduleName=MODEL_MONITOR_SCHEDULE_NAME)\n",
    "time.sleep(30)  # allow time for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model endpoint\n",
    "sm_client.delete_endpoint(EndpointName=MODEL_ENDPOINT)\n",
    "time.sleep(60)  # allow time for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model endpoint config\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=MODEL_ENDPOINT)\n",
    "time.sleep(30)  # allow time for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you would need to cancel subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2e02c806d6c435701c3fe2dcf6fb76d36ada477d3f21b11430872f87928bda84"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
