{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Job\n",
    "\n",
    "An Amazon SageMaker *training job* is a compute process that trains an ML model in an containerized environment. In this notebook, you will create a training job with your own custom container on Amazon SageMaker. To read more about training job, refer to the [official docs](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)\n",
    "\n",
    "The outline of this notebook is:\n",
    "- set up an service execution for SageMaker to run a training job\n",
    "- build a light-weighted container based on continuumio/miniconda\n",
    "- test your container locally\n",
    "- push your container to Elastic Container Registry (ECR)\n",
    "- upload your training data to an S3 bucket\n",
    "- create a training job with everything you did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # your gateway to AWS APIs\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def current_time():\n",
    "    ct = datetime.datetime.now() \n",
    "    return str(ct.now()).replace(\":\", \"-\").replace(\" \", \"-\")[:19]\n",
    "\n",
    "def account_id():\n",
    "    return boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a service role for SageMaker\n",
    "\n",
    "Review [notebook on execution role](https://github.com/hsl89/amazon-sagemaker-examples/blob/execution-role/sagemaker-fundamentals/execution-role/execution-role.ipynb) for step-by-step instructions on how to create an IAM Role.\n",
    "\n",
    "The service role is intended to be assumed by the SageMaker service to procure resources in your AWS account on your behalf. \n",
    "\n",
    "1. If you are running this this notebook on SageMaker infrastructure like Notebook Instances or Studio, then we will use the role you used to spin up those resources\n",
    "\n",
    "2. If you are running this notebook on an EC2 instance, then we will create a service role attach `AmazonSageMakerFullAccess` to it. If you already have a SageMaker service role, you can paste its `role_arn` here. \n",
    "\n",
    "First, let's get some helper functions for creating execution role. We discussed those functions in the [notebook on execution role](https://github.com/hsl89/amazon-sagemaker-examples/blob/execution-role/sagemaker-fundamentals/execution-role/execution-role.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=$(ls . | grep iam_helpers.py)\n",
    "\n",
    "if [ -f \"$file\" ]\n",
    "then\n",
    "    rm $file\n",
    "fi\n",
    "\n",
    "wget https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/sagemaker-fundamentals/sagemaker-fundamentals/execution-role/iam_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up service role for SageMaker\n",
    "from iam_helpers import create_execution_role\n",
    "\n",
    "sts = boto3.client('sts')\n",
    "caller = sts.get_caller_identity()\n",
    "\n",
    "if ':user/' in caller['Arn']: # as IAM user\n",
    "    # either paste in a role_arn with or create a new one and attach \n",
    "    # AmazonSageMakerFullAccess\n",
    "    role_name = 'sm'\n",
    "    role_arn = create_execution_role(role_name=role_name)['Role']['Arn']\n",
    "    \n",
    "    # attach the permission to the role\n",
    "    # skip it if you want to use a SageMaker service that \n",
    "    # already has AmazonFullSageMakerFullAccess\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "    )\n",
    "elif 'assumed-role' in caller['Arn']: # on SageMaker infra\n",
    "    assumed_role = caller['Arn']\n",
    "    role_arn = re.sub(r\"^(.+)sts::(\\d+):assumed-role/(.+?)/.*$\", r\"\\1iam::\\2:role/\\3\", assumed_role)\n",
    "else:\n",
    "    print(\"I assume you are on an EC2 instance launched with an IAM role\")\n",
    "    role_arn = caller['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training environement into a docker image\n",
    "\n",
    "Before creating a training job on Amazon SageMaker, you need to package the entire runtime environment of your ML project into a docker image and push the image into the Elastic Container Registry (ECR) under your account. \n",
    "\n",
    "When triggering a training job, your requested SageMaker instance will pull that image from your ECR and execute it with the data you specified in an S3 URI. \n",
    "\n",
    "It important to know how SageMaker runs your image. For **training job**, SageMaker runs your image like\n",
    "```\n",
    "docker run <image> train\n",
    "```\n",
    "i.e. your image needs to have an executable `train` and it is the executable that starts the model training process. You will see later in the notebook how to create it. \n",
    "\n",
    "The next natural thing to ask is how does the image running on SageMaker instance access the data that the model needs to be trained on? SageMaker requires you to reserve `/opt/ml` directory inside your image for it to provide training information. When you trigger a training job, you will need to specify the location of your training data, and the SageMaker instance running your image will mount your data into `/opt/ml/input`. \n",
    "\n",
    "To read more about SageMaker uses `/opt/ml` to provide training information, refer to the [official docs](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Dockerfile\n",
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "`train.py` in `container/` is the main script for training the model. We copied it into `/usr/bin`, renamed it as `train` and made it an executable in the docker image. This way when the container is executed as \n",
    "```\n",
    "docker run <image> train\n",
    "```\n",
    "The script in `/usr/bin/train` (in the container) will run. \n",
    "\n",
    "Note that this is one way to run the training logic on SageMaker. As long as the command \n",
    "```\n",
    "docker run <image> train \n",
    "```\n",
    "triggers your training logic you can do whatever you want. \n",
    "\n",
    "Next, we build the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-image:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what's in the training script `container/train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize container/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "It is a skeleton of a typical ML training logic. The main function fetches training data in `/opt/ml/input/data/train`. To verify we indeed have access to the data, we will print out the names of the files in `/opt/ml/input/data/train`. When you actually run this training logic on SageMaker, you can view the stdout through CloudWatch. We will discuss this in more detail later in this notebook. \n",
    "\n",
    "When the main function finishes model training, it saves the model checkpoint in `/opt/ml/model`. The SageMaker Instance running your container will then upload everything in `/opt/ml/model` to an S3 URI that you will later configure yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your container\n",
    "\n",
    "It is a good practice to test your container before sending it to SageMaker, because you can debug and iterate much faster on your local machine. \n",
    "\n",
    "You are strongly encouraged to read through the section on [How Amazon SageMaker Provides Training Information](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html) from the official doc and figure out local testing environment that replicates how SageMaker provides training information to your container. \n",
    "\n",
    "We will use docker python client to execute the container. To see our implementation of local testing environment, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize container/local_test/test_container.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination \n",
    "\n",
    "Our testing script runs the docker image `example-image:latest` with `train` command, mimicking how SageMaker runs your container for a training job. It mounts the local directory `container/local_test/ml/` to `/opt/ml` in the docker image, mimicking how SageMaker provides the training information to the container. \n",
    "\n",
    "The directory `container/local_test/ml` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -R container/local_test/ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directories `container/local_test/ml/input/data/train` and `container/local_test/ml/input/data/test` contains some csv files, which will be available in `/opt/ml/input/data/train` and `/opt/ml/input/data/test` as the training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test\n",
    "!python container/local_test/test_container.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should see a model checkpoint in `container/local_test/ml/model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls container/local_test/ml/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push your docker image to ECR\n",
    "\n",
    "Now you have build your image tested it locally. Next thing you need to do is to push it to the Elastic Container Registry under your account. Later, when you trigger a training job, the SageMaker instance you requested will pull that image. \n",
    "\n",
    "To do so, you will need to create a repo in your ECR to host it. You might have guess that this operation requires some permission on your ECR resources. That's right. You (the principal running this notebook) needs permission to create repository in ECR and get authorization token from it and the role you created before (which you will later pass to SageMaker) needs permission to get authorization token (and pull the image). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you do not have enough permissions on the ECR resources under your organization's account. Then the admin of the account needs to grant you the ECR permissions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a repository in your ECR\n",
    "\n",
    "Suppose you have enough ECR permissions, we now create a repository in your ECR to host the image `example-image:latest`. It is convenient to set the name of the repository should be the same as the name of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr = boto3.client('ecr')\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName='example-image')\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a repository called `example-image`, then there are two ways you can continue\n",
    "* Delete the repository can create new one with the same name\n",
    "* Create a repository using a name other than `example-image`\n",
    "\n",
    "We will provide code for the second route below. But you will need to run it with caution, because the repository `example-image` is probably used by your org for production, and it happens to coincides with our choice of repository name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to delete the `example-image` repository,\n",
    "Change this cell from markdown to python, then run it. \n",
    "\"\"\"\n",
    "try:\n",
    "    ecr.delete_repository(\n",
    "        repositoryName='example-image')\n",
    "    \n",
    "    ecr.create_repository(\n",
    "        repositoryName='example-image')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag your image and push to ECR\n",
    "\n",
    "Now let's tag the image with the full address of the repository we just created and push it there. Before doing that, you will need to grant docker access to your ECR. Refer to the [registry authentication section](https://docs.aws.amazon.com/AmazonECR/latest/userguide/registry_auth.html) from the ECR documentation for more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-image:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-image:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the ECR repository\n",
    "repo_res = ecr.describe_images(\n",
    "    repositoryName='example-image')\n",
    "pp.pprint(repo_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "\n",
    "SageMaker provides training data to your image through an S3 bucket that your service role has read access to. This means before triggering a training job, you need to make your training available in such an S3 bucket.\n",
    "\n",
    "In this notebook, we will use preloaded data on a public bucket `sagemaker-sample-files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the bucket\n",
    "public_bucket = \"sagemaker-sample-files\"\n",
    "s3 = boto3.client('s3')\n",
    "obj_res = s3.list_objects_v2(\n",
    "    Bucket=\"sagemaker-sample-files\")\n",
    "\n",
    "# print out object keys compactly\n",
    "for obj in obj_res['Contents']:\n",
    "    if '/tabular/fraud_detection/synthethic_fraud_detection_SA' in obj['Key']:\n",
    "        print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend the data under `datasets/tabular/synthetic_fraud_detection_SA` is the data for your ML project.\n",
    "\n",
    "The public bucket `sagemaker-sample-files` is located in us-east-1. We first need to copy the data to a bucket of yours that share the same region with the SageMaker instance you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bucket\n",
    "def create_tmp_bucket():\n",
    "    \"\"\"Create an S3 bucket that is intended to be used for short term\"\"\"\n",
    "    bucket = f\"sagemaker-{current_time()}\" # accessible by SageMaker\n",
    "    region = boto3.Session().region_name\n",
    "    boto3.client('s3').create_bucket(\n",
    "        Bucket=bucket,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': region\n",
    "        })\n",
    "    return bucket\n",
    "\n",
    "bucket = create_tmp_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bucket is created by you. By default, all objects in the bucket are private and are accessible by the owner of the bucket. To make the bucket accessible by SageMaker service, normally you would need to explicitly add the permission to access this bucket to the SageMaker service role. But we do not have to it here, because the bucket is prefixed by \"sagemaker\" and in the policy `AmazonSageMakerFullAccess`, we allow the service role to acccess all S3 prefixed by \"sagemaker\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix = 'input_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from sagemaker-samplef-files to {bucket}\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# copy remote csv files to local\n",
    "files = []\n",
    "data_dir = '/tmp'\n",
    "for obj in obj_res['Contents']:\n",
    "    if '/tabular/fraud_detection/synthethic_fraud_detection_SA' in obj['Key']:\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.csv'):\n",
    "            filename=key.split('/')[-1]\n",
    "            files.append(filename)\n",
    "            with open(os.path.join(data_dir, filename), 'wb') as f:\n",
    "                s3.download_fileobj(public_bucket, key, f)\n",
    "\n",
    "# upload from local to the bucket you just created\n",
    "for fname in files:\n",
    "    with open(os.path.join(data_dir, fname), 'rb') as f:\n",
    "        key = input_prefix + fname\n",
    "        s3.upload_fileobj(f, bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect your bucket\n",
    "obj_res = s3.list_objects_v2(\n",
    "    Bucket=bucket)\n",
    "\n",
    "for obj in obj_res['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare an S3 URI for saving model artifact\n",
    "\n",
    "After your image is done with model training, it needs to write the trained model artifact into `/opt/ml/model`. This is directory where SageMaker looks for the trained model artifact and upload it to an S3 URI you will configure later. Naturally, the execution role `sm` needs to have write permission to this S3 URI. \n",
    "\n",
    "Refer to the section on [How Amazon SageMaker Processes Training Output](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html) in the official docs for more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together\n",
    "\n",
    "Now you have everything you need to create a training job. Let's review what you have done. You have \n",
    "* created an execution role for SageMaker service\n",
    "* built and tested a docker image that includes the runtime and logic of your model training\n",
    "* made the image accessible to SageMaker by hosting it on ECR\n",
    "* made the training data available to SageMaker by hosting it on S3\n",
    "* pointed SageMaker to an S3 bucket to write output \n",
    "\n",
    "Let pull the trigger and create a training job. We will invoke `CreateTrainingJob` API via boto3. You are strongly encouraged to read through the [description of the API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job) before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "# name training job\n",
    "training_job_name = 'example-training-job-{}'.format(current_time())\n",
    "\n",
    "# input data prefix\n",
    "data_path = \"s3://\" + bucket + '/' + input_prefix\n",
    "\n",
    "# location that SageMaker saves the model artifacts\n",
    "output_prefix = 'example/output/'\n",
    "output_path = \"s3://\" + bucket + '/' + output_prefix\n",
    "\n",
    "# ECR URI of your image\n",
    "region = boto3.Session().region_name\n",
    "account = account_id()\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/example-image:latest\".format(account, region)\n",
    "\n",
    "algorithm_specification = {\n",
    "    'TrainingImage': image_uri,\n",
    "    'TrainingInputMode': 'File',\n",
    "}\n",
    "\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        'ChannelName': 'train',\n",
    "            'DataSource':{\n",
    "                'S3DataSource':{\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': data_path,\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "        }\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'ChannelName': 'test',\n",
    "        'DataSource':{\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': data_path,\n",
    "                'S3DataDistributionType': 'FullyReplicated',\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': output_path\n",
    "}\n",
    "\n",
    "resource_config = {\n",
    "    'InstanceType': 'ml.m5.large',\n",
    "    'InstanceCount':1,\n",
    "    'VolumeSizeInGB':10\n",
    "}\n",
    "\n",
    "stopping_condition={\n",
    "    'MaxRuntimeInSeconds':120,\n",
    "}\n",
    "\n",
    "enable_network_isolation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_res = sm_boto3.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    AlgorithmSpecification=algorithm_specification,\n",
    "    RoleArn=role_arn,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    ResourceConfig=resource_config,\n",
    "    StoppingCondition=stopping_condition,\n",
    "    EnableNetworkIsolation=enable_network_isolation,\n",
    "    EnableManagedSpotTraining=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the status of the training job\n",
    "tj_state = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=training_job_name)\n",
    "pp.pprint(tj_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training job status every 30 seconds\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    tj_state = sm_boto3.describe_training_job(\n",
    "        TrainingJobName=training_job_name)\n",
    "    if tj_state['TrainingJobStatus'] in ['Completed', 'Stopped', 'Failed']:\n",
    "        stopped=True\n",
    "    else:\n",
    "        print(\"Training in progress\")\n",
    "        time.sleep(30)\n",
    "\n",
    "if tj_state['TrainingJobStatus'] == 'Failed':\n",
    "    print(\"Training job failed \")\n",
    "    print(\"Failed Reason: {}\".tj_state['FailedReason'])\n",
    "else:\n",
    "    print(\"Training job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the trained model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"== Output config:\")\n",
    "print(tj_state['OutputDataConfig'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"== Model artifact:\")\n",
    "pp.pprint(s3.list_objects_v2(Bucket=bucket, Prefix=output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = boto3.client('logs')\n",
    "\n",
    "log_res= logs.describe_log_streams(\n",
    "    logGroupName='/aws/sagemaker/TrainingJobs',\n",
    "    logStreamNamePrefix=training_job_name)\n",
    "\n",
    "for log_stream in log_res['logStreams']:\n",
    "    # get one log event\n",
    "    log_event = logs.get_log_events(\n",
    "        logGroupName='/aws/sagemaker/TrainingJobs',\n",
    "        logStreamName=log_stream['logStreamName'])\n",
    "    \n",
    "    # print out messages from the log event\n",
    "    for ev in log_event['events']:\n",
    "        for k, v in ev.items():\n",
    "            if k == 'message':\n",
    "                print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You now understand the basics of a training job on SageMaker. It's funny to think that after this long notebook, you get a trained model artifact, which is a pickled None instance. But keep in mind that you can follow the exact same process to train a state-of-art model with billions of parameters and the compute cost is proportional to how long you train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ECR repo\n",
    "del_repo_res = ecr.delete_repository(\n",
    "    repositoryName='example-image',\n",
    "    force=True)\n",
    "\n",
    "pp.pprint(del_repo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the S3 bucket\n",
    "def delete_bucket_force(bucket_name):\n",
    "    objs = s3.list_objects_v2(Bucket=bucket_name)['Contents']\n",
    "    for obj in objs:\n",
    "        s3.delete_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=obj['Key'])\n",
    "    \n",
    "    return s3.delete_bucket(Bucket=bucket_name)\n",
    "\n",
    "del_buc_res = delete_bucket_force(bucket)\n",
    "\n",
    "pp.pprint(del_buc_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
