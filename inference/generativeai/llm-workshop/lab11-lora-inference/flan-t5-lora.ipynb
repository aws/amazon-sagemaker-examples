{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Flan T5 Finetuned with LoRA using SageMaker LMI DLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will walk through how you can take a Flan T5 model fine tuned via Low Rank Adapters (LoRA) and deploy it for inference using SageMaker Large Model Inference (LMI) DLCs. We will use the FasterTransformer DLC for optimized inference. For more information on LoRA, please see the [relevant paper](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installs the dependencies required to package the model and run inferences using Amazon SageMaker. Update SageMaker, boto3 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix_fastertransformer = \"hf-large-model-ft/code_flant5lora/fastertransformer\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Model Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating model artifacts for a LoRA fine-tuned model is very similar to the process for creating artifacts for a standard model to use with the LMI DLCs. For this notebook, we will be using [this LoRA checkpoint available on the huggingface hub](https://huggingface.co/lorahub/flan_t5_large-quarel_logic_test). If you have LoRA checkpoints for a different model, you can substitute those here as long as the base model architecture is supported by FasterTransformer. We will demonstrate two ways for creating model artifacts depending on whether you are deploying a fine-tuned model available from the HuggingFace hub, or a fine-tuned model with local artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf code_flant5_lora\n",
    "!mkdir -p code_flant5_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Create a serving.properties for a LoRA checkpoint available on the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_flant5_lora/serving.properties\n",
    "engine = FasterTransformer\n",
    "# built in default handler. handles model conversion from LoRA to FasterTransformer compatible\n",
    "option.entryPoint = djl_python.fastertransformer\n",
    "# Pointer to LoRA checkpoints available on the huggingface hub\n",
    "option.model_id = lorahub / flan_t5_large - quarel_logic_test\n",
    "option.tensor_parallel_degree = 1\n",
    "option.dtype = fp16\n",
    "option.model_loading_timeout = 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all you need for deploying a LoRA fine-tuned model available on the HuggingFace hub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Create a serving.properties for a local LoRA checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a LoRA checkpoint available locally that you want to deploy for inference, you will need to package the checkpoints as part of the code artifact that is uploaded to S3. The direcotry structure should look like this:\n",
    "\n",
    "```\n",
    "code_flant5_lora/\n",
    "├── lora_checkpoint_dir\n",
    "│   ├── adapter_config.json\n",
    "│   ├── adapter_model.bin # can also be in safetensors format\n",
    "├── (optional) base_model_artifacts\n",
    "│   ├── *.bin # checkpoint files\n",
    "│   ├── *.json # configuration files for model, tokenizer, generation etc.\n",
    "├── serving.properties\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have saved your LoRA checkpoints in a directory called `lora_checkpoint_dir`, the serving.properties file will look like this (uncommented, of course):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine=FasterTransformer\n",
    "# option.entryPoint=djl_python.fastertransformer\n",
    "# # Pointer to LoRA checkpoints available on the huggingface hub\n",
    "# option.model_id=lora_checkpoint_dir\n",
    "# # built in default handler. handles model conversion from LoRA to FasterTransformer compatible\n",
    "# option.tensor_parallel_degree=1\n",
    "# option.dtype=fp16\n",
    "# option.model_loading_timeout=1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For LoRA models generated by the HuggingFace Peft library, the adapter_config.json file specifies the base model that was used to train in a field called `base_model_name_or_path`. To use this model for inference, the LMI container expects the base model path relative to the model directory (in this example that would be the code_flant5_lora directory). If the directory is not found, the base model will be downloaded from HuggingFace hub at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_flant5_lora/requirements.txt\n",
    "peft==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Tarball and then upload to S3 location\n",
    "Next, we will package our artifacts as `*.tar.gz` files for uploading to S3 for SageMaker to use for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf code_flant5_lora/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C code_flant5_lora .\n",
    "s3_code_artifact_fastertransformer = sess.upload_data(\n",
    "    \"model.tar.gz\", bucket, s3_code_prefix_fastertransformer\n",
    ")\n",
    "print(\n",
    "    f\"S3 Code or Model tar for fastertransformer uploaded to --- > {s3_code_artifact_fastertransformer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a serving container, SageMaker Model and SageMaker endpoint\n",
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the serving container\n",
    "Here we define the container to use for the model for inference. We will be using SageMaker's Large Model Inference(LMI) container using FasterTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-fastertransformer\", region=region, version=\"0.23.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker model, endpoint configuration and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_ds = name_from_base(f\"flant5-lora-ds\")\n",
    "print(model_name_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_ds,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact_fastertransformer,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name_ds\n",
    "print(f\"Building EndpointConfig and Endpoint for: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "            # \"VolumeSizeInGB\": 512\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "\n",
    "In this example we are running a LoRA fine-tuned version of the flan-t5-large model, but the same steps can be used to deploy larger models using tensor parallelism and larger instances. Note that this model may not be accurate and provide correct outputs, but this example is intended to showcase the functionality of deploying LoRA fine-tuned models with optimized inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Infer the date from context.  \n",
    "Q: Today, 8/3/1997, is a day that we will never forget. \n",
    "What is the date one week ago from today in MM/DD/YYYY? Options: \n",
    "(A) 03/27/1998 \n",
    "(B) 09/02/1997 \n",
    "(C) 07/27/1997 \n",
    "(D) 06/29/1997 \n",
    "(E) 07/27/1973 \n",
    "(F) 12/27/1997 A:\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": prompt,\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(data),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "output = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we showed how to take LoRA fine-tune checkpoints and deploy the full fine-tuned model on SageMaker using the FasterTransformer LMI DLC. This DLC will merge the LoRA weights with the base model weights, and then optimize the resulting model using tensor parallelism and fused CUDA kernels (if the model type is supported).\n",
    "\n",
    "The same functionality is available in the DeepSpeed DLC. You can refer to the Falcon 7b LoRA inference example in this notebook: [Serve Falcon 7b Fine-Tuned with LoRA using SageMaker LMI DLC](https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/generativeai/llm-workshop/lab11-lora-inference/falcon-7b-lora.ipynb)\n",
    "\n",
    "A unique feature of LoRA is that since the fine tuned model artifacts are separate from the base model, we can deploy a single endpoint with multiple adapters and switch between them at inference time. This functionality is demonstrated in this notebook: [Serve Multiple Fine-Tuned LoRA Adapters on a single endpoint using SageMaker LMI DLC](https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/generativeai/llm-workshop/lab11-lora-inference/lora-multi-adapter.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|lab11-lora-inference|flan-t5-lora.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
