{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bloom using the HuggingFace Trainer with the sharded data parallelism tequnique in the SageMaker Model Parallelism library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use the SageMaker Model Parallelism (SMP) library's Trainer as a drop-in replacement for Hugging Face Transformer's Trainer and enable [Sharded Data Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html) technique in [SageMaker's Model Parallelism (SMP) library](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html) to train a Bloom model. You'll learn how to train the model with sharded data parallelism on the [GLUE/SST2 dataset](https://huggingface.co/datasets/glue/viewer/sst2/train) dataset.\n",
    "\n",
    "The Bloom model was proposed by BigScience in the paper [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf). The original Bloom is a large transformer-based language model with 176 billion parameters. In this notebook, we will experiment with the 560-million-parameter version. This notebook uses the [Hugging Face Transformers Bloom](https://bigscience.huggingface.co/blog/bloom) implementation with SageMaker model parallel integration.\n",
    "\n",
    "Sharded data parallelism is a memory-saving distributed training technique that splits the state of a model (model parameters, gradients, and optimizer states) across GPUs in a data parallel group. There are two main benefits: one, you can fit larger models, which would otherwise run out of memory with standard data parallelism, on fewer GPUs; and two, you can increase the batch size using the freed-up GPU memory.\n",
    "\n",
    "This notebook requires the following prerequisites:\n",
    "- `run_clm.py`: This is an entry point script, which is the example training script for the SageMaker Hugging Face estimator. This script is responsible for end-to-end training of the Bloom model.\n",
    "- `requirements.txt`: This file lists additional Python library dependencies that SageMaker will automatically install. This needs to be in the same directory as your entry point script.\n",
    "- `smp_trainer.py`: This file inherits Hugging Face Transformer's Trainer and adds sharded data parallelism using the SMP library.\n",
    "\n",
    "**Note**: This notebook assumes that the AWS Region is `us-west-2`. If your AWS Region is different from `us-west-2`, make sure that you change the region code throughout this notebook.\n",
    "\n",
    "### Additional resources\n",
    "If you are new to Amazon SageMaker, you may find the following resources helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "- To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "- To learn more about using the SageMaker Python SDK with PyTorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "- To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "- To learn more about sharded data parallelism, check [Sharded Data Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html) or the blog [Near-linear scaling of gigantic-model training on AWS](https://www.amazon.science/blog/near-linear-scaling-of-gigantic-model-training-on-aws)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt your entrypoint script to use SMP Trainer\n",
    "To update your entrypoint script to use SMP Trainer instead of Hugging Face Trainer, all you need to do is the following.\n",
    "\n",
    "Replace the following line:  \n",
    "  `from transformers import Trainer`  \n",
    "with the SMP Trainer version:  \n",
    "  `from smp_trainer import SMPTrainer as Trainer`  \n",
    "\n",
    "You can now follow the rest of this notebook for details on how to enable sharded data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker initialization\n",
    "\n",
    "Upgrade SageMaker SDK to the latest version. \n",
    "\n",
    "**NOTE:** Upgrading libraries might require a kernel restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run once, restart kernel, then comment out this cell\n",
    "# # update sagemaker to the latest 2.x version\n",
    "# ! pip3 install -qU pip\n",
    "! pip3 install -qU \"sagemaker>=2,<3\"\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import the SageMaker modules and retrieve information of your current SageMaker work environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "# You also need to specify an Amazon S3 bucket to store the output data such as training artifacts.\n",
    "# The following cell sets up the default S3 bucket paired with the current SageMaker session. You can also modify this as needed.\n",
    "s3_output_bucket = f\"s3://{default_bucket}/output\"\n",
    "print(f\"Your output data will be stored in: {s3_output_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hyperparameters, metric definitions, and MPI options\n",
    "The following `hyperparameters` dictionary passes arguments to the training script (`run_clm.py`) and sets the model parallel configuration when creating a training job.\n",
    "\n",
    "Note that the `run_clm.py` file is currently modified to work with SageMaker. If you want to run your own script, you'll need to modify your script similar to the modified `run_clm.py` script. You can find them quickly by searching for `SageMaker Support`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_steps = 60  # Set the interval for saving checkpoints\n",
    "max_steps = 100  # Set the total number of steps you want to run\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model_name_or_path\": \"bigscience/bloom-560m\",\n",
    "    \"output_dir\": \"/opt/ml/checkpoints\",\n",
    "    \"overwrite_output_dir\": \"\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"save_steps\": save_steps,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_eval_samples\": 50,\n",
    "    \"preprocessing_num_workers\": 1,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"eval_accumulation_steps\": 2,\n",
    "    \"logging_steps\": 1,\n",
    "    \"dataloader_drop_last\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "This section loads the [GLUE/SST2](https://huggingface.co/datasets/glue/viewer/sst2/train) dataset and splits it to training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use any dataset available from Hugging Face Datasets\n",
    "# Modify these parameters as needed\n",
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below or define your own.\n",
    "\n",
    "You can also specify different training parameters here such as sharded data parallel degree, batch size, and fp16 which will affect if your model can fit on your instance configuration.\n",
    "\n",
    "For more information on these parameters and how to use them, see [SageMaker Distributed Training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html) in the *SageMaker Developer Guide*.\n",
    "\n",
    "**Note:** you may need to adjust these parameters such as `sdp_degree` and `per_device_train_batch_size` if you choose to train another size model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"bloom-560m\"\n",
    "\n",
    "if model_config == \"bloom-560m\":\n",
    "    # 560M parameters\n",
    "    hyperparameters[\"per_device_train_batch_size\"] = 4\n",
    "    hyperparameters[\"per_device_eval_batch_size\"] = 2\n",
    "    sdp_degree = 2\n",
    "    microbatches = 1\n",
    "    fp16 = False\n",
    "    hyperparameters[\"fp16\"] = fp16\n",
    "    prescaled_batch = True\n",
    "    shard_optimizer_state = False\n",
    "    pp_degree = 1\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown model config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify essential parameters for a SageMaker Training job\n",
    "\n",
    "Next, you will use the [SageMaker Estimator API](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker training job, passing values through the following parameters, such as the training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances.\n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the type and the number of EC2 instances to use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the instance_type here\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "\n",
    "# Set to the number of instances you want to use\n",
    "# bloom-560m needs >= 1 p4d instances\n",
    "instance_count = 1\n",
    "\n",
    "# set to the number of GPUs on that instance\n",
    "# p3d's and p4d's have 8 GPUs each\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set parameters for model parallelism and MPI options\n",
    "\n",
    "The following code sets up parameters for model parallelism. Note that the `sharded_data_parallel_degree` parameter must be added to activate sharded data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM_HP_MP_PARAMETERS = {\n",
    "    \"microbatches\": microbatches,\n",
    "    \"optimize\": \"speed\",\n",
    "    \"pipeline\": \"interleaved\",\n",
    "    \"placement_strategy\": \"cluster\",\n",
    "    \"partitions\": pp_degree,\n",
    "    \"prescaled_batch\": prescaled_batch,\n",
    "    \"shard_optimizer_state\": shard_optimizer_state,\n",
    "    \"fp16\": fp16,\n",
    "    \"sharded_data_parallel_degree\": sdp_degree,  # Add this to activate sharded data parallelism\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets up MPI options. You can also add custom `mpi` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += (\n",
    "    \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    ")\n",
    "if instance_type in [\"ml.p3dn.24xlarge\", \"ml.p4d.24xlarge\"]:\n",
    "    mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 \"\n",
    "if SM_HP_MP_PARAMETERS[\"partitions\"] > 1:\n",
    "    mpioptions += \"-x SMP_ENABLE_CROSS_NODE_D2D=1 \"\n",
    "# Uncomment out the following line if you want to save the full model checkpoint\n",
    "# instead of the partial model checkpoint.\n",
    "# Setting the flag to anything will enable the save full model logic.\n",
    "# mpioptions += \"-x HF_TRAINER_SMP_SDP_SAVE_FULL_MODEL=1 \"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Resume training from a previous checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose to resume training from a previous checkpoint. To resume, set `resume_from_checkpoint` to `True` in the following code cell. Specify the S3 bucket path in which the checkpoint is stored. For convenience, we use the same bucket to load checkpoints and save output artifacts. You can also customize and set your own. You can also specify whether to load from a partial checkpoint or full checkpoint. Trainer saves both.\n",
    "\n",
    "**Note:** The checkpoint path (`checkpoint_s3_uri`) is not unique per job.\n",
    "You need to modify as needed for different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint = False\n",
    "# Set `resume_from_full_checkpoint` to true if you want to load full ckpt instead of partial.\n",
    "# Note: You need to uncomment the above\n",
    "#   HF_TRAINER_SMP_SDP_SAVE_FULL_MODEL environment option.\n",
    "resume_from_full_checkpoint = False\n",
    "\n",
    "\n",
    "# We label our job with the model configuration and the number of nodes\n",
    "job_name = f\"{model_config}_nodes-{instance_count}\"\n",
    "# Here, we use the same bucket for both checkpoints and outputs\n",
    "checkpoint_bucket = s3_output_bucket\n",
    "# If you want to resume training, set checkpoint_s3_uri to the same checkpoint_s3_uri path as a previous job.\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/{job_name}/checkpoints\"\n",
    "\n",
    "# The previous checkpoint to load must have the same model config.\n",
    "if resume_from_checkpoint:\n",
    "    # The checkpoint step you want to resume training from.\n",
    "    # Here, we set it to the first checkpoint saved, but you can set it to any.\n",
    "    checkpoint_step = save_steps\n",
    "    checkpoint_dir = f\"/opt/ml/checkpoints/checkpoint-{checkpoint_step}\"\n",
    "    hyperparameters[\"resume_from_checkpoint\"] = checkpoint_dir\n",
    "    if resume_from_full_checkpoint:\n",
    "        hyperparameters[\"load_full\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker PyTorch estimator\n",
    "\n",
    "The following cell constructs a `PyTorch` estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `run_clm.py` file and the private preview documentation. We will be using `PyTorch 1.13.1` along with `Transformers 4.21.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "# Set a unique base job name\n",
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-trainer-{model_config}-{machine_str}-sdp{sdp_degree}-bs{hyperparameters[\"per_device_train_batch_size\"]}'\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"run_clm.py\",\n",
    "    source_dir=os.getcwd(),  # copies your current working directory to S3 for SageMaker\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    # these two map to the same config\n",
    "                    \"partitions\": SM_HP_MP_PARAMETERS[\"partitions\"],\n",
    "                    \"microbatches\": SM_HP_MP_PARAMETERS[\"microbatches\"],\n",
    "                    \"shard_optimizer_state\": SM_HP_MP_PARAMETERS[\"shard_optimizer_state\"],\n",
    "                    \"prescaled_batch\": SM_HP_MP_PARAMETERS[\"prescaled_batch\"],\n",
    "                    \"fp16\": True,\n",
    "                    \"optimize\": SM_HP_MP_PARAMETERS[\"optimize\"],\n",
    "                    \"auto_partition\": True,\n",
    "                    \"default_partition\": 0,\n",
    "                    \"sharded_data_parallel_degree\": SM_HP_MP_PARAMETERS[\n",
    "                        \"sharded_data_parallel_degree\"\n",
    "                    ],  # Add this to activate sharded data parallelism\n",
    "                    \"sdp_reduce_bucket_size\": int(5e8),  # Optional\n",
    "                    \"sdp_param_persistence_threshold\": int(1e6),  # Optional\n",
    "                    \"sdp_max_live_parameters\": int(1e9),  # Optional\n",
    "                    \"sdp_gradient_clipping\": 1.0,  # Optional\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    py_version=\"py39\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.13.1\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of the Bloom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "smp_estimator.fit(\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the training logs\n",
    "\n",
    "You can access the training logs using [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1**, which is the main node whose output stream has the entire training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see **Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics** in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of Amazon CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html).\n",
    "\n",
    "For additional information about monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
