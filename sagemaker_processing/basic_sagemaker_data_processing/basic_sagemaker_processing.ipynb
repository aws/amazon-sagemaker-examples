{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic SageMaker Processing Script\n",
    "\n",
    "This notebook corresponds to the section \"Preprocessing Data With The Built-In Scikit-Learn Container\" in [this](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/) blog post. \n",
    "It shows a very basic example of using SageMaker Processing to create train, test and validation datasets. SageMaker Processing is used to create these datasets, which then are written back to S3.\n",
    "\n",
    "First, letâ€™s create an SKLearnProcessor object, passing the scikit-learn version we want to use, as well as our managed infrastructure requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\", role=role, instance_type=\"ml.m5.xlarge\", instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the raw data from a public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_data = \"s3://sagemaker-sample-data-{}/processing/census/census-income.csv\".format(region)\n",
    "df = pd.read_csv(input_data, nrows=10)\n",
    "df.to_csv(\"dataset.csv\")\n",
    "# df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the processing script that will be used by SageMaker Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data locally\n",
    "input_data_path = os.path.join(\"/opt/ml/processing/input\", \"dataset.csv\")\n",
    "# Preprocess the data set\n",
    "df = pd.read_csv(input_data_path)\n",
    "downsampled = df\n",
    "print(\"shape of data is:\")\n",
    "print(downsampled.shape)\n",
    "# Split data set into training, validation, and test\n",
    "train, test = train_test_split(downsampled, test_size=0.2)\n",
    "train, validation = train_test_split(train, test_size=0.2)\n",
    "# Create local output directories\n",
    "try:\n",
    "    os.makedirs(\"/opt/ml/processing/output/train\")\n",
    "    os.makedirs(\"/opt/ml/processing/output/validation\")\n",
    "    os.makedirs(\"/opt/ml/processing/output/test\")\n",
    "    print(\"Successfully created directories\")\n",
    "except Exception as e:\n",
    "    # if the Processing call already creates these directories (or directory otherwise cannot be created)\n",
    "    print(e)\n",
    "    print(\"Could Not Make Directories\")\n",
    "    pass\n",
    "\n",
    "# Save data locally\n",
    "try:\n",
    "    train.to_csv(\"/opt/ml/processing/output/train/train.csv\")\n",
    "    validation.to_csv(\"/opt/ml/processing/output/validation/validation.csv\")\n",
    "    test.to_csv(\"/opt/ml/processing/output/test/test.csv\")\n",
    "    print(\"Files Successfully Written\")\n",
    "except Exception as e:\n",
    "    print(\"Could Not Write the Files\")\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "print(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Execute the Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    # arguments = ['arg1', 'arg2'],\n",
    "    inputs=[ProcessingInput(source=\"dataset.csv\", destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output/train\"),\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output/validation\"),\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output/test\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
