{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec84d9d",
   "metadata": {},
   "source": [
    "# Right-sizing your Amazon SageMaker Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff3d98",
   "metadata": {},
   "source": [
    "__Disclaimer__:\n",
    "* To run this notebook, you are recommended to use a ml.m5.4xlarge or a larger instance-type to avoid running into CPU limit errors for load testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59926bc",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fcb57c",
   "metadata": {},
   "source": [
    "This notebook is intended to guide you through the process of choosing the correct instance type for model serving depending on the following criteria:\n",
    "1. Number of requests per second\n",
    "2. Endpoint costs.\n",
    "\n",
    "By running part of this notebook, you will run the load tests and identify the optimal instance type. These load tests are executed with the [Locust](https://locust.io/) load testing framework. Locust allows you to run load tests in parallel until the instance fails for each instance type that you wants to compare, thus providing a comprehensive view of performance vs cost for each instance type. \n",
    "\n",
    "The load testing to identify the best fit instance can be carried out via two different approaches. The first approach runs a set of automatic tests to build a performance map of all candidate instance types you specify. The second approach lets you take a more hands-on aproach to iterate manually.\n",
    "\n",
    "This notebook demonstrates the endpoint instance type optimization for the Wide ResNet-50 Image Classification model available as part of [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html). SageMaker JumpStart is a capability of Amazon SageMaker that accelerates machine learning workflows with one-click access to popular model collections and end-to-end solutions. Note that you may however, tweak this notebook and use it to test other ML models you have trained, or models from the AWS ML MarketPlace.\n",
    "\n",
    "**Note** - The cost of running this notebook depends on the model you use and its corresponding acceptable instance types. Although the ML model configured in this notebook does not have any software costs, ML Models from AWS Marketplace may incur additional software costs. This notebook will cost between 30-40$ USD to deploy and test the endpoints.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "0. [Prerequisites](#prereq)\n",
    "\n",
    "1. [Step 1: Setting up the model and endpoint](#setup-model-endpoint)\n",
    "\n",
    "    1. [Step 1.1: Set up environment](#env-setup)\n",
    "    2. [Step 1.2: Identify and prepare data for load testing](#data-setup)\n",
    "    3. [Step 1.3: Identify and create model objects for endpoints](#ml-setup)\n",
    "    4. [Step 1.4: Set up Lambda function and API Gateway](#infra-setup)\n",
    "    \n",
    "2. [Step 2: Load Testing](#load-testing)\n",
    "\n",
    "    1. [Step 2.1: Comprehensive testing](#comprehensive-testing)\n",
    "        1. [Step 2.1.1: Deploy endpoints](#deploy-ep)\n",
    "        2. [Step 2.1.2: Test endpoints with a sample paylod](#check-eps)\n",
    "        3. [Step 2.1.3: Execute load tests](#run-load-tests)\n",
    "        4. [Step 2.1.4: Performance vs price plot](#plot)\n",
    "        5. [Step 2.1.5: Finalize configuration](#recommendations)\n",
    "    \n",
    "    2. [Step 2.2: Semi-automatic testing](#semi-aut-testing)\n",
    "        1. [Step 2.2.1: First iteration](#first-iteration)\n",
    "        2. [Step 2.2.2: Second iteration](#second-iteration)\n",
    "        \n",
    "3. [Step 3: Clean up Resources](#clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38443d4d",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "<a id='prereq'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39fddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Locust library for load testing\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install locust==1.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9f4e3",
   "metadata": {},
   "source": [
    "1. If you'd like to perform load tests for your custom algorithm and model, you should already have a model registered on Amazon SageMaker, and have the model ARN ready.\n",
    "2. You will need **sufficient account limits** to create all endpoints you'd like to test. This notebook tests GPU instances, including ml.p3.2xlarge and ml.g4dn.xlarge instances. If you do not have the sufficient instance count limits, you can request account quotas updates [here](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-limit/), or edit the instance types accordingly in the first cell under the [\"Comprehensive testing\"](#comprehensive-testing) section. To execute this notebook as-is, you will need the following limits - \n",
    "\n",
    "    - ml.c5.xlarge - 1\n",
    "    - ml.c5.2xlarge - 1\n",
    "    - ml.m5.large - 1\n",
    "    - ml.m5.xlarge - 1\n",
    "    - ml.p2.xlarge - 1\n",
    "    - ml.p3.2xlarge - 1\n",
    "    - ml.g4dn.xlarge -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "print(get_execution_role())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b78fec",
   "metadata": {},
   "source": [
    "3. **Amazon SageMaker execution role with _Administrator_ permissions on the account**, or the following AWS managed policies:\n",
    "    - AmazonSageMakerFullAccess (this policy is attached by default to notebook execution roles). \n",
    "    - IAMFullAccess\n",
    "    - AmazonAPIGatewayAdministrator\n",
    "    - AWSPriceListServiceFullAccess\n",
    "    - AWSLambda_FullAccess\n",
    "    \n",
    "If you are unsure, go to the IAM console [here](https://console.aws.amazon.com/iam/home?region=us-east-1#/roles), search for the notebook execution role (the role ARN is printed in the cell above), and attach the above policies using the 'Attach policies' button. \n",
    "\n",
    "<img src=\"assets/IAMRole.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc63113",
   "metadata": {},
   "source": [
    "## Step 1: Setting up the model and endpoint\n",
    "\n",
    "<a id='setup-model-endpoint'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70447f4e",
   "metadata": {},
   "source": [
    "In this section, you will load all the necessary libraries and create general use variables, download the data used for the load testing, and configure the Pytorch ResNet-50 model object, by pointing it to the source of the model (in a public AWS S3 Bucket). You will also set up an Amazon API Gateway and AWS Lambda function to trigger the endpoints for load testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fcb32",
   "metadata": {},
   "source": [
    "### Step 1.1: Set up environment\n",
    "<a id='env-setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c981ea3",
   "metadata": {},
   "source": [
    "In this section, you will import the necessary libraries and declare variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b472201",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import all necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import random\n",
    "import requests\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from IPython.display import Image\n",
    "from botocore.config import Config\n",
    "\n",
    "# Import from helper functions\n",
    "from api_helper import create_infra, delete_infra\n",
    "from sagemaker_helper import deploy_endpoints, clean_up_endpoints, get_supported_instance_types\n",
    "from load_test_helper import (\n",
    "    run_load_tests,\n",
    "    generate_plots,\n",
    "    get_min_max_instances,\n",
    "    generate_latency_plot,\n",
    ")\n",
    "\n",
    "# Define the boto3 clients/resources that will be used later on\n",
    "sm_client = boto3.client('sagemaker', config=Config(connect_timeout=5, read_timeout=60, retries={'max_attempts': 20}))\n",
    "sagemaker_session = sagemaker.Session(sagemaker_client=sm_client)\n",
    "\n",
    "# Define session variables\n",
    "sagemaker_session = sagemaker.Session(sagemaker_client=sm_client)\n",
    "region = sagemaker_session.boto_region_name\n",
    "account_id = sagemaker_session.account_id()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fb5ec",
   "metadata": {},
   "source": [
    "### Step 1.2: Identify and prepare data for load testing\n",
    "<a id='data-setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e075d26",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "The **Pytorch ResNet50** ML model used by this notebook has been trained on the [ImageNet](http://www.image-net.org/about) dataset, which consists of around 100,000 image classes, with 1000 images each. To run the load testing on the ML model, you will use the COCO dataset. In next cell, you will download the images to the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Download the COCO dataset and unzip the file\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip val2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a38d64",
   "metadata": {},
   "source": [
    "Next, you will load and visualize a sample input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images are unzipped automatically to the val2017/ folder\n",
    "images_dir = \"./val2017/\"\n",
    "random_file = random.choice(os.listdir(images_dir))\n",
    "print(random_file)\n",
    "# Load and display image\n",
    "pil_img = Image(filename=f\"{images_dir}{random_file}\")\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb23f7c",
   "metadata": {},
   "source": [
    "### Step 1.3: Identify and create model objects for endpoints\n",
    "<a id='ml-setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16cc15",
   "metadata": {},
   "source": [
    "Next, you will create ML model objects. In this notebook, you will be using the Wide Resnet50 PyTorch model available in `s3:jumpstart-cache-prod-us-east-1/`, and for the supporting model container, AWS Deep Learning Containers for Pytorch. \n",
    "\n",
    "- You can view the list of all current JumpStart models using the `aws s3 ls jumpstart-cache-prod-{region}` command. \n",
    "- For all supported containers, see [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import model\n",
    "from time import gmtime, strftime\n",
    "\n",
    "time_stamp = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "pytorch_model_gpu = model.FrameworkModel(\n",
    "    name=f\"pytorch-model-gpu-{time_stamp}\",\n",
    "    model_data=f\"s3://jumpstart-cache-prod-{region}/pytorch-infer/infer-pytorch-ic-resnet50.tar.gz\",\n",
    "    image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.6.0-gpu-py3\",\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "pytorch_model_cpu = model.FrameworkModel(\n",
    "    name=f\"pytorch-model-cpu-{time_stamp}\",\n",
    "    model_data=f\"s3://jumpstart-cache-prod-{region}/pytorch-infer/infer-pytorch-ic-resnet50.tar.gz\",\n",
    "    image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py3\",\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef32cac",
   "metadata": {},
   "source": [
    "At this point, you have created two Amazon SageMaker models, for the CPU and GPU versions. The Amazon SageMaker endpoints will not be created yet, since deploying the endpoints is part of the process that will be automated based on your choice of instance types. The next step for you is to configure the threshold limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the minimum and maximum number of requests for your model here\n",
    "min_requests_per_second = 50\n",
    "max_requests_per_second = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67f0d2",
   "metadata": {},
   "source": [
    "### Optional: Using a model from AWS Marketplace (PyTorch ResNet50 Model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1a4f7",
   "metadata": {},
   "source": [
    "In certain cases, you may want to use models from the AWS Marketplace. You can follow the steps below to subscribe to the model package from the AWS Marketplace and create a dynamic model which you will configure for the load testing.\n",
    "\n",
    "For example, you could use the **Pytorch ResNet50** model available in the AWS Marketplace [here](https://aws.amazon.com/marketplace/ai/configuration?productId=f2590bef-2833-45ce-951b-55a28349e14f&ref=sa_campaign_ds_vj) in place of the existing model. To follow the same approach of the notebook, consider both the CPU and GPU ML models.\n",
    "\n",
    "\n",
    "1. Open the following ML models in separate tabs. Note that both ML models have application/x-image as the mime type.\n",
    "    * [Wide ResNet 50 - CPU](https://aws.amazon.com/marketplace/pp/prodview-dnc7grtzdiihs) \n",
    "    * [Wide ResNet 50 - GPU](https://aws.amazon.com/marketplace/pp/prodview-v2r2tm2tepa3o) \n",
    "    \n",
    "2. For both the ML models follow the following process:\n",
    "    1. Read the **Highlights** section and then **product overview** section of the listing.\n",
    "    2. View **usage information** and then **additional resources**.\n",
    "    3. Note the supported instance types.\n",
    "    4. Next, click on **Continue to subscribe**.\n",
    "    5. Review **End user license agreement**, **support terms**, as well as **pricing information**.\n",
    "    6. **\"Accept Offer\"** button needs to be clicked if your organization agrees with EULA, pricing information as well as support terms.\n",
    "    7. Choose **Continue to Configuration**.\n",
    "    8. Copy the **Product Arn** and specify the same below\n",
    "    \n",
    "3. Note down the model ARNs from AWS Marketplace\n",
    "```\n",
    "model_arn_cpu = \"arn:aws:sagemaker:us-east-1:865070037744:model-package/pytorch-ic-wide-resnet50-2-cpu-6a1d8d24bbc97d8de3e39de7e74b3293\"\n",
    "model_arn_gpu = \"arn:aws:sagemaker:us-east-1:865070037744:model-package/pytorch-ic-wide-resnet50-2-gpu-445fe358cb7a3a0d92861174cf00c113\"\n",
    "```\n",
    "\n",
    "4. Create CPU and GPU models on Amazon SageMaker from the given model ARNs\n",
    "\n",
    "```\n",
    "from sagemaker import ModelPackage\n",
    "\n",
    "pytorch_model_cpu = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_arn_cpu,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "pytorch_model_gpu = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_arn_gpu,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e20720",
   "metadata": {},
   "source": [
    "### Step 1.4: Set up Lambda Function and an API Gateway\n",
    "<a id='infra-setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92369d",
   "metadata": {},
   "source": [
    "**Prerequisite** - If you are using your own model, ensure that it is loaded on Amazon SageMaker to be hosted on an endpoint before proceeding with this step. \n",
    "\n",
    "To test the functioning of the model endpoint in a production environment, you will implement an API Gateway and an AWS Lambda function to direct the requests to the model endpoint. The infrastructure approach followed is the same as described in the blog [Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/).\n",
    "\n",
    "The blog suggests the following architecture:\n",
    "1. An Amazon API Gateway that receives the prediction requests\n",
    "2. A AWS Lambda function that sits behind the API Gateway and calls the endpoint for requests\n",
    "\n",
    "As a complimentary element of this infrastructure, you will also need to account for the roles and policies that allow the resources to call the required AWS services.\n",
    "\n",
    "To successfully set up the suggested infrastructure, you will also need corresponding IAM roles and policies that allow the resources to call the required AWS services. To make it easy for you to set up the infrasture suggested for the load test, you can use the `create_infra()` helper function. The function outputs the API gateway URL that will be used later on to request the predictions via POST requests.\n",
    "\n",
    "__Note:__\n",
    "If you are using your custom model, edit the lambda_index function to update the right payload (input data and content types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a58f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_index.py\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "import ast\n",
    "\n",
    "# SageMaker runtime is used to invoke the endpoint\n",
    "runtime = boto3.client(\n",
    "    \"runtime.sagemaker\",\n",
    "    config=Config(connect_timeout=30, read_timeout=60, retries={\"max_attempts\": 20}),\n",
    ")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Invokes SageMaker endpoint and return response\n",
    "\n",
    "    This function invokes a given SageMaker endpoint and returns\n",
    "    the endpoint response. SageMaker error codes are sent back for\n",
    "    mapping by API gateway if the invocation results in an error.\n",
    "\n",
    "    Inputs:\n",
    "    data - image object to be classified\n",
    "    endpoint - name of the endpoint to invoke\n",
    "\n",
    "    Output:\n",
    "    predicted response/error code.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data sent through API gateway\n",
    "    data = json.loads(json.dumps(event))\n",
    "    payload = ast.literal_eval(data[\"data\"])\n",
    "    endpoint_name = data[\"endpoint\"]\n",
    "\n",
    "    try:\n",
    "        # Invoke endpoint\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/x-image\",\n",
    "            Accept=\"application/json\",\n",
    "            Body=payload,\n",
    "        )\n",
    "\n",
    "        # Read success/failure response\n",
    "        response_code = response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "\n",
    "        return response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "    except ClientError as e:\n",
    "\n",
    "        # Return failure code\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ModelError\":\n",
    "            response = json.loads(e.response[\"OriginalMessage\"])\n",
    "            return {\"statusCode\": response[\"code\"], \"body\": response[\"type\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d728da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This function takes a few seconds to deploy the full architecture\n",
    "# including a lambda function and an API gateway\n",
    "# For easy identification of resources, specify a prefix for the application\n",
    "# Note: If you re-run this cell, rename the project_name to avoid 'entity exists' errors\n",
    "project_name = \"right-size-endpoints\"\n",
    "api_url = create_infra(project_name, account_id, region)\n",
    "\n",
    "# note the REST API id for clean up\n",
    "rest_api_id = api_url.replace(\"https://\", \"\").split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eaec5e",
   "metadata": {},
   "source": [
    "## Step 2: Load testing\n",
    "<a id='load-testing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00519298",
   "metadata": {},
   "source": [
    "In the second section, deploy the model in the form of multiple Amazon SageMaker Endpoints, to perform load testing. \n",
    "\n",
    "There are two ways of executing this notebook - \n",
    "- **Comprehensive testing**: In this section, you will deploy the supported instance types of the model. You will then perform testing across all supported instance types and run the load tests until the endpoint fails for each instance type. In this notebook, we \"fail\" an endpoint when the instance cannot respond to 1% of the incoming requests. Finally, you will plot performance in maximum requests per second against the price (assuming single instance endpoints), so that the you can make an informed decision on choosing the right instance. \n",
    "\n",
    "- **Semi-automatic testing**: If you are familiar with Amazon SageMaker instances and have a shortlisted set of instance types and number of instances that you would like to test, proceed to the [Semi-Automatic Testing](#semi-aut-testing) section, skipping the comprehensive testing.\n",
    "\n",
    "*Note: If you like to increase or decrease the error rate, you can edit it in line 15 of `locust_file.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6790d",
   "metadata": {},
   "source": [
    "### Step 2.1: Comprehensive testing\n",
    "<a id='comprehensive-testing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d0828",
   "metadata": {},
   "source": [
    "This section creates an endpoint each for each of the supported instance types. For the current model, you can find the supported instance types from SageMaker Studio -> JumpStart -> Choose model -> Deploy Model -> Deployment Configuration dropdown, as shown in the screenshot below.\n",
    "\n",
    "<img src=\"assets/Supported-types.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6de9f0",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the helper function to retrieve the list from the metadata file on S3 (at the time of writing this notebook, the latest configuration was `metadata-modelzoo_v3.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe253a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the latest metadata file locally\n",
    "from sagemaker_helper import get_supported_instance_types\n",
    "\n",
    "!aws s3 cp s3://jumpstart-cache-prod-us-east-1/metadata-modelzoo_v3.json metadata.json\n",
    "\n",
    "# Run function to print all supported inference instance types\n",
    "# update the second argument with the model name if you plan to use a different JumpStart model\n",
    "types = get_supported_instance_types(\"metadata.json\", \"pytorch-ic-resnet50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00510c03",
   "metadata": {},
   "source": [
    "For AWS ML Marketplace models - from the model configuration page [here](https://aws.amazon.com/marketplace/pp/prodview-dnc7grtzdiihs) under the 'Pricing' tab, you can obtain the supported instance types. \n",
    "\n",
    "Next, you will create multiple endpoints, one of each instance type, and keep the instance count at 1 (as specified in the following configuration). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642f2f1",
   "metadata": {},
   "source": [
    "### Step 2.1.1  Deploy endpoints\n",
    "<a id='deploy-ep'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0957748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Create a dict with the supported instances if running for a different model\n",
    "\n",
    "endpoints_dict = []\n",
    "for _type in types:\n",
    "    endpoints_dict.append({\"instance_type\": _type, \"instance_count\": 1})\n",
    "\n",
    "print(endpoints_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The endpoints are created synchronously, so this cell will take a few minutes to execute\n",
    "endpoints = deploy_endpoints(endpoints_dict, pytorch_model_cpu, pytorch_model_gpu)\n",
    "print(endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720828b9",
   "metadata": {},
   "source": [
    "**Note**: If you encounter a throttling exception when deploying models, you can execute the cell again and it will deploy only the missing endpoints. For futher information on resolving the exception, see [How can I resolve SageMaker Python SDK rate exceeded and throttling exceptions?](https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-python-throttlingexception/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021b057",
   "metadata": {},
   "source": [
    "### Step 2.1.2: Test endpoints with a sample payload\n",
    "<a id='test-eps'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26df86",
   "metadata": {},
   "source": [
    "Now that there are multiple endpoints, let's test all the endpoints to make sure they are running without any errors. A HTTP 200 response for the requests ensures that the model is working as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a random image from the images folder\n",
    "random_file = random.choice(os.listdir(images_dir))\n",
    "\n",
    "# Create an Image object by reading the image\n",
    "with open(f\"{images_dir}{random_file}\", \"rb\") as f:\n",
    "    image = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the lambda function once for each endpoint and check for HTTP 200 response\n",
    "for ep in endpoints:\n",
    "    # Create a payload for Lambda - input variables are the image and endpoint name\n",
    "    payload = {\"data\": str(image), \"endpoint\": ep}\n",
    "\n",
    "    response = requests.post(api_url, json=payload)\n",
    "    print(f\"Endpoint {ep}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf54730",
   "metadata": {},
   "source": [
    "### Step 2.1.3: Execute load tests\n",
    "<a id='run-load-tests'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff2ca0",
   "metadata": {},
   "source": [
    "Now, to run the load tests, you will execute the `run_load_tests` function that calls a short bash script that runs locust.\n",
    "\n",
    "To run the tests, you will just need to pass the list of endpoints to test to the function `run_load_tests()`. The load test generates multiple comma separated files (.csv) with the test results and store them in a folder with the name `results-YYYY-MM-DD-HH-MM-SS`. Inside this folder you can find the individual test results for each endpoint instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff213fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the run_locust file\n",
    "!cat run_locust.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8f9cb",
   "metadata": {},
   "source": [
    "__Run_locust.sh breakdown__\n",
    "\n",
    "The `run_locust.sh` file takes in endpoint name and the created API gateway URL as inputs and invokes the load test. \n",
    "\n",
    "The `env` prefix specifies environment variables that are sent to the locust file. Since we're testing multiple endpoints, send the endpoint name as a variable.\n",
    "\n",
    "`locust -f locust_file.py` runs the load tests. Refer to the file for details on how the load testing is conducted.\n",
    "\n",
    "`--headless` runs the test without a web UI. Refer [here](https://docs.locust.io/en/stable/running-locust-without-web-ui.html) for documentation.\n",
    "\n",
    "`--csv` specifies a prefix to store the test results. Here, specify the endpoint name for easy identification.\n",
    "\n",
    "`-u` specifies the number of users to spawn.\n",
    "\n",
    "`-r` specifies the spawn rate.\n",
    "\n",
    "`--host` specifies the endpoint URL to test.\n",
    "\n",
    "\n",
    "For all available confiuration options, refer to the [Locust documentation.](https://docs.locust.io/en/stable/configuration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52699c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This cell runs the load tests on all endpoints. Grab a cup of coffee and wait for it to complete!\n",
    "\n",
    "# NOTE: If you are using a smaller instance than a ml.m5.4xlarge, this cell might take from\n",
    "# minutes to hours to run. On the ml.m5.xlarge instance for the given endpoints, this took\n",
    "# about 30 minutes.\n",
    "\n",
    "# You can also test the invocation metrics on the CloudWatch console\n",
    "results_folder = run_load_tests(api_url, endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c1b1e",
   "metadata": {},
   "source": [
    "**Background on Locust load tests**\n",
    "\n",
    "The `run_load_tests` function performs load tests and organizes the resulting files into a single folder. Locust saves up to four files for each load test with the following suffixes, similar to the directory structure below.\n",
    "\n",
    "```\n",
    "├── results-<timestamp>\n",
    "│   ├── <endpoint-name>\n",
    "│   │   ├── <endpoint-name>_failures.csv\n",
    "│   │   ├── <endpoint-name>_stats.csv\n",
    "│   │   └── <endpoint-name>_stats_history.csv\n",
    "│   │   └── <endpoint-name>_exceptions.csv (optional)\n",
    "│   ├── <endpoint-name> ...\n",
    "```\n",
    "The first two files contain the failures and stats for the whole test run, with a row for every stats entry and an aggregated row. The stats history will get new rows with the current (10 seconds sliding window) stats appended during the whole test run. You can find more information on the files and how to increase/decrease the interval of writing stats in the [documentation](https://docs.locust.io/en/stable/retrieving-stats.html).\n",
    "\n",
    "In the next section, plot the maximum number of requests per second handled by each endpoint. You are also recommended to explore the files generated to check for exceptions, failures, users/requests generated etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df8467",
   "metadata": {},
   "source": [
    "### Step 2.1.4: Performance vs price plot\n",
    "<a id='plot'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17e4ea",
   "metadata": {},
   "source": [
    "AWS constantly updates instance prices to provide best value to our customers. The Price List Service API (AKA the Query API) and AWS Price List API (AKA the Bulk API) enables you to query for the prices of AWS services using either JSON (with the Price List Service API) or HTML (with the AWS Price List API). To query the instance prices in real time, your Amazon SageMaker execution role must have permissions to access the service (refer to the [Prerequisites](#prereq) section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687263fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your results in a single chart to compare instance types\n",
    "# Set sep_cpu_gpu=True if you want the results plotted in two separate\n",
    "# graphs for the CPU and GPU instances\n",
    "results = generate_plots(endpoints, endpoints_dict, results_folder, sep_cpu_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e43e2d",
   "metadata": {},
   "source": [
    "**Load test analysis**\n",
    "1. The GPU instances perform significantly better for image classification problems, with the least metric at 25 requests per second for USD1.125 per hour ( ml.p2.xlarge instance), versus the most expensive CPU providing around 4 requests per second at USD0.40 per hour (USD2.80 for 25 requests per second, or 2.5 times more expensive).\n",
    "\n",
    "2. Within the GPU instances, the `ml.g4dn.xlarge` instance is the industry's most cost-effective GPU instance for deploying machine learning models that are graphics-intensive such as image classification, object detection etc. At USD0.70 an hour, it performs twice as better as the next cheaper option, the `ml.p2.xlarge`. \n",
    "\n",
    "3. Depending on your requests per second (10 rps or lower versus 50 rps or more), you can choose the CPU or the GPU option. If you go the GPU route, the G4 instance clearly emerges the winner. In the next section, we will programmatically obtain the recommended instance type and autoscaling configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cc07d",
   "metadata": {},
   "source": [
    "__Optional: View latency metrics__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d4d1d",
   "metadata": {},
   "source": [
    "Optionally, if latency is what you desire, you can also view the average, minimum and maximum response times for each endpoint using the `generate_latency_plot` function from the helper file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latency_plot(endpoints, results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc82156",
   "metadata": {},
   "source": [
    "**Latency metrics analysis**\n",
    "\n",
    "The minimum response times stay at below one second for all endpoints regardless of the instance type. However, the average response time is the least (quickest) for the three GPU isntances, with the `ml.g4dn.xlarge` and `ml.p3.2xlarge` having the lowest average response times. \n",
    "\n",
    "**Note** - The response time includes model processing time and the API Gateway/Lambda processing time. If a faster response is required for your application, you can use the Locust result file to examine the response times and choose accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f5d63",
   "metadata": {},
   "source": [
    "### Step 2.1.5 Finalize configuration\n",
    "<a id='recommendations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d598ed",
   "metadata": {},
   "source": [
    "Now that you are done with the \"grid search\" to run all possible endpoints, you can decide on the final endpoint configuration for your use case based on the number of average and maximum requests per second expected for your application.\n",
    "\n",
    "In the following cell, enter the minimum and maximum requests and then run the remaining section to print the recommended instances for autoscaling. If your application or endpoint gets spiky loads, it is a good idea to configure an auto-scaling configuration. \n",
    "\n",
    "For other best practices on ML deployment, see [Deployment Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/best-practices.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get suggestions for a specific payload\n",
    "\n",
    "# This function calculates the number of instances based on linear scaling\n",
    "# and returns the suggested instance type and counts\n",
    "phase2_endpoints_dict = get_min_max_instances(\n",
    "    results, min_requests_per_second, max_requests_per_second\n",
    ")\n",
    "\n",
    "# Print recommended number of instances based on linear scaling\n",
    "print(\"Recommended endpoint configuration for the specified number of requests: \")\n",
    "print(\"________________________________________________________________\")\n",
    "print(f\"Instance Type: {phase2_endpoints_dict[0]['instance_type']}\")\n",
    "print(f\"Minimum Instance Count: {phase2_endpoints_dict[0]['instance_count']}\")\n",
    "print(f\"Maximum Instance Count: {phase2_endpoints_dict[1]['instance_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b2359",
   "metadata": {},
   "source": [
    "The function assumes linear scaling while calculating the minimum and maximum number of instances needed to serve your expected requests per second. The code calculates the price for each instance type and instance count, and returns the least expensive option. \n",
    "\n",
    "Test the endpoints with the given instance count configurations to ensure that they can serve your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Optional: test the instances\n",
    "phase2_endpoints = deploy_endpoints(phase2_endpoints_dict, pytorch_model_cpu, pytorch_model_gpu)\n",
    "phase2_results_folder = run_load_tests(api_url, phase2_endpoints)\n",
    "generate_plots(phase2_endpoints, phase2_endpoints_dict, phase2_results_folder, sep_cpu_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcf886",
   "metadata": {},
   "source": [
    "As we can see, the endpoint with two instances `ml.g4dn.xlarge` can withstand our load requirements of 110 requests per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc83cc4",
   "metadata": {},
   "source": [
    "---------------\n",
    "**Additional note:** Amazon provides you with the ability to automaticaly scale endpoints for your hosted models. When workload increases, autoscaling provisions more instances, and when workload decreases, it removes unnecessary instances so you don't pay for instances that you aren't using.\n",
    "\n",
    "The main components of an autoscaling policy are - \n",
    "1. A target metric - an Amazon CloudWatch metric that is monitored to determine if and when to scale\n",
    "2. Minimum and maximum capacity - minimum and maximum number of instances (provided in the cell above)\n",
    "3. Cool down period - time, in seconds, after a scale-in or scale-out activity completes before another scale-out activity can start\n",
    "4. IAM policy and role to allow Amazon SageMaker to configure autoscaling.\n",
    "\n",
    "Next, you can update the **min_capacity** and the **max_capacity** you identified in the preceding cell, and adjust the target value, scale in and scale out cool down values in the following configuration. -\n",
    "```\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic'\n",
    "\n",
    "client = boto3.client('application-autoscaling')\n",
    "\n",
    "response = client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=5, # obtained from load testing\n",
    "    MaxCapacity=7\n",
    ")\n",
    "\n",
    "response = client.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id, # Endpoint name \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        # update target value, scale in and scale out cool down values as per your requirements\n",
    "        'TargetValue': 1000.0, # The target value for the metric (in minutes)\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance', \n",
    "        'ScaleInCooldown': 600,  # wait 600 seconds before terminating additional instances\n",
    "        'ScaleOutCooldown': 300 # wait 300 seconds before adding new instances\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "For more information, see [Automatically Scale Amazon SageMaker Models](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c8a59",
   "metadata": {},
   "source": [
    "### Step 2.2: Semi-automatic testing\n",
    "<a id='semi-aut-testing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8f2f7",
   "metadata": {},
   "source": [
    "As commented previously, the semi-automatic testing is intended for a user more experienced with Amazon Sagemaker, and who perhaps has interest on running the iteration process of finding the best instance, manually.\n",
    "\n",
    "To be able to run this, you need to know, or have an idea, of what the maximum and average requests per second your endpoint will be serving. This is a key starting point to be able to use the functionality of this notebook.\n",
    "\n",
    "As a rule of thumb, test the following two different instance types first, with a single instance of each:\n",
    "\n",
    "1. The \"most expensive\" CPU that can host your model (cost per hour).\n",
    "2. The \"cheapest\" GPU that can host your model.\n",
    "\n",
    "Then, use your minimum and maximum requests per second goal (rps) that you will need to handle, throughout the decision process below.\n",
    "\n",
    "\n",
    "**NOTE**: The idea behind this approach is to identify what type of instance might suit you better. \n",
    "\n",
    "The possible outcome results of this first iteration are as follows:\n",
    "\n",
    "1. If your target rps is above the max rps achieved by a single instance of the cheapest available GPU, it is possible that you will have either to use a more expensive GPU or multiple instances of the cheapest one. --> In this scenario, it is very likely that the CPU instances might not be the best option (in some cases you will have to launch too many CPUs to match the performance of a single GPU, for a higher price).\n",
    "\n",
    "<img src=\"assets/semi-auto-scenario1.png\" width=\"250\">\n",
    "\n",
    "2. If your target rps are below the max rps achieved by a single instance of the most expensive CPU, it is possible that you will be able to use a cheaper CPU instance, or just stay with the one you tested (depending on the target rps vs the max rps achieved).  --> In this scenario, it is very likely that the GPU instances might not be the best option.\n",
    "\n",
    "<img src=\"assets/semi-auto-scenario2.png\" width=\"250\">\n",
    "\n",
    "3. If your target rps are between the max rps achieved by the cheapest GPU and the most expensive CPU, then you can evaluate from a cost perspective what might be the best - either use multiple CPU instances (assuming linear scaling), or use a single GPU.\n",
    "\n",
    "<img src=\"assets/semi-auto-scenario3.png\" width=\"250\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf317e",
   "metadata": {},
   "source": [
    "### Step 2.2.1: First iteration\n",
    "<a id='first-iteration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5be92",
   "metadata": {},
   "source": [
    "For the model that we will be using (**Pytorch ResNet50**), the most expensive CPU based instance is the `ml.c5.2xlarge` and the cheapest GPU based instance is `ml.g4dn.xlarge`. Deploy the model to these two instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be100b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_endpoints_dict = [\n",
    "    {\"instance_type\": \"ml.c5.2xlarge\", \"instance_count\": 1},\n",
    "    {\"instance_type\": \"ml.g4dn.xlarge\", \"instance_count\": 1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2591149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the endpoints and plot max requests per second for each endpoint type\n",
    "\n",
    "sat_endpoints = deploy_endpoints(sat_endpoints_dict, pytorch_model_cpu, pytorch_model_gpu)\n",
    "sat_results_folder = run_load_tests(api_url, sat_endpoints)\n",
    "results = generate_plots(sat_endpoints, sat_endpoints_dict, sat_results_folder, sep_cpu_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically get suggested instance type and counts\n",
    "sa_endpoints_dict = get_min_max_instances(results, min_requests_per_second, max_requests_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa087a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recommended number of instances based on linear scaling\n",
    "print(\"Recommended endpoint configuration for the specified number of requests: \")\n",
    "print(\"________________________________________________________________\")\n",
    "print(f\"Instance Type: {sa_endpoints_dict[0]['instance_type']}\")\n",
    "print(f\"Minimum Instance Count: {sa_endpoints_dict[0]['instance_count']}\")\n",
    "print(f\"Maximum Instance Count: {sa_endpoints_dict[1]['instance_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c6086",
   "metadata": {},
   "source": [
    "In the results above, the goal of average rps exceeds both the capacity of the most expensive CPU and the cheapest GPU. In addition, the recommended number/type of instances (assuming linear scaling) favours the GPU rather than the CPU.\n",
    "\n",
    "In this sense, our exploratory work implies that we need to proceed with a second iteration to see which instances can serve our model properly.\n",
    "\n",
    "_Note: If you updated the maximum and minimum requests per second, your results may vary._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6afa2",
   "metadata": {},
   "source": [
    "### Step 2.2.2: Second iteration\n",
    "<a id='second-iteration'></a>\n",
    "\n",
    "In this second iteration, since you already found that the CPU instances perform worse, run a test comparing only GPUs.\n",
    "\n",
    "In the previous test, the `ml.g4dn.xlarge` instance serves up to ~50rps, so assuming a linear scalability, test if using 2 instances in an endpoint will allow it to serve 100rps, to verify linear scaling.\n",
    "\n",
    "In addition, test the performance of the most expensive GPU (available for this model) to see how it would perform in comparison with the `ml.g4dn.xlarge x 2`. Depending on the results, you can decide if another iteration is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d536d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_p2_endpoints_dict = [\n",
    "    {\"instance_type\": \"ml.p3.2xlarge\", \"instance_count\": 1},\n",
    "    {\"instance_type\": \"ml.g4dn.xlarge\", \"instance_count\": 2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the endpoints\n",
    "sat_p2_endpoints = deploy_endpoints(sat_p2_endpoints_dict, pytorch_model_cpu, pytorch_model_gpu)\n",
    "\n",
    "# Run the load tests and organize the data\n",
    "sat_p2_results_folder = run_load_tests(api_url, sat_p2_endpoints)\n",
    "\n",
    "# Generate the plots showing the results\n",
    "results = generate_plots(\n",
    "    sat_p2_endpoints, sat_p2_endpoints_dict, sat_p2_results_folder, sep_cpu_gpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20230147",
   "metadata": {},
   "source": [
    "In this case, the `ml.g4dn.xlarge x 2` endpoint managed to deal with 100rps (actually, a bit more), whereas the `ml.p3.2xlarge` started to fail close to 50rps (very similar performance to the `ml.g4dn.xlarge`).\n",
    "\n",
    "Now, with regards to price, the `ml.g4dn.xlarge x 2` configuration looks like the final choice, in the sense that even though it has 2 instances, its price is even lower than a single instance `ml.p3.2xlarge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a02259",
   "metadata": {},
   "source": [
    "## Step 3. Clean up Resources\n",
    "<a id='clean-up'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e28052",
   "metadata": {},
   "source": [
    "Remember to delete the endpoints (optionally unsubscribe from AWS Marketplace if you used a ML Marketplace model) once your tests are complete to avoid incurring additional daily/monthly costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a63d00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete endpoints from comprehensive testing and semi-automatic testing\n",
    "clean_up_endpoints(endpoints + phase2_endpoints + sat_endpoints + sat_p2_endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7c890",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/models) created from the model package or using the algorithm. \n",
    "\n",
    "Note - You can find this information by looking at the container name associated with the model.\n",
    "\n",
    "__Steps to unsubscribe to product from AWS Marketplace:___\n",
    "1. Navigate to __Machine Learning__ tab on [Your Software subscriptions page](https://aws.amazon.com/marketplace/ai/library?productType=ml)\n",
    "2. Locate the listing that you would need to cancel subscription for, and then choose __Cancel Subscription__ to cancel the subscription.\n",
    "\n",
    "Note: If you do not delete the endpoint, you will be charged for the model as long as it is in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068425a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete infrastructure created to load test the endpoints\n",
    "models_list = [pytorch_model_cpu.name, pytorch_model_gpu.name]\n",
    "\n",
    "delete_infra(project_name, account_id, rest_api_id, models_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "404.567px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
