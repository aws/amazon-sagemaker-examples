{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Your Own Model with SageMaker Script Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how you can bring your own model by using custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker's prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.\n",
    "\n",
    "SageMaker Script Mode is flexible so you'll also be seeing examples of how to include your own dependencies, such as a custom Python library, in your training and inference.\n",
    "\n",
    "The following diagram provides a solution overview:\n",
    "\n",
    "<img title=\"SageMaker Script Mode\" alt=\"Solution diagram\" src=\"solution-diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along, you need to create an IAM role, SageMaker Notebook instance, and S3 bucket. You may click on the CloudFormation button which will create the aforementioned resources and clone the `amazon-sagemaker-examples` GitHub repo into the notebook instance. [![Launch Stack](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home#/stacks/new?stackName=ScriptModeDemo&templateURL=https://script-mode-blog.s3.amazonaws.com/script-mode-blog-cfn.yml). Give the S3bucket a unique name; you can also give the CloudFormation stack and notebook unique names such as \"script mode\". You can leave the other default settings in the CloudFormation template.\n",
    "\n",
    "Once the SageMaker Notebook instance is created, choose `conda_python3` as the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.serializers import NumpySerializer, JSONSerializer, CSVSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer, JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from generate_synthetic_housing_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SageMaker version is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.24.1\":\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.24.1\"])\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Useful SageMaker variables\n",
    "try:\n",
    "    # You're using a SageMaker notebook\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # You're using a notebook somewhere else\n",
    "    print(\"Setting role and SageMaker session manually...\")\n",
    "    bucket = \"bobby-demo\"\n",
    "    region = \"us-west-2\"\n",
    "\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    sagemaker_execution_role_name = (\n",
    "        \"AmazonSageMaker-ExecutionRole-20200630T141851\"  # Change this to your role name\n",
    "    )\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    boto3.setup_default_session(region_name=region, profile_name=\"default\")\n",
    "    sess = sagemaker.Session(sagemaker_client=sagemaker_client, default_bucket=bucket)\n",
    "\n",
    "# Local data paths\n",
    "train_dir = os.path.join(os.getcwd(), \"data/train\")\n",
    "test_dir = os.path.join(os.getcwd(), \"data/test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Data paths in S3\n",
    "s3_prefix = \"script-mode-workflow\"\n",
    "csv_s3_prefix = f\"{s3_prefix}/csv\"\n",
    "csv_s3_uri = f\"s3://{bucket}/{s3_prefix}/csv\"\n",
    "numpy_train_s3_prefix = f\"{s3_prefix}/numpy/train\"\n",
    "numpy_train_s3_uri = f\"s3://{bucket}/{numpy_train_s3_prefix}\"\n",
    "numpy_test_s3_prefix = f\"{s3_prefix}/numpy/test\"\n",
    "numpy_test_s3_uri = f\"s3://{bucket}/{numpy_test_s3_prefix}\"\n",
    "csv_train_s3_uri = f\"{csv_s3_uri}/train\"\n",
    "csv_test_s3_uri = f\"{csv_s3_uri}/test\"\n",
    "\n",
    "# Enable Local Mode training\n",
    "enable_local_mode_training = False\n",
    "\n",
    "# Endpoint names\n",
    "sklearn_endpoint_name = \"randomforestregressor-endpoint\"\n",
    "pytorch_endpoint_name = \"pytorch-endpoint\"\n",
    "xgboost_endpoint_name = \"xgboost-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/local_mode_setup.sh\n",
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/daemon.json\n",
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Synthetic Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the examples below, we'll be generating a synthetic housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_houses(1506)\n",
    "\n",
    "# Get training columns\n",
    "train_cols = list(df.columns)\n",
    "del train_cols[-1]\n",
    "train_cols\n",
    "\n",
    "# Split data\n",
    "training_index = math.floor(0.8 * df.shape[0])\n",
    "x_train, y_train = df[train_cols][:training_index], df.PRICE[:training_index]\n",
    "x_test, y_test = df[train_cols][training_index:], df.PRICE[training_index:]\n",
    "\n",
    "# Scale price\n",
    "y_train = y_train / 100000\n",
    "y_test = y_test / 100000\n",
    "\n",
    "# Standardize data\n",
    "x_train_np = StandardScaler().fit_transform(x_train)\n",
    "x_test_np = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearrange dataframe for SageMaker training and scale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data=x_train_np)\n",
    "train_df.columns = x_train.columns\n",
    "train_df[\"PRICE\"] = y_train / 100000\n",
    "first_col = train_df.pop(\"PRICE\")\n",
    "train_df.insert(0, \"PRICE\", first_col)\n",
    "\n",
    "test_df = pd.DataFrame(data=x_test_np)\n",
    "test_df.columns = x_test.columns\n",
    "test_df[\"PRICE\"] = y_test.reset_index(drop=True) / 100000\n",
    "first_col = test_df.pop(\"PRICE\")\n",
    "test_df.insert(0, \"PRICE\", first_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as both CSV and Numpy data types to demonstrate data type flexibility in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "train_df.to_csv(f\"{train_dir}/train.csv\", header=False, index=False)\n",
    "test_df.to_csv(f\"{test_dir}/test.csv\", header=False, index=False)\n",
    "\n",
    "# Save as Numpy\n",
    "np.save(os.path.join(train_dir, \"x_train.npy\"), x_train_np)\n",
    "np.save(os.path.join(test_dir, \"x_test.npy\"), x_test_np)\n",
    "np.save(os.path.join(train_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(test_dir, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource_bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, \"train.csv\")).upload_file(\n",
    "    \"data/train/train.csv\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, \"test.csv\")).upload_file(\"data/test/test.csv\")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"x_train.npy\")).upload_file(\n",
    "    \"data/train/x_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"y_train.npy\")).upload_file(\n",
    "    \"data/train/y_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"x_test.npy\")).upload_file(\n",
    "    \"data/test/x_test.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"y_test.npy\")).upload_file(\n",
    "    \"data/test/y_test.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first \"level\" of script mode is the ability to define your own training job, model, and inference process without any dependencies. This is done using a customized python script and pointing that script as the \"entry point\" when defining your SageMaker training estimator. There is no \"out-of-the-box\" random forest algorithm on SageMaker, but there is support for scikit-learn containers which does have random forest implementations, including regressors and classifiers. Here, we demonstrate the implementation of a custom random forest regressor to predict housing prices using our synthetic housing data set.\n",
    "\n",
    "Script Mode in SageMaker allows you to take control of the training and inference process without having to go through the trouble of creating and maintaining your own docker containers. For example, if you want to use a scikit-learn algorithm, just use the AWS-provided scikit-learn container and pass it your own training and inference code. On your behalf, the SageMaker Python SDK will package this entry point script (which can be your training and/or inference code), upload it to S3, and set two environment variables that are read at runtime and load the custom training and inference functions from the entry point script. These two environment variables are `SAGEMAKER_SUBMIT_DIRECTORY` which is set to the S3 path of the package and `SAGEMAKER_PROGRAM` which is set to the name of the script (which in our case is `train_deploy_scikitlearn_without_dependencies.py`).\n",
    "\n",
    "The process is the same if you want to use an XGBoost model (use the XGBoost container) or a custom PyTorch model (use the PyTorch container). Since you're passing in your own script (which is why we call it \"script mode\"), you get to define the model, the training process, and the inference process as well.\n",
    "\n",
    "Below we include an entry point script called `train_deploy_scikitlearn_without_dependencies.py` which contains our custom training and inference code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\"max_depth\": 20, \"n_jobs\": 4, \"n_estimators\": 120}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\", \"test\": f\"file://{test_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": csv_train_s3_uri, \"test\": csv_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_scikitlearn_without_dependencies.py\",\n",
    "    \"source_dir\": \"scikitlearn_script\",\n",
    "    \"framework_version\": \"1.0-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"randomforestregressor-model\",\n",
    "}\n",
    "\n",
    "estimator = SKLearn(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=sklearn_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    sklearn_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=sklearn_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    sklearn_predictor = Predictor(\n",
    "        endpoint_name=\"randomforestregressor-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=NumpySerializer(),\n",
    "        deserializer=NumpyDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the SageMaker endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second \"level\" of script mode is the ability to modularize and logically organize your custom training jobs, models, and inference processes.\n",
    "\n",
    "Sometimes keeping all your code in one Python file can be unwieldy. Script Mode gives you the flexibility to parse out your code into multiple Python files. To illustrate this feature we build a custom PyTorch model and logically separate the model definition from the the training and inference logic. This is done by stipulating the source directory when defining your SageMaker training estimator (illustrated below). Once again, the model is not supported \"out-of-the-box\", but the PyTorch framework is and can be leveraged in the same manner as scikit-learn was in the previous example.\n",
    "\n",
    "In this PyTorch example, we want to separate the actual neural network definition from the rest of the code by putting it into its own file as demonstrated in the `pytorch_script/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epochs\": 25, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\", \"test\": f\"file://{test_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": numpy_train_s3_uri, \"test\": numpy_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_pytorch_without_dependencies.py\",\n",
    "    \"source_dir\": \"pytorch_script\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"pytorch-model\",\n",
    "    \"framework_version\": \"1.5\",\n",
    "    \"py_version\": \"py3\",\n",
    "}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, after the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=pytorch_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    pytorch_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=pytorch_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    pytorch_predictor = Predictor(\n",
    "        endpoint_name=\"pytorch-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor.serializer = JSONSerializer()\n",
    "pytorch_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "pytorch_predictor.predict(x_test.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third \"level\" of script mode is the ability to bring your own libraries and dependencies to support custom functionality within your models, training jobs, and inference processes. This supercharges your customization options, and allows you to import libraries you have created yourself or Python packages hosted on PyPi.\n",
    "\n",
    "Perhaps the number of Python files you have is becoming unwieldy now or you want more organization. In this scenario, you might be tempted to create your own Python library. Or maybe you wish to implement a function not currently supported by SageMaker in the training phase (such as  k-fold cross validation).\n",
    "\n",
    "Script Mode supports adding custom libraries and those libraries don't have to be in the same directory as your entry point Python script. You simply need to stipulate the custom library or other dependencies when defining your SageMaker training estimator (illustrated below). SageMaker will copy the library folder to the same folder where the entry point script is located when the training job is kicked off.\n",
    "\n",
    "In this example, we implement k-fold cross validation for an XGBoost model using a custom built library called `my_custom_library`. While XGBoost is supported \"out-of-the-box\" on SageMaker, that version does not support k-fold cross validation for training. Thus we use script mode to leverage the supported XGBoost container and the concomitant flexibility to include our custom libraries and dependencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"num_round\": 6, \"K\": 5}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": csv_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_xgboost_with_dependencies.py\",\n",
    "    \"source_dir\": \"xgboost_script\",\n",
    "    \"dependencies\": [\"my_custom_library\"],\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"xgboost-model\",\n",
    "    \"framework_version\": \"1.0-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "}\n",
    "\n",
    "estimator = XGBoost(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the model with k-fold cross validation, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=xgboost_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=xgboost_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    xgboost_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=xgboost_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    xgboost_predictor = Predictor(\n",
    "        endpoint_name=\"xgboost-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor.serializer = CSVSerializer()\n",
    "xgboost_predictor.deserializer = JSONDeserializer()\n",
    "xgboost_predictor.predict(x_test.values[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = (\n",
    "    [sklearn_endpoint_name, sklearn_predictor],\n",
    "    [pytorch_endpoint_name, pytorch_predictor],\n",
    "    [xgboost_endpoint_name, xgboost_predictor],\n",
    ")\n",
    "\n",
    "for resource in resources:\n",
    "    existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "        NameContains=resource[0], MaxResults=30\n",
    "    )[\"Endpoints\"]\n",
    "    if existing_endpoints:\n",
    "        resource[1].delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
