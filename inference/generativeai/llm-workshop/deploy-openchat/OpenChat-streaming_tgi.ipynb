{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b4d8cc-1c25-4acf-b42d-312d9cee942d",
   "metadata": {},
   "source": [
    "# Real-Time Inference Streaming with HuggingFace TGI and the OpenChat Model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269967fc-cbb4-466e-b7ce-9cfbf6eef9e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431b4ab-eb5f-48f0-90c8-80eac7bce47e",
   "metadata": {},
   "source": [
    "This tutorial will guide you through using the [Hugging Face Large Language Model](https://huggingface.co/blog/sagemaker-huggingface-llm) Inference Container on Amazon SageMaker to deploy OpenChat, which is a conversational AI assistant that uses large language models to engage in open-ended dialogue and assist with a variety of tasks. You'll run inference streaming with this container, which is powered by [Text Generation Inference](https://github.com/huggingface/text-generation-inference) (TGI) - an open-source, purpose-built solution for deploying and serving LLMs.\n",
    "\n",
    "TGI enables high-performance text generation by leveraging Tensor Parallelism and dynamic batching. It supports the most popular open-source LLMs, including StarCoder, BLOOM, GPT-NeoX, Llama, and T5.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aedbedc-1e30-4aec-998a-08e82ad5d212",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- OpenChat is an innovative library of open-source language models, fine-tuned with [C-RLFT](https://arxiv.org/pdf/2309.11235.pdf) - a strategy inspired by offline reinforcement learning.\n",
    "\n",
    "- The models learn from mixed-quality data without preference labels, delivering exceptional performance on par with `ChatGPT`, even with a `7B` model which can be run on a consumer GPU (e.g. RTX 3090).\n",
    "\n",
    "- Despite their simple approach, they are committed to developing a high-performance, commercially viable, open-source large language model, and continue to make significant strides toward this vision.\n",
    "\n",
    "For more information:\n",
    "\n",
    "- [OpenChat Git repo](https://github.com/imoneoi/openchat)\n",
    "- [Huggingface - OpenChat](https://huggingface.co/openchat)\n",
    "- [Research Paper](https://arxiv.org/pdf/2309.11235.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ea645-1436-4649-a163-9e4e7b2dd608",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Installing and importing dependencies\n",
    "\n",
    "We begin by importing the necessary libraries and configuring several global variables using the boto3 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3565b7-eb65-4536-960d-15ae93ebe8aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker pip boto3 botocore --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d64dbb4-d069-4d77-882b-7fa67541b185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.base_deserializers import StreamDeserializer\n",
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import io\n",
    "\n",
    "sagemaker_session = Session()\n",
    "role = sagemaker_session.get_caller_identity_arn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb65fb0-4f26-492d-8938-c02c2ec5ba04",
   "metadata": {},
   "source": [
    "Deploying Hugging Face models in Amazon SageMaker is slightly different from deploying regular Hugging Face models. To do this, you need to first retrieve the container URI and provide it to your HuggingFaceModel model class, along with an `image_uri` that points to the image you want to use.\n",
    "\n",
    "To retrieve the URI for the new Hugging Face Large Language Model (LLM) Deep Learning Containers (DLC) in Amazon SageMaker, you can use the `get_huggingface_llm_image_uri()` method provided by the SageMaker SDK. This method allows you to retrieve the URI for the desired Hugging Face LLM DLC based on the specified backend, session, AWS region, and version.\n",
    "\n",
    "By using this method, you can easily obtain the necessary container URI to deploy your Hugging Face LLM model in Amazon SageMaker, without having to manually look up and manage the URI information. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a91a29c-97e8-4f92-9481-79db68dd45e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.2-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\"huggingface\")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f1bd18-2ded-4554-8216-1dbc4a01ccae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_model_id = \"openchat/openchat-3.5-0106\"  # model id from huggingface.co/models\n",
    "number_of_gpus = 4  # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = (\n",
    "    300  # Increase the timeout for the health check to 5 minutes for downloading the model\n",
    ")\n",
    "instance_type = \"ml.g5.12xlarge\"  # the instance type ml.g5.12xlarge has 4 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535ddb5-e972-4e01-8ead-8a5b1d1a2a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Deploy OpenChat using the TGI image\n",
    "To deploy the `HuggingFaceModel` on Amazon SageMaker, we'll use the `deploy` method. We'll be deploying the model on the `ml.g5.12xlarge` instance type, as specified earlier. Details of the below environment variables are described at [here](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28857d57-8e28-43ef-b77c-51bf45edc670",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.name_from_base(\"tgi-model-openchat\")\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": hf_model_id,\n",
    "        \"SM_NUM_GPUS\": str(number_of_gpus),\n",
    "        \"MAX_INPUT_LENGTH\": \"1024\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"2048\",\n",
    "        \"HF_MODEL_REVISION\": \"dfcf6be1e44eb54db7af0d05d2760fb1d4969845\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086bd1fb-2246-4a94-b338-2fa7134debb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac0cce-1c19-4953-a84b-e816853b55f7",
   "metadata": {},
   "source": [
    "## Step 3: Initiate streaming inference requests to the deployed SageMaker model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cfbb83-dc0e-487a-9195-907246fd578c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.set_stream_logger(\"\", logging.INFO)\n",
    "llm.deserializer = StreamDeserializer()\n",
    "smr = boto3.client(\"sagemaker-runtime\")\n",
    "stop_token = \"<|endoftext|>\"\n",
    "prompt = \"How to study GenAI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc22ecaf-7a3e-4061-ab80-753831077af5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 2041, \"return_full_text\": False},\n",
    "    \"stream\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2bff3e-69f8-44e4-8baa-108c6a9ba846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input from a TGI (Text Generation Interface) container.\n",
    "\n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'data:{\"token\": {\"text\": \" a\"}}\\n\\n'\n",
    "    b'data:{\"token\": {\"text\": \" challenging\"}}\\n\\n'\n",
    "    b'data:{\"token\": {\"text\": \" problem\"\n",
    "    b'}}'\n",
    "    ...\n",
    "    ```\n",
    "\n",
    "    While usually each PayloadPart event from the event stream will contain a complete JSON object,\n",
    "    this is not guaranteed, and some of the JSON objects may be split across multiple PayloadPart events.\n",
    "    For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "\n",
    "    This class accounts for this by concatenating the bytes written via the 'write' function,\n",
    "    and then exposing a method ('scan_lines') that will return lines (ending with a '\\n' character)\n",
    "    within the buffer. It maintains the position of the last read position to ensure that previous\n",
    "    bytes are not exposed again. It will also save any pending lines that do not end with a '\\n'\n",
    "    to make sure truncations are concatenated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\", chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d852448b-89ee-471b-8e1d-913e54ad8a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# How to study GenAI\n",
      "\n",
      "GenAI is a complex and rapidly evolving field, and studying it requires a multidisciplinary approach. Here are some steps you can take to study GenAI effectively:\n",
      "\n",
      "1. Understand the basics: Start by learning the fundamentals of artificial intelligence, machine learning, and natural language processing. This will provide you with a solid foundation for understanding GenAI.\n",
      "\n",
      "2. Learn programming languages: Familiarize yourself with programming languages such as Python, which is commonly used in AI and machine learning. This will enable you to work with AI tools and frameworks.\n",
      "\n",
      "3. Study AI models and algorithms: Learn about different AI models and algorithms, such as neural networks, deep learning, and reinforcement learning. This will help you understand how AI systems work and how they can be applied to various tasks.\n",
      "\n",
      "4. Explore GenAI applications: Study the different applications of GenAI, such as natural language understanding, computer vision, and robotics. This will help you understand the practical implications of GenAI and how it can be used to solve real-world problems.\n",
      "\n",
      "5. Keep up with research: Stay up-to-date with the latest research in GenAI by following relevant journals, conferences, and online resources. This will help you stay informed about the latest developments in the field.\n",
      "\n",
      "6. Practice with hands-on projects: Apply your knowledge by working on hands-on projects, such as building a chatbot or creating a machine learning model. This will help you gain practical experience and improve your skills.\n",
      "\n",
      "7. Collaborate with others: Join online forums, attend workshops, and participate in hackathons to collaborate with other GenAI enthusiasts. This will help you learn from others and expand your network.\n",
      "\n",
      "8. Develop critical thinking skills: Develop your critical thinking skills to evaluate the effectiveness and ethical implications of GenAI technologies. This will help you become a responsible and informed AI practitioner.\n",
      "\n",
      "By following these steps, you can effectively study GenAI and gain the skills and knowledge needed to succeed in this rapidly evolving field.\n",
      "\n",
      "##### GenAI\n",
      "\n",
      "GenAI is a complex and rapidly evolving field, and studying it requires a multidisciplinary approach. Here are some steps you can take to study GenAI effectively:\n",
      "\n",
      "1. Understand the basics: Start by learning the fundamentals of artificial intelligence, machine learning, and natural language processing. This will provide you with a solid foundation for understanding GenAI.\n",
      "\n",
      "2. Learn programming languages: Familiarize yourself with programming languages such as Python, which is commonly used in AI and machine learning. This will enable you to work with AI tools and frameworks.\n",
      "\n",
      "3. Study AI models and algorithms: Learn about different AI models and algorithms, such as neural networks, deep learning, and reinforcement learning. This will help you understand how AI systems work and how they can be applied to various tasks.\n",
      "\n",
      "4. Explore GenAI applications: Study the different applications of GenAI, such as natural language understanding, computer vision, and robotics. This will help you understand the practical implications of GenAI and how it can be used to solve real-world problems.\n",
      "\n",
      "5. Keep up with research: Stay up-to-date with the latest research in GenAI by following relevant journals, conferences, and online resources. This will help you stay informed about the latest developments in the field.\n",
      "\n",
      "6. Practice with hands-on projects: Apply your knowledge by working on hands-on projects, such as building a chatbot or creating a machine learning model. This will help you gain practical experience and improve your skills.\n",
      "\n",
      "7. Collaborate with others: Join online forums, attend workshops, and participate in hackathons to collaborate with other GenAI enthusiasts. This will help you learn from others and expand your network.\n",
      "\n",
      "8. Develop critical thinking skills: Develop your critical thinking skills to evaluate the effectiveness and ethical implications of GenAI technologies. This will help you become a responsible and informed AI practitioner.\n",
      "\n",
      "By following these steps, you can effectively study GenAI and gain the skills and knowledge needed to succeed in this rapidly evolving field.\n",
      "\n",
      "##### GenAI\n",
      "\n",
      "GenAI is a complex and rapidly evolving field, and studying it requires a multidisciplinary approach. Here are some steps you can take to study GenAI effectively:\n",
      "\n",
      "1. Understand the basics: Start by learning the fundamentals of artificial intelligence, machine learning, and natural language processing. This will provide you with a solid foundation for understanding GenAI.\n",
      "\n",
      "2. Learn programming languages: Familiarize yourself with programming languages such as Python, which is commonly used in AI and machine learning. This will enable you to work with AI tools and frameworks.\n",
      "\n",
      "3. Study AI models and algorithms: Learn about different AI models and algorithms, such as neural networks, deep learning, and reinforcement learning. This will help you understand how AI systems work and how they can be applied to various tasks.\n",
      "\n",
      "4. Explore GenAI applications: Study the different applications of GenAI, such as natural language understanding, computer vision, and robotics. This will help you understand the practical implications of GenAI and how it can be used to solve real-world problems.\n",
      "\n",
      "5. Keep up with research: Stay up-to-date with the latest research in GenAI by following relevant journals, conferences, and online resources. This will help you stay informed about the latest developments in the field.\n",
      "\n",
      "6. Practice with hands-on projects: Apply your knowledge by working on hands-on projects, such as building a chatbot or creating a machine learning model. This will help you gain practical experience and improve your skills.\n",
      "\n",
      "7. Collaborate with others: Join online forums, attend workshops, and participate in hackathons to collaborate with other GenAI enthusiasts. This will help you learn from others and expand your network.\n",
      "\n",
      "8. Develop critical thinking skills: Develop your critical thinking skills to evaluate the effectiveness and ethical implications of GenAI technologies. This will help you become a responsible and informed AI practitioner.\n",
      "\n",
      "By following these steps, you can effectively study GenAI and gain the skills and knowledge needed to succeed in this rapidly evolving field.\n",
      "\n",
      "##### GenAI\n",
      "\n",
      "GenAI is a complex and rapidly evolving field, and studying it requires a multidisciplinary approach. Here are some steps you can take to study GenAI effectively:\n",
      "\n",
      "1. Understand the basics: Start by learning the fundamentals of artificial intelligence, machine learning, and natural language processing. This will provide you with a solid foundation for understanding GenAI.\n",
      "\n",
      "2. Learn programming languages: Familiarize yourself with programming languages such as Python, which is commonly used in AI and machine learning. This will enable you to work with AI tools and frameworks.\n",
      "\n",
      "3. Study AI models and algorithms: Learn about different AI models and algorithms, such as neural networks, deep learning, and reinforcement learning. This will help you understand how AI systems work and how they can be applied to various tasks.\n",
      "\n",
      "4. Explore GenAI applications: Study the different applications of GenAI, such as natural language understanding, computer vision, and robotics. This will help you understand the practical implications of GenAI and how it can be used to solve real-world problems.\n",
      "\n",
      "5. Keep up with research: Stay up-to-date with the latest research in GenAI by following relevant journals, conferences, and online resources. This will help you stay informed about the latest developments in the field.\n",
      "\n",
      "6. Practice with hands-on projects: Apply your knowledge by working on hands-on projects, such as building a chatbot or creating a machine learning model. This will help you gain practical experience and improve your skills.\n",
      "\n",
      "7. Collaborate with others: Join online forums, attend workshops, and participate in hackathons to collaborate with other GenAI enthusiasts. This will help you learn from others and expand your network.\n",
      "\n",
      "8. Develop critical thinking skills: Develop your critical thinking skills to evaluate the effectiveness and ethical implications of GenAI technologies. This will help you become a responsible and informed AI practitioner.\n",
      "\n",
      "By following these steps, you can effectively study GenAI and gain the skills and knowledge needed to succeed in this rapidly evolving field.\n",
      "\n",
      "##### GenAI\n",
      "\n",
      "GenAI is a complex and rapidly evolving field, and studying it requires a multidisciplinary approach. Here are some steps you can take to study GenAI effectively:\n",
      "\n",
      "1. Understand the basics: Start by learning the fundamentals of artificial intelligence, machine learning, and natural language processing. This will provide you with a solid foundation for understanding GenAI.\n",
      "\n",
      "2. Learn programming languages: Familiarize yourself with programming languages such as Python, which is commonly used in AI and machine learning. This will enable you to work with AI tools and frameworks.\n",
      "\n",
      "3. Study AI models and algorithms: Learn about different AI models and algorithms, such as neural networks, deep learning, and reinforcement learning. This will help you understand how AI systems work and how they can be applied to various tasks.\n",
      "\n",
      "4. Explore GenAI applications: Study the different applications of GenAI, such as natural language understanding, computer vision, and robotics. This will help you understand the practical implications of GenAI and how it can be used to solve real-world problems.\n",
      "\n",
      "5. Keep up with research: Stay up-to-date with the latest research in GenAI by following relevant journals, conferences, and online resources. This will help you"
     ]
    }
   ],
   "source": [
    "resp = smr.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=llm.endpoint_name, Body=json.dumps(body), ContentType=\"application/json\"\n",
    ")\n",
    "event_stream = resp[\"Body\"]\n",
    "start_json = b\"{\"\n",
    "for line in LineIterator(event_stream):\n",
    "    if line != b\"\" and start_json in line:\n",
    "        data = json.loads(line[line.find(start_json) :].decode(\"utf-8\"))\n",
    "        if data[\"token\"][\"text\"] != stop_token:\n",
    "            print(data[\"token\"][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219f23a-e48c-49ad-b436-22584f153661",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ba4dc-7f04-4d77-a4eb-252a8dd58461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm.delete_model()\n",
    "# llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575944d-9c34-48f5-987d-87c7e878f242",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|deploy-openchat|OpenChat-streaming_tgi.ipynb)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
