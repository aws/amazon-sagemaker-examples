{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways: on autopilot (hence the name) or with human guidance, without code through SageMaker Studio, or using the AWS SDKs. This notebook, as a first glimpse, will use the AWS SDKs to simply create and deploy a machine learning model.\n",
    "\n",
    "Able to predict Corporate Bankruptcy is very imporant for any Wholesale/Capital Market credit business .Trying to predict Bankruptcy\n",
    "is also important for Credit Risk management.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify SageMaker Version\n",
    "\n",
    "First lets verify the SageMaker version. If it is the latest SDK Version, install the previous one and restart the kernel. This is to make sure we do not have any breaking change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.2.4-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.3\n",
      "    Uninstalling pip-20.2.3:\n",
      "      Successfully uninstalled pip-20.2.3\n",
      "Successfully installed pip-20.2.4\n",
      "Version is good\n",
      "Requirement already up-to-date: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: fsspec>=0.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.17.45,>=1.17.44 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (1.17.44)\n",
      "Requirement already satisfied, skipping upgrade: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (4.7.6)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already up-to-date: arff in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.9)\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "! pip install --upgrade pip \n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    !sys.executable -m pip install \"sagemaker>=1.71.0,<2.0.0\"\n",
    "    print(\"Installing previous SageMaker Version. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"Version is good\")\n",
    "#install s3fs - this package is used by pandas to read file from s3\n",
    "!pip install --upgrade s3fs\n",
    "#install arff package, this package is used to read the bankruptcy data which is in ARFF format\n",
    "!pip install --upgrade arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "First let's import the packages we need.\n",
    "We also need arff package to load the bankrupty data as it is in arff format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime, sleep\n",
    "from sagemaker import get_execution_role\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.automl.automl import AutoML\n",
    "import botocore\n",
    "import time\n",
    "from sagemaker import AutoML\n",
    "!pip install wget\n",
    "import wget\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime, sleep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime, sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve,f1_score, precision_score, recall_score,plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset.\n",
    "\n",
    "We will use from the UCI Machine Learning Repository the Polish companies bankruptcy dataset. It has 64 features and one target attribute. More details are found here: \n",
    "Zieba, M., Tomczak, S. K., & Tomczak, J. M. (2016). Ensemble Boosted Trees with Synthetic Features Generation in Application to Bankruptcy Prediction. Expert Systems with Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "role = get_execution_role()\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\"\n",
    "bankruptcy_file = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "  inflating: 1year.arff              \n",
      "  inflating: 2year.arff              \n",
      "  inflating: 3year.arff              \n",
      "  inflating: 4year.arff              \n",
      "  inflating: 5year.arff              \n"
     ]
    }
   ],
   "source": [
    "!unzip -o data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Let's take a quick look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.39641</td>\n",
       "      <td>2.0472</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.38825</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.33050</td>\n",
       "      <td>1.1389</td>\n",
       "      <td>0.50494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.39718</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.4277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.42002</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.9870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.5548</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.3090</td>\n",
       "      <td>0.30408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.81774</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.2078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.86610</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.14207</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.5497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.22960</td>\n",
       "      <td>1.4063</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.63070</td>\n",
       "      <td>1.1559</td>\n",
       "      <td>0.38677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.48431</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.210</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.8980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Attr1    Attr2    Attr3   Attr4    Attr5    Attr6     Attr7    Attr8  \\\n",
       "0  0.200550  0.37951  0.39641  2.0472  32.3510  0.38825  0.249760  1.33050   \n",
       "1  0.209120  0.49988  0.47225  1.9447  14.7860  0.00000  0.258340  0.99601   \n",
       "2  0.248660  0.69592  0.26713  1.5548  -1.1523  0.00000  0.309060  0.43695   \n",
       "3  0.081483  0.30734  0.45879  2.4928  51.9520  0.14988  0.092704  1.86610   \n",
       "4  0.187320  0.61323  0.22960  1.4063  -7.3128  0.18732  0.187320  0.63070   \n",
       "\n",
       "    Attr9   Attr10  ...    Attr56   Attr57   Attr58    Attr59  Attr60  Attr61  \\\n",
       "0  1.1389  0.50494  ...  0.121960  0.39718  0.87804  0.001924  8.4160  5.1372   \n",
       "1  1.6996  0.49788  ...  0.121300  0.42002  0.85300  0.000000  4.1486  3.2732   \n",
       "2  1.3090  0.30408  ...  0.241140  0.81774  0.76599  0.694840  4.9909  3.9510   \n",
       "3  1.0571  0.57353  ...  0.054015  0.14207  0.94598  0.000000  4.5746  3.6147   \n",
       "4  1.1559  0.38677  ...  0.134850  0.48431  0.86515  0.124440  6.3985  4.3158   \n",
       "\n",
       "    Attr62  Attr63   Attr64  class  \n",
       "0   82.658  4.4158   7.4277      0  \n",
       "1  107.350  3.4000  60.9870      0  \n",
       "2  134.270  2.7185   5.2078      0  \n",
       "3   86.435  4.2228   5.5497      0  \n",
       "4  127.210  2.8692   7.8980      0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankruptcydata1 = arff.loadarff('1year.arff')\n",
    "bankruptcy_df = pd.DataFrame(bankruptcydata1[0])\n",
    "bankruptcy_df['class'] = bankruptcy_df['class'].map(int)\n",
    "bankruptcy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, class 0 = Not Bankrupt(i.e did not file bankruptcy), class 1 = Bankrupt(filed bankruptcy). As we can see, other than Amount, other columns are anonymized in the dataset. All column descriptions are available in the html page, we saved the column names in bankruptcyfeatures.csv file.Our goal is to predict which companies will file for bankruptcy next month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the data, we want to predict bankruptcy filing for next month ###\n",
    "## Mapping Features Name to the Data Frame ##\n",
    "We created Attribute to Feature name mapping in bankruptcyfeatures.csv from the column descriptions in the html page. We will use this file to rename the column names of bankruptcy_df. Please note, we are adding column names just for display - Autopilot does not need column names.  \n",
    "\n",
    "Please note, we mapped target attribute **class** to **bankrupt** to make it more clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.read_csv('bankruptcyfeatures.csv',header=0)\n",
    "\n",
    "name_mapping = {}\n",
    "for index,row in feature_names.iterrows():\n",
    "    name_mapping[row[0]] = row[1]\n",
    "\n",
    "bankruptcy_df.columns= np.array(feature_names['economic_factor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check if the dataset is balanced - whether the number of bankruptcies represents roughly half of the dataset.\n",
    "We also check if the dataset has any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6756\n",
      "1     271\n",
      "Name: bankrupt, dtype: int64\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#our target is to predict bankrupt column\n",
    "target_variable = 'bankrupt'\n",
    "print (bankruptcy_df[target_variable].value_counts())\n",
    "#check for null values\n",
    "print (bankruptcy_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see number of **bankruptcy** records are only around 4% of **not bankrupt**.\n",
    "### Also we can see the dataset features have many NaN values. We will let Autopilot handle these NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we need to split the data into train and test data sets.  The test data will be used to measure the ability of the Autopilot genereated model to generalize to previously unseen data. We will use a 80-20 ratio of training versus testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(bankruptcy_df, test_size=.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (5621, 65)\n",
      "Test dataset size = (1406, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset size = {}\".format(train.shape))\n",
    "print(\"Test dataset size = {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will configure Sagemaker Autopilot. \n",
    "We give a job name **automl-bankruptcy**, create a session with Sagemaker client. We need to have a **s3** bucket to store train/test data and all other artifacts Autopilot will produce. We are using default **s3** bucket, you can create your own bucket. Training and Test data is used from the \n",
    "previous steps and uploaded to **s3** bucket under \"train\" and \"test\" respectively. training_data[target_variable] has the target (bankruptcy 1, Not bankrupt 0). **S3Uri** field in input_data_config tells Autopilot training data location. **TargetAttributeName** indicates target variable for the training job. \n",
    "\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset.\n",
    "\n",
    "The required inputs for invoking an Autopilot job are:\n",
    "\n",
    "    Amazon S3 location for input dataset and for all output artifacts\n",
    "    Name of the column of the dataset you want to predict (y in this case)\n",
    "    An IAM role\n",
    "\n",
    "Currently Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order, is expected to have a header row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-us-east-1-245779447069/sagemaker/automl-bankruptcy/train/train_data.csv\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-245779447069/sagemaker/automl-bankruptcy/test/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "auto_ml_job_name = 'automl-bankruptcy'\n",
    "sm = boto3.client('sagemaker')\n",
    "session = sagemaker.Session()\n",
    "\n",
    "prefix = 'sagemaker/' + auto_ml_job_name\n",
    "bucket = session.default_bucket()\n",
    "training_data = train\n",
    "X_test = test.drop(columns = [target_variable])\n",
    "y_test = test[target_variable]\n",
    "\n",
    "test_data = X_test\n",
    "\n",
    "train_file = 'train_data.csv'\n",
    "training_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'test_data.csv';\n",
    "test_data.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)\n",
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/{}/train'.format(bucket,prefix)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': target_variable\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S3OutputPath': 's3://sagemaker-us-east-1-245779447069/sagemaker/automl-bankruptcy/output'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "output_data_config = {\n",
    "    'S3OutputPath': 's3://{}/{}/output'.format(bucket,prefix)\n",
    "  }\n",
    "output_data_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create the Autopilot job. Please see below for an example on creating an Autopilot job. We set the maximum candidate models (attribute max_candidates) with different parameters to 250. We also set ProblemType='BinaryClassification'. Please note you do not need to set ProblemType and MetricName.If you do not set these 2 field, Autopilot will automatically determine the type of supervised learning problem by analyzing the data(for binary classification problem - default metric is F1).  We set MetricName(parameter job_objective) to AUC or F1 (value of eval_obj when the function is called). You can find out all options for the job configuration here (https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-auto-ml-job.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, here we first define the functions and use them later from a calling function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_automl_object(eval_obj,base_job_name):\n",
    "    \n",
    "    target_attribute_name = target_variable\n",
    "    role = get_execution_role()\n",
    "    automl = AutoML(role=role,\n",
    "                    target_attribute_name=target_attribute_name,\n",
    "                    base_job_name=base_job_name,\n",
    "                    sagemaker_session=session,\n",
    "                    problem_type='BinaryClassification',\n",
    "                    job_objective={'MetricName': eval_obj},\n",
    "                    max_candidates=250\n",
    "                    )\n",
    "    return automl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the AutoML object is created, we call the fit() function to train the AutoML object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_fit(automl, base_job_name):\n",
    "    automl.fit(train_file, job_name=base_job_name, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we create the Autopilot job, we monitor the response of the Autopilot job that we created above. We check the job status every 30 seconds, once the job status returns ‘Completed’ we exit the loop. \n",
    "Before completing the job, loop will print **InProgress**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(automl):\n",
    "    describe_response = automl.describe_auto_ml_job()\n",
    "    print (describe_response)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "\n",
    "    while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "        describe_response = automl.describe_auto_ml_job()\n",
    "        job_run_status = describe_response['AutoMLJobStatus']\n",
    "        print (job_run_status)\n",
    "        sleep(30)\n",
    "    print ('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After the job is complete, we select the best candidate job and check the Accuracy.\n",
    "We select the best candidate and check the accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidate(automl):\n",
    "    best_candidate = automl.describe_auto_ml_job()['BestCandidate']\n",
    "    best_candidate_name = best_candidate['CandidateName']\n",
    "    print(best_candidate)\n",
    "    print('\\n')\n",
    "    print(\"CandidateName: \" + best_candidate_name)\n",
    "    print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "    print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))\n",
    "    return best_candidate_name,best_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a model from the best candidate. In addition to predicted label, we want probability of the prediction - this probability will be used later to plot AUC and Precision/Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(automl,best_candidate_name,best_candidate,timestamp_suffix):\n",
    "    model_name = 'automl-bankruptcy-model-' + timestamp_suffix\n",
    "    inference_response_keys = ['predicted_label', 'probability']\n",
    "    model = automl.create_model(name=best_candidate_name,\n",
    "                                candidate=best_candidate,inference_response_keys=inference_response_keys)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select multiple candidates(example by Objective, in this case AUC).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, we run a Transform job to get inference (i.e Prediction about the default) from the test data set and save to S3. It is worth noting that when you deploy the model as an endpoint or create a Transformer, SageMaker handles the deployment of the feature engineering pipeline and the ML algorithm, so end users can send the data in its raw format for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer(model,best_candidate,eval_obj):\n",
    "    s3_transform_output_path = 's3://{}/{}/inference-results/'.format(bucket, prefix);\n",
    "    output_path = s3_transform_output_path + best_candidate['CandidateName'] +'/'\n",
    "    transformer=model.transformer(instance_count=1, \n",
    "                              instance_type='ml.m5.xlarge',\n",
    "                              assemble_with='Line',\n",
    "                              output_path=output_path)\n",
    "    transformer.transform(data=test_data_s3_path, split_type='Line', content_type='text/csv', wait=False)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we read the inference/predicted data into Pandas Data Frame(**data**)\n",
    "This function will read the file from s3 (generated from create_transformer), create a Data Frame for label(predicted 0/1) and probability(probability of the prediction 0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pred_df(transformer):\n",
    "    print ('***predict output path ***')\n",
    "    print (transformer.output_path, '{}.out'.format(test_file))\n",
    "    pred_csv = get_csv_from_s3(transformer.output_path,'{}.out'.format(test_file))\n",
    "    data=pd.read_csv(io.StringIO(pred_csv), header=None)\n",
    "    data.columns= ['label', 'proba']    \n",
    "    return data\n",
    "\n",
    "def get_csv_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:].strip('/')\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = None \n",
    "    loop = True\n",
    "    while (loop):\n",
    "        try:\n",
    "            obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "            pred_body  = obj.get()[\"Body\"].read().decode('utf-8')    \n",
    "            print ('predict file is avilable s3')    \n",
    "            loop = False\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print('prediction file still not avilable in s3 sleeping for 2 minutes')\n",
    "            time.sleep(120)\n",
    "    return pred_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download Candidate Definition notebook from the following s3 location.\n",
    "We can download data exploration notebook to see details of Autopilot data analysis. This report provides insights about the dataset you provided as input to the AutoML job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_notebooks(automl,eval_obj):\n",
    "    print (\"download CandidateDefinitionNotebookLocation for \" + eval_obj)\n",
    "    print (automl.describe_auto_ml_job()['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation'])\n",
    "    print (\"download DataExplorationNotebookLocation for \" + eval_obj)\n",
    "    print (automl.describe_auto_ml_job()['AutoMLJobArtifacts']['DataExplorationNotebookLocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper function run_automl_process is called with objective AUC and F1. This wrapper function calls multiple functions to creare AutoML object, run training process, create model from best trained job and finally return predicted data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automl_process(eval_obj):\n",
    "    timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "    base_job_name = auto_ml_job_name + '-' + eval_obj + timestamp_suffix\n",
    "    print (base_job_name)\n",
    "    automl = create_automl_object(eval_obj,base_job_name)\n",
    "    automl_fit(automl,base_job_name)\n",
    "    check_status(automl)\n",
    "    best_candidate_name,best_candidate=get_best_candidate(automl)\n",
    "    model = create_model(automl,best_candidate_name,best_candidate,timestamp_suffix)\n",
    "    transformer=create_transformer(model,best_candidate,eval_obj)\n",
    "    pred_df = return_pred_df(transformer)\n",
    "    download_notebooks(automl,eval_obj)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run for the auto pilot job. We call the wrapper function run_automl_process with objective AUC and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********running with eval objective AUC***********\n",
      "automl-bankruptcy-AUC27-18-42-53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AutoMLJobName': 'automl-bankruptcy-AUC27-18-42-53', 'AutoMLJobArn': 'arn:aws:sagemaker:us-east-1:245779447069:automl-job/automl-bankruptcy-auc27-18-42-53', 'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-245779447069/auto-ml-input-data/train_data.csv'}}, 'TargetAttributeName': 'bankrupt'}], 'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-245779447069/'}, 'RoleArn': 'arn:aws:iam::245779447069:role/service-role/AmazonSageMaker-ExecutionRole-20200330T163636', 'AutoMLJobObjective': {'MetricName': 'AUC'}, 'ProblemType': 'BinaryClassification', 'AutoMLJobConfig': {'CompletionCriteria': {'MaxCandidates': 250}, 'SecurityConfig': {'EnableInterContainerTrafficEncryption': False}}, 'CreationTime': datetime.datetime(2020, 10, 27, 18, 42, 54, 425000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2020, 10, 27, 18, 42, 54, 425000, tzinfo=tzlocal()), 'AutoMLJobStatus': 'InProgress', 'AutoMLJobSecondaryStatus': 'Starting', 'GenerateCandidateDefinitionsOnly': False, 'ResponseMetadata': {'RequestId': '662d1618-7526-4e6c-8d80-c2f3a52df618', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '662d1618-7526-4e6c-8d80-c2f3a52df618', 'content-type': 'application/x-amz-json-1.1', 'content-length': '911', 'date': 'Tue, 27 Oct 2020 18:42:55 GMT'}, 'RetryAttempts': 0}}\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "{'CandidateName': 'tuning-job-1-4ca4a464d3054bad9e-120-e95c4016', 'FinalAutoMLJobObjectiveMetric': {'MetricName': 'validation:auc', 'Value': 0.9505800008773804}, 'ObjectiveStatus': 'Succeeded', 'CandidateSteps': [{'CandidateStepType': 'AWS::SageMaker::ProcessingJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:245779447069:processing-job/db-1-1e88d6f5b39e4263a891f5a23127eafaa63f6fd3a4b248baae1c0d9387', 'CandidateStepName': 'db-1-1e88d6f5b39e4263a891f5a23127eafaa63f6fd3a4b248baae1c0d9387'}, {'CandidateStepType': 'AWS::SageMaker::TrainingJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:245779447069:training-job/automl-ban-dpp0-1-a290f585355149e899bd6a784f6744070b48ffce11a54', 'CandidateStepName': 'automl-ban-dpp0-1-a290f585355149e899bd6a784f6744070b48ffce11a54'}, {'CandidateStepType': 'AWS::SageMaker::TransformJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:245779447069:transform-job/automl-ban-dpp0-csv-1-cc3262fd4a6a41b49d775be1d8b2d4b2f5324e08d', 'CandidateStepName': 'automl-ban-dpp0-csv-1-cc3262fd4a6a41b49d775be1d8b2d4b2f5324e08d'}, {'CandidateStepType': 'AWS::SageMaker::TrainingJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:245779447069:training-job/tuning-job-1-4ca4a464d3054bad9e-120-e95c4016', 'CandidateStepName': 'tuning-job-1-4ca4a464d3054bad9e-120-e95c4016'}], 'CandidateStatus': 'Completed', 'InferenceContainers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:0.2-1-cpu-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-245779447069/automl-bankruptcy-AUC27-18-42-53/data-processor-models/automl-ban-dpp0-1-a290f585355149e899bd6a784f6744070b48ffce11a54/output/model.tar.gz', 'Environment': {'AUTOML_TRANSFORM_MODE': 'feature-transform', 'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'application/x-recordio-protobuf', 'SAGEMAKER_PROGRAM': 'sagemaker_serve', 'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'}}, {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-245779447069/automl-bankruptcy-AUC27-18-42-53/tuning/automl-ban-dpp0-xgb/tuning-job-1-4ca4a464d3054bad9e-120-e95c4016/output/model.tar.gz', 'Environment': {'MAX_CONTENT_LENGTH': '20971520', 'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'text/csv', 'SAGEMAKER_INFERENCE_OUTPUT': 'predicted_label', 'SAGEMAKER_INFERENCE_SUPPORTED': 'predicted_label,probability,probabilities'}}, {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:0.2-1-cpu-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-245779447069/automl-bankruptcy-AUC27-18-42-53/data-processor-models/automl-ban-dpp0-1-a290f585355149e899bd6a784f6744070b48ffce11a54/output/model.tar.gz', 'Environment': {'AUTOML_TRANSFORM_MODE': 'inverse-label-transform', 'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'text/csv', 'SAGEMAKER_INFERENCE_INPUT': 'predicted_label', 'SAGEMAKER_INFERENCE_OUTPUT': 'predicted_label', 'SAGEMAKER_INFERENCE_SUPPORTED': 'predicted_label,probability,labels,probabilities', 'SAGEMAKER_PROGRAM': 'sagemaker_serve', 'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'}}], 'CreationTime': datetime.datetime(2020, 10, 27, 19, 27, 27, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2020, 10, 27, 19, 28, 14, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2020, 10, 27, 19, 59, 46, 197000, tzinfo=tzlocal())}\n",
      "\n",
      "\n",
      "CandidateName: tuning-job-1-4ca4a464d3054bad9e-120-e95c4016\n",
      "FinalAutoMLJobObjectiveMetricName: validation:auc\n",
      "FinalAutoMLJobObjectiveMetricValue: 0.9505800008773804\n",
      "***predict output path ***\n",
      "s3://sagemaker-us-east-1-245779447069/sagemaker/automl-bankruptcy/inference-results/tuning-job-1-4ca4a464d3054bad9e-120-e95c4016/ test_data.csv.out\n",
      "prediction file still not avilable in s3 sleeping for 2 minutes\n",
      "prediction file still not avilable in s3 sleeping for 2 minutes\n",
      "prediction file still not avilable in s3 sleeping for 2 minutes\n",
      "predict file is avilable s3\n",
      "download CandidateDefinitionNotebookLocation for AUC\n",
      "s3://sagemaker-us-east-1-245779447069/automl-bankruptcy-AUC27-18-42-53/sagemaker-automl-candidates/pr-1-cd097d688233498295613d5336c717201095f742016749319628ecee71/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb\n",
      "download DataExplorationNotebookLocation for AUC\n",
      "s3://sagemaker-us-east-1-245779447069/automl-bankruptcy-AUC27-18-42-53/sagemaker-automl-candidates/pr-1-cd097d688233498295613d5336c717201095f742016749319628ecee71/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb\n",
      "*********running with eval objective F1***********\n",
      "automl-bankruptcy-F127-20-06-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AutoMLJobName': 'automl-bankruptcy-F127-20-06-23', 'AutoMLJobArn': 'arn:aws:sagemaker:us-east-1:245779447069:automl-job/automl-bankruptcy-f127-20-06-23', 'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-245779447069/auto-ml-input-data/train_data.csv'}}, 'TargetAttributeName': 'bankrupt'}], 'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-245779447069/'}, 'RoleArn': 'arn:aws:iam::245779447069:role/service-role/AmazonSageMaker-ExecutionRole-20200330T163636', 'AutoMLJobObjective': {'MetricName': 'F1'}, 'ProblemType': 'BinaryClassification', 'AutoMLJobConfig': {'CompletionCriteria': {'MaxCandidates': 250}, 'SecurityConfig': {'EnableInterContainerTrafficEncryption': False}}, 'CreationTime': datetime.datetime(2020, 10, 27, 20, 6, 23, 988000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2020, 10, 27, 20, 6, 23, 988000, tzinfo=tzlocal()), 'AutoMLJobStatus': 'InProgress', 'AutoMLJobSecondaryStatus': 'Starting', 'GenerateCandidateDefinitionsOnly': False, 'ResponseMetadata': {'RequestId': 'ce7d805c-0a16-4e6e-9935-716f2027b341', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ce7d805c-0a16-4e6e-9935-716f2027b341', 'content-type': 'application/x-amz-json-1.1', 'content-length': '908', 'date': 'Tue, 27 Oct 2020 20:06:25 GMT'}, 'RetryAttempts': 0}}\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "print ('*********running with eval objective AUC***********')\n",
    "data_auc =run_automl_process('AUC')\n",
    "print ('*********running with eval objective F1***********')\n",
    "data_f1 = run_automl_process('F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, we run a Transform job to get inference (i.e Prediction about the default) from the test data set and save to S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot ROC - the Area under the Curve (AUC) for true positive (in this data set Bankrupt) vs false positive (predicted as Bankrupt but not Bankrupt in the ground truth). The higher the prediction quality of the classification model, the more the AUC curve is skewed to the top left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue','green']\n",
    "model_names = ['Objective : AUC','Objective : F1']\n",
    "models = [data_auc,data_f1]\n",
    "for i in range(0,len(models)):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, models[i]['proba'])\n",
    "    fpr, tpr, _  = metrics.roc_curve(y_test, models[i]['proba'])\n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=str('Auto Pilot {:.2f} '+ model_names[i]).format(auc_score),color=colors[i]) \n",
    "        \n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('ROC Cuve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall curve compares the trade-off between precision and recall, with the best models having a precision-recall curve that is flat initially, dropping steeply as the recall approaches 1. The higher precision + recall, more the curve will be skewed towards upper right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue','green']\n",
    "model_names = ['Objective : AUC','Objective : F1']\n",
    "models = [data_auc,data_f1]\n",
    "\n",
    "print ('model ', 'F1 ', 'precision ', 'recall ')\n",
    "for i in range(0,len(models)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, models[i]['proba'])\n",
    "    print (model_names[i],f1_score(y_test, np.array(models[i]['label'])),precision_score(y_test, models[i]['label']),recall_score(y_test, models[i]['label']) )\n",
    "    plt.plot(recall,precision,color=colors[i],label=model_names[i])\n",
    "        \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion <a name=\"Conclusion\"></a>\n",
    "We can see that with very little data scienece knowledge, we are able to create a moderately accurate prediction for a complex Financial event like Bankruptcy. From the AUC and Precision+Recall plots, we can also see that Autopilot handled highly imbalanced data resonably well. We think the reason for the 62% Recall (rather than achieving higher score) is as follows: the bankruptcy dataset is missing some important features of Bankruptcy filing - short term liqudity, short term funding source etc.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup <a name=\"Cleanup\"></a>\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "# s3_bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can delete the models by calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
