{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision for Medical Imaging: Part 4. SageMaker Pipelines\n",
    "This notebook is the final part of a 4-part series of techniques and services offer by SageMaker to build a model which predicts if an image of cells contains cancer. This notebook describes how to automate the ML workflow using SageMaker Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset for this demo comes from the [Camelyon16 Challenge](https://camelyon16.grand-challenge.org/) made available under the CC0 licencse. The raw data provided by the challenge has been processed into 96x96 pixel tiles by [Bas Veeling](https://github.com/basveeling/pcam) and also made available under the CC0 license. For detailed information on each dataset please see the papers below:\n",
    "* Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199â€“2210. [doi:jama.2017.14585](https://doi.org/10.1001/jama.2017.14585)\n",
    "* B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. \"Rotation Equivariant CNNs for Digital Pathology\". [arXiv:1806.03962](http://arxiv.org/abs/1806.03962)\n",
    "\n",
    "The tiled dataset from Bas Veeling is over 6GB of data. In order to easily run this demo, the dataset has been pruned to the first 14,000 images of the tiled dataset and comes included in the repo with this notebook for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Sagemaker SDK and Boto3\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE</b> You may get an error from pip's dependency resolver; you can ignore this error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Boto3 Clients and Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"  # Change region as needed\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\"image-classification\", region)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_layers\": 18,\n",
    "    \"use_pretrained_model\": 1,\n",
    "    \"augmentation_type\": \"crop_color_transform\",\n",
    "    \"image_shape\": \"3,96,96\",\n",
    "    \"num_classes\": 2,\n",
    "    \"num_training_samples\": num_training_samples,\n",
    "    \"mini_batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"precision_dtype\": \"float32\",\n",
    "}\n",
    "\n",
    "estimator_config = {\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"image_uri\": training_image,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"instance_count\": 1,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"volume_size\": 100,\n",
    "    \"max_run\": 360000,\n",
    "    \"output_path\": f\"s3://{bucket}/{prefix}/training_jobs\",\n",
    "}\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(**estimator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create RecordIO Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_uri = f\"s3://{bucket}/{prefix}/data\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=\"data/camelyon16_tiles.h5\", desired_s3_uri=base_uri\n",
    ")\n",
    "\n",
    "input_data = ParameterString(name=\"InputData\", default_value=input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename=\"split_data.py\", Bucket=bucket, Key=f\"{prefix}/code/split_data.py\")\n",
    "split_data_script_uri = f\"s3://{bucket}/{prefix}/code/split_data.py\"\n",
    "split_data_instance_type = \"ml.t3.large\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=split_data_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"image-classication-split-data\",\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "split_data_step = ProcessingStep(\n",
    "    name=\"SplitData\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=input_data, destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\", source=\"/opt/ml/processing/output/data/train\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"val_data\", source=\"/opt/ml/processing/output/data/val\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\", source=\"/opt/ml/processing/output/data/test\"\n",
    "        ),\n",
    "    ],\n",
    "    code=split_data_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_inputs = {\n",
    "    \"train\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=split_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"train_data\"\n",
    "        ].S3Output.S3Uri,\n",
    "        content_type=\"application/x-recordio\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        input_mode=\"Pipe\",\n",
    "    ),\n",
    "    \"validation\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=split_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"val_data\"\n",
    "        ].S3Output.S3Uri,\n",
    "        content_type=\"application/x-recordio\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        input_mode=\"Pipe\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "train_step = TrainingStep(name=\"TrainModel\", estimator=image_classifier, inputs=train_step_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=image_classifier,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"image/jpeg\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name=f\"{mpg_name}-pipline\",\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"ModelPreDeployment\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"deploy_model.py\", Bucket=bucket, Key=f\"{prefix}/code/deploy_model.py\"\n",
    ")\n",
    "deploy_model_script_uri = f\"s3://{bucket}/{prefix}/code/deploy_model.py\"\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{prefix}-deploy-model\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name=\"DeployModel\",\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\",\n",
    "        create_model_step.properties.ModelName,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-instance-type\",\n",
    "        deploy_instance_type,\n",
    "        \"--endpoint-name\",\n",
    "        \"cv-model-pipeline\",\n",
    "    ],\n",
    "    code=deploy_model_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"{prefix}-pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data, model_approval_status],\n",
    "    steps=[split_data_step, train_step, register_step, create_model_step, deploy_step],\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"ModelApprovalStatus\": \"Approved\"}\n",
    "\n",
    "start_response = pipeline.start(parameters=parameters)\n",
    "start_response.wait(max_attempts=100)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage\n",
    "\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker_session)\n",
    "for execution_step in reversed(start_response.list_steps()):\n",
    "    pprint(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.sagemaker_session.delete_endpoint(mpg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
