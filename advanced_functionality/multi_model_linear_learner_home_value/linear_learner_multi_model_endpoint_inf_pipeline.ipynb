{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "traditional-wages",
   "metadata": {
    "papermill": {
     "duration": 0.031495,
     "end_time": "2021-05-26T15:50:29.843348",
     "exception": false,
     "start_time": "2021-05-26T15:50:29.811853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using Linear Learner\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "Amazon SageMaker inference pipeline model consists of a sequence of containers that serve inference requests by combining preprocessing, predictions and post-processing data science tasks.  An inference pipeline allows you to apply the same preprocessing code used during model training, to process the inference request data used for predictions.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used with inference pipeline, this notebook provides an example using a set of Linear Learner models that each predict housing prices for a single location. This domain is used as a simple example to easily experiment with multi-model endpoints.  \n",
    "\n",
    "This notebook showcases three MME capabilities: \n",
    "* Native MME support with Amazon SageMaker Linear Learner algorithm.  Because of the native support there is no need for you to create a custom container.  \n",
    "* Native MME support with Amazon SageMaker Inference Pipelines.\n",
    "* Granular InvokeModel access to multiple models hosted on the MME using IAM condition key.\n",
    "\n",
    "To demonstrate these capabilities, the notebook discusses the use case of predicting house prices in multiple cities using linear regression.  House prices are predicted based on features like number of bedrooms, number of garages, square footage etc.  Depending on the city, the features affect the house price differently.  For example, small changes in the square footage cause a drastic change in house prices in New York when compared to price changes in Houston.  For accurate house price predictions, we will train multiple linear regression models, a unique location specific model per city.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-premium",
   "metadata": {
    "papermill": {
     "duration": 0.030685,
     "end_time": "2021-05-26T15:50:29.904617",
     "exception": false,
     "start_time": "2021-05-26T15:50:29.873932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Contents\n",
    "\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Preprocess the raw housing data using Scikit Learn model](#Preprocess-synthetic-housing-data-using-scikit-learn)\n",
    "1. [Train multiple house value prediction models for multiple cities](#Train-multiple-house-value-prediction-models)\n",
    "1. [Create model entity with multi model support](#Create-sagemaker-multi-model-support)\n",
    "1. [Create an inference pipeline with sklearn model and MME linear learner model](#Create-inference-pipeline)\n",
    "1. [Exercise the inference pipeline - Get predictions from the different  linear learner models](#Exercise-inference-pipeline)\n",
    "1. [Update Multi Model Endpoint with new models](#update-models)\n",
    "1. [Explore granular access to the target models of MME](#Finegrain-control-invoke-models)\n",
    "1. [Endpoint CloudWatch Metrics Analysis](#CW-metric-analysis)\n",
    "1. [Clean up](#CleanUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-trance",
   "metadata": {
    "papermill": {
     "duration": 0.031801,
     "end_time": "2021-05-26T15:50:29.967168",
     "exception": false,
     "start_time": "2021-05-26T15:50:29.935367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Section 1 - Generate synthetic data for housing models <a id='Generate-synthetic-data-for-housing-models'></a>\n",
    "\n",
    "In this section, you will generate synthetic data that will be used to train the linear learner models.  The data generated consists of 6 numerical features - the year the house was built in, house size in square feet, number of bedrooms, number of bathroom, the lot size and number of garages and two categorial features - deck and front_porch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "available-earth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:30.034512Z",
     "iopub.status.busy": "2021-05-26T15:50:30.033735Z",
     "iopub.status.idle": "2021-05-26T15:50:30.670958Z",
     "shell.execute_reply": "2021-05-26T15:50:30.671341Z"
    },
    "papermill": {
     "duration": 0.672748,
     "end_time": "2021-05-26T15:50:30.671480",
     "exception": false,
     "start_time": "2021-05-26T15:50:29.998732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from random import choice\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker.multidatamodel import MULTI_MODEL_CONTAINER_MODE\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coated-worst",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:30.737312Z",
     "iopub.status.busy": "2021-05-26T15:50:30.736726Z",
     "iopub.status.idle": "2021-05-26T15:50:30.738470Z",
     "shell.execute_reply": "2021-05-26T15:50:30.738819Z"
    },
    "papermill": {
     "duration": 0.036469,
     "end_time": "2021-05-26T15:50:30.738945",
     "exception": false,
     "start_time": "2021-05-26T15:50:30.702476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "directed-joseph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:30.807237Z",
     "iopub.status.busy": "2021-05-26T15:50:30.806720Z",
     "iopub.status.idle": "2021-05-26T15:50:30.808436Z",
     "shell.execute_reply": "2021-05-26T15:50:30.808780Z"
    },
    "papermill": {
     "duration": 0.038925,
     "end_time": "2021-05-26T15:50:30.808909",
     "exception": false,
     "start_time": "2021-05-26T15:50:30.769984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    \"\"\"Generate price based on features of the house\"\"\"\n",
    "    \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        garage = 1\n",
    "    else:\n",
    "        garage = 0\n",
    "        \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        front_porch = 1\n",
    "    else:\n",
    "        front_porch = 0\n",
    "        \n",
    "    price = int(150 * house['SQUARE_FEET'] + \\\n",
    "                10000 * house['NUM_BEDROOMS'] + \\\n",
    "                15000 * house['NUM_BATHROOMS'] + \\\n",
    "                15000 * house['LOT_ACRES'] + \\\n",
    "                10000 * garage + \\\n",
    "                10000 * front_porch + \\\n",
    "                15000 * house['GARAGE_SPACES'] - \\\n",
    "                5000 * (MAX_YEAR - house['YEAR_BUILT']))\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reflected-bundle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:30.876784Z",
     "iopub.status.busy": "2021-05-26T15:50:30.876212Z",
     "iopub.status.idle": "2021-05-26T15:50:30.878414Z",
     "shell.execute_reply": "2021-05-26T15:50:30.878001Z"
    },
    "papermill": {
     "duration": 0.038457,
     "end_time": "2021-05-26T15:50:30.878520",
     "exception": false,
     "start_time": "2021-05-26T15:50:30.840063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_yes_no():\n",
    "    \"\"\"Generate values (y/n) for categorical features\"\"\"\n",
    "    answer = choice(['y', 'n'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "orange-minnesota",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:30.947369Z",
     "iopub.status.busy": "2021-05-26T15:50:30.946819Z",
     "iopub.status.idle": "2021-05-26T15:50:30.948831Z",
     "shell.execute_reply": "2021-05-26T15:50:30.949152Z"
    },
    "papermill": {
     "duration": 0.039614,
     "end_time": "2021-05-26T15:50:30.949283",
     "exception": false,
     "start_time": "2021-05-26T15:50:30.909669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    \"\"\"Generate a row of data (single house information)\"\"\"\n",
    "    house = {'SQUARE_FEET':    np.random.normal(3000, 750),\n",
    "             'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "             'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "             'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "             'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "             'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10))),\n",
    "             'FRONT_PORCH':   gen_yes_no(),\n",
    "             'DECK':          gen_yes_no()\n",
    "            }\n",
    "    \n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [house['YEAR_BUILT'],   \n",
    "            house['SQUARE_FEET'], \n",
    "            house['NUM_BEDROOMS'], \n",
    "            house['NUM_BATHROOMS'], \n",
    "            house['LOT_ACRES'],    \n",
    "            house['GARAGE_SPACES'],\n",
    "            house['FRONT_PORCH'],    \n",
    "            house['DECK'], \n",
    "            price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "another-judges",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:31.018298Z",
     "iopub.status.busy": "2021-05-26T15:50:31.017625Z",
     "iopub.status.idle": "2021-05-26T15:50:31.019485Z",
     "shell.execute_reply": "2021-05-26T15:50:31.019859Z"
    },
    "papermill": {
     "duration": 0.038635,
     "end_time": "2021-05-26T15:50:31.019991",
     "exception": false,
     "start_time": "2021-05-26T15:50:30.981356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    \"\"\"Generate housing dataset\"\"\"\n",
    "    house_list = []\n",
    "    \n",
    "    for _ in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        house_list, \n",
    "        columns=[\n",
    "            'YEAR_BUILT',    \n",
    "            'SQUARE_FEET',  \n",
    "            'NUM_BEDROOMS',            \n",
    "            'NUM_BATHROOMS',\n",
    "            'LOT_ACRES',\n",
    "            'GARAGE_SPACES',\n",
    "            'FRONT_PORCH',\n",
    "            'DECK', \n",
    "            'PRICE']\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amber-windsor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:31.087814Z",
     "iopub.status.busy": "2021-05-26T15:50:31.087312Z",
     "iopub.status.idle": "2021-05-26T15:50:31.089160Z",
     "shell.execute_reply": "2021-05-26T15:50:31.089508Z"
    },
    "papermill": {
     "duration": 0.03816,
     "end_time": "2021-05-26T15:50:31.089633",
     "exception": false,
     "start_time": "2021-05-26T15:50:31.051473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, test): \n",
    "    \"\"\"Save the housing data locally\"\"\"\n",
    "    os.makedirs('data/{0}/train'.format(location), exist_ok=True)\n",
    "    train.to_csv('data/{0}/train/train.csv'.format(location), sep=',', header=False, index=False)\n",
    "       \n",
    "    os.makedirs('data/{0}/test'.format(location), exist_ok=True)\n",
    "    test.to_csv('data/{0}/test/test.csv'.format(location), sep=',', header=False, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surgical-overview",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:31.187082Z",
     "iopub.status.busy": "2021-05-26T15:50:31.176939Z",
     "iopub.status.idle": "2021-05-26T15:50:31.443145Z",
     "shell.execute_reply": "2021-05-26T15:50:31.442632Z"
    },
    "papermill": {
     "duration": 0.322139,
     "end_time": "2021-05-26T15:50:31.443254",
     "exception": false,
     "start_time": "2021-05-26T15:50:31.121115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate housing data for multiple locations.\n",
    "#Change \"PARALLEL_TRAINING_JOBS \" to a lower number to limit the number of training jobs and models. Or to a higher value to experiment with more models.\n",
    "\n",
    "PARALLEL_TRAINING_JOBS = 4\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    \n",
    "    #Spliting data into train and test in 90:10 ratio\n",
    "    #Not splitting the train data into train and val because its not preprocessed yet\n",
    "    train, test = train_test_split(houses, test_size=0.1)\n",
    "    save_data_locally(loc, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stylish-flavor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:31.518140Z",
     "iopub.status.busy": "2021-05-26T15:50:31.517679Z",
     "iopub.status.idle": "2021-05-26T15:50:31.524271Z",
     "shell.execute_reply": "2021-05-26T15:50:31.524645Z"
    },
    "papermill": {
     "duration": 0.049818,
     "end_time": "2021-05-26T15:50:31.524781",
     "exception": false,
     "start_time": "2021-05-26T15:50:31.474963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>3236.316691</td>\n",
       "      <td>6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.13</td>\n",
       "      <td>3</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>624897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>2167.509634</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>275976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1599.113490</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>302867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>2772.308254</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>523046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005</td>\n",
       "      <td>3099.231612</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>510234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0        2014  3236.316691             6            1.5       1.13   \n",
       "1        1991  2167.509634             4            2.0       1.39   \n",
       "2        2016  1599.113490             3            1.0       1.20   \n",
       "3        2016  2772.308254             2            1.5       0.98   \n",
       "4        2005  3099.231612             6            2.5       1.19   \n",
       "\n",
       "   GARAGE_SPACES FRONT_PORCH DECK   PRICE  \n",
       "0              3           y    n  624897  \n",
       "1              0           n    y  275976  \n",
       "2              1           n    n  302867  \n",
       "3              3           y    n  523046  \n",
       "4              0           n    n  510234  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows the first few lines of data.\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-ceramic",
   "metadata": {
    "papermill": {
     "duration": 0.032432,
     "end_time": "2021-05-26T15:50:31.589708",
     "exception": false,
     "start_time": "2021-05-26T15:50:31.557276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 2 - Preprocess the raw housing data using Scikit Learn <a id='Preprocess-synthetic-housing-data-using-scikit-learn'></a>\n",
    "\n",
    "In this section, the categorical features of the data (deck and porch) are pre-processed using sklearn to convert them to one hot encoding representation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alive-statistics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:31.667935Z",
     "iopub.status.busy": "2021-05-26T15:50:31.667158Z",
     "iopub.status.idle": "2021-05-26T15:50:32.766354Z",
     "shell.execute_reply": "2021-05-26T15:50:32.765784Z"
    },
    "papermill": {
     "duration": 1.144441,
     "end_time": "2021-05-26T15:50:32.766474",
     "exception": false,
     "start_time": "2021-05-26T15:50:31.622033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET :  sagemaker-us-west-2-688520471316\n",
      "ROLE :  arn:aws:iam::688520471316:role/hongshan-sagemaker-experiment\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "BUCKET  = sagemaker_session.default_bucket()\n",
    "print(\"BUCKET : \", BUCKET)\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"ROLE : \", role)\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION = boto3.Session().region_name\n",
    "\n",
    "DATA_PREFIX = 'DEMO_MME_LINEAR_LEARNER'\n",
    "HOUSING_MODEL_NAME = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "signal-better",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:32.836767Z",
     "iopub.status.busy": "2021-05-26T15:50:32.835981Z",
     "iopub.status.idle": "2021-05-26T15:50:32.846955Z",
     "shell.execute_reply": "2021-05-26T15:50:32.847308Z"
    },
    "papermill": {
     "duration": 0.048026,
     "end_time": "2021-05-26T15:50:32.847439",
     "exception": false,
     "start_time": "2021-05-26T15:50:32.799413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create the SKLearn estimator with the sklearn_preprocessor.py as the script\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'sklearn_preprocessor.py'\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    framework_version=\"0.20.0\",\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lesbian-tumor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:32.923537Z",
     "iopub.status.busy": "2021-05-26T15:50:32.919886Z",
     "iopub.status.idle": "2021-05-26T15:50:33.266437Z",
     "shell.execute_reply": "2021-05-26T15:50:33.266811Z"
    },
    "papermill": {
     "duration": 0.386462,
     "end_time": "2021-05-26T15:50:33.266950",
     "exception": false,
     "start_time": "2021-05-26T15:50:32.880488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data uploaded to :  s3://sagemaker-us-west-2-688520471316/housing-data/NewYork_NY/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-west-2-688520471316/housing-data/LosAngeles_CA/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-west-2-688520471316/housing-data/Chicago_IL/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-west-2-688520471316/housing-data/Houston_TX/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "#Upload the raw training data to S3 bucket, to be accessed by SKLearn\n",
    "train_inputs = []\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "\n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path='data/{}/train/train.csv'.format(loc),\n",
    "        bucket=BUCKET,\n",
    "        key_prefix='housing-data/{}/train'.format(loc)\n",
    "    )\n",
    "    \n",
    "    train_inputs.append(train_input)\n",
    "    print(\"Raw training data uploaded to : \", train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loved-refund",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:33.339344Z",
     "iopub.status.busy": "2021-05-26T15:50:33.338608Z",
     "iopub.status.idle": "2021-05-26T15:50:38.538599Z",
     "shell.execute_reply": "2021-05-26T15:50:38.538957Z"
    },
    "papermill": {
     "duration": 5.238874,
     "end_time": "2021-05-26T15:50:38.539104",
     "exception": false,
     "start_time": "2021-05-26T15:50:33.300230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing fit input data at  0  for loc  NewYork_NY\n",
      "preprocessing fit input data at  1  for loc  LosAngeles_CA\n",
      "preprocessing fit input data at  2  for loc  Chicago_IL\n",
      "preprocessing fit input data at  3  for loc  Houston_TX\n"
     ]
    }
   ],
   "source": [
    "##Launch multiple scikit learn training to process the raw synthetic data generated for multiple locations.\n",
    "##Before executing this, take the training instance limits in your account and cost into consideration.\n",
    "\n",
    "sklearn_preprocessors = []\n",
    "sklearn_preprocessors_preprocessor_jobs = []\n",
    "\n",
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    "    print(\"preprocessing fit input data at \", index , \" for loc \", loc)\n",
    "     \n",
    "    job_name='scikit-learn-preprocessor-{}'.format(strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "    \n",
    "    sklearn_preprocessor.fit({'train': train_inputs[index]}, job_name=job_name, wait=False)\n",
    "\n",
    "    sklearn_preprocessors.append(sklearn_preprocessor)\n",
    "    sklearn_preprocessors_preprocessor_jobs.append(job_name)\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continental-sitting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:38.611678Z",
     "iopub.status.busy": "2021-05-26T15:50:38.611051Z",
     "iopub.status.idle": "2021-05-26T15:50:38.612814Z",
     "shell.execute_reply": "2021-05-26T15:50:38.613161Z"
    },
    "papermill": {
     "duration": 0.040016,
     "end_time": "2021-05-26T15:50:38.613285",
     "exception": false,
     "start_time": "2021-05-26T15:50:38.573269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(job_name):\n",
    "    \"\"\" Wait for the training job to complete \"\"\"\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    \n",
    "    waiter = sm_client.get_waiter('training_job_completed_or_stopped')\n",
    "    waiter.wait(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ordinary-bennett",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:38.685877Z",
     "iopub.status.busy": "2021-05-26T15:50:38.685227Z",
     "iopub.status.idle": "2021-05-26T15:50:38.687498Z",
     "shell.execute_reply": "2021-05-26T15:50:38.687052Z"
    },
    "papermill": {
     "duration": 0.04018,
     "end_time": "2021-05-26T15:50:38.687602",
     "exception": false,
     "start_time": "2021-05-26T15:50:38.647422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_for_batch_transform_job_to_complete(job_name):\n",
    "    \"\"\"Wait for the batch transform job to complete\"\"\"\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    \n",
    "    waiter = sm_client.get_waiter('transform_job_completed_or_stopped')\n",
    "    waiter.wait(TransformJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acting-break",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:50:38.759799Z",
     "iopub.status.busy": "2021-05-26T15:50:38.759347Z",
     "iopub.status.idle": "2021-05-26T15:54:40.640266Z",
     "shell.execute_reply": "2021-05-26T15:54:40.640630Z"
    },
    "papermill": {
     "duration": 241.918637,
     "end_time": "2021-05-26T15:54:40.640772",
     "exception": false,
     "start_time": "2021-05-26T15:50:38.722135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job scikit-learn-preprocessor-2021-05-28-20-30-42 to complete...\n",
      "Waiting for job scikit-learn-preprocessor-2021-05-28-20-30-43 to complete...\n",
      "Waiting for job scikit-learn-preprocessor-2021-05-28-20-30-45 to complete...\n",
      "Waiting for job scikit-learn-preprocessor-2021-05-28-20-30-46 to complete...\n"
     ]
    }
   ],
   "source": [
    "#Wait for the preprocessor jobs to finish\n",
    "for job_name in sklearn_preprocessors_preprocessor_jobs:\n",
    "    wait_for_training_job_to_complete(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "anticipated-faith",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:54:40.717213Z",
     "iopub.status.busy": "2021-05-26T15:54:40.716742Z",
     "iopub.status.idle": "2021-05-26T15:54:44.819651Z",
     "shell.execute_reply": "2021-05-26T15:54:44.820058Z"
    },
    "papermill": {
     "duration": 4.144566,
     "end_time": "2021-05-26T15:54:44.820199",
     "exception": false,
     "start_time": "2021-05-26T15:54:40.675633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform the raw data at  0  for loc  NewYork_NY\n",
      "Transform the raw data at  1  for loc  LosAngeles_CA\n",
      "Transform the raw data at  2  for loc  Chicago_IL\n",
      "Transform the raw data at  3  for loc  Houston_TX\n"
     ]
    }
   ],
   "source": [
    "##Once the preprocessor is fit, use tranformer to preprocess the raw training data and store the transformed data right back into s3.\n",
    "##Before executing this, take the training instance limits in your account and cost into consideration.\n",
    "\n",
    "preprocessor_transformers = []\n",
    "\n",
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    "    print(\"Transform the raw data at \", index , \" for loc \", loc)\n",
    "       \n",
    "    sklearn_preprocessor = sklearn_preprocessors[index]\n",
    "    \n",
    "    transformer = sklearn_preprocessor.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        assemble_with='Line',\n",
    "        accept='text/csv'\n",
    "    )\n",
    "    \n",
    "    preprocessor_transformers.append(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dynamic-basics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T15:54:44.898231Z",
     "iopub.status.busy": "2021-05-26T15:54:44.897724Z",
     "iopub.status.idle": "2021-05-26T16:18:05.471741Z",
     "shell.execute_reply": "2021-05-26T16:18:05.471240Z"
    },
    "papermill": {
     "duration": 1400.615639,
     "end_time": "2021-05-26T16:18:05.471852",
     "exception": false,
     "start_time": "2021-05-26T15:54:44.856213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=7511 sha256=e00ab46063caf72697d42e64febe16e009c8ee342dfd25b706671b7d1b24e3c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j3axvnb1/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [44] [INFO] Booting worker with pid: 44\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:40:19 +0000] [45] [INFO] Booting worker with pid: 45\u001b[0m\n",
      "\u001b[34m2021-05-28 20:40:22,256 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:40:22 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:40:22 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2021-05-28 20:40:22,974 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:40:22 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:40:22 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2021-05-28 20:40:22,974 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          2019  3976.570641             4  ...            y     n  710335\u001b[0m\n",
      "\u001b[34m1          2006  3197.546695             6  ...            n     n  521732\u001b[0m\n",
      "\u001b[34m2          2011  4132.936014             6  ...            n     n  685390\u001b[0m\n",
      "\u001b[34m3          1998  1424.702040             3  ...            y     n  256655\u001b[0m\n",
      "\u001b[34m4          1990  3783.375487             3  ...            n     y  505006\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1991  1896.077082             4  ...            y     n  256761\u001b[0m\n",
      "\u001b[34m896        1995  2597.348652             3  ...            n     y  344452\u001b[0m\n",
      "\u001b[34m897        1993  3540.359952             3  ...            n     y  491953\u001b[0m\n",
      "\u001b[34m898        1995  2968.028890             4  ...            n     n  449504\u001b[0m\n",
      "\u001b[34m899        1993  3470.114570             3  ...            y     n  476367\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[35mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[35m0          2019  3976.570641             4  ...            y     n  710335\u001b[0m\n",
      "\u001b[35m1          2006  3197.546695             6  ...            n     n  521732\u001b[0m\n",
      "\u001b[35m2          2011  4132.936014             6  ...            n     n  685390\u001b[0m\n",
      "\u001b[35m3          1998  1424.702040             3  ...            y     n  256655\u001b[0m\n",
      "\u001b[35m4          1990  3783.375487             3  ...            n     y  505006\u001b[0m\n",
      "\u001b[35m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[35m895        1991  1896.077082             4  ...            y     n  256761\u001b[0m\n",
      "\u001b[35m896        1995  2597.348652             3  ...            n     y  344452\u001b[0m\n",
      "\u001b[35m897        1993  3540.359952             3  ...            n     y  491953\u001b[0m\n",
      "\u001b[35m898        1995  2968.028890             4  ...            n     n  449504\u001b[0m\n",
      "\u001b[35m899        1993  3470.114570             3  ...            y     n  476367\n",
      "\u001b[0m\n",
      "\u001b[35m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[34mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 2.46428102  1.23056626 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379  0.23263744  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.66009195  1.43087013  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.14933344  0.67178085 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.05171382 -0.06137462 -0.01504275 ...  0.          1.\n",
      "   0.        ]\n",
      " [-0.14933344  0.58179659 -0.72759385 ...  1.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[34mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 2.46428102  1.23056626 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379  0.23263744  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.66009195  1.43087013  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.14933344  0.67178085 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.05171382 -0.06137462 -0.01504275 ...  0.          1.\n",
      "   0.        ]\n",
      " [-0.14933344  0.58179659 -0.72759385 ...  1.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:40:23 +0000] \"POST /invocations HTTP/1.1\" 200 128528 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 2.46428102  1.23056626 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379  0.23263744  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.66009195  1.43087013  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.14933344  0.67178085 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.05171382 -0.06137462 -0.01504275 ...  0.          1.\n",
      "   0.        ]\n",
      " [-0.14933344  0.58179659 -0.72759385 ...  1.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[35mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 2.46428102  1.23056626 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379  0.23263744  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.66009195  1.43087013  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.14933344  0.67178085 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.05171382 -0.06137462 -0.01504275 ...  0.          1.\n",
      "   0.        ]\n",
      " [-0.14933344  0.58179659 -0.72759385 ...  1.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:40:23 +0000] \"POST /invocations HTTP/1.1\" 200 128528 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-05-28T20:40:22.923:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Launching batch transform job: sagemaker-scikit-learn-2021-05-28-20-34-55-699\n",
      ".............................\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=7511 sha256=907904319376e986aaaa7d50353c7b2af7cc8bb63e8fe1c4aea26600c84e930f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bd0bm4xo/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:27 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:27 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:27 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:27 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:27 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:28 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:45:28 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\n",
      "\u001b[34m2021-05-28 20:45:31,211 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m2021-05-28 20:45:31,211 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:45:31 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2021-05-28 20:45:31,830 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:45:31 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2021-05-28 20:45:31,830 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:45:32 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          1995  2938.115745             6  ...            n     y  481967\u001b[0m\n",
      "\u001b[34m1          1993  3619.165275             4  ...            y     n  547274\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:45:32 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[35m0          1995  2938.115745             6  ...            n     y  481967\u001b[0m\n",
      "\u001b[35m1          1993  3619.165275             4  ...            y     n  547274\u001b[0m\n",
      "\u001b[34m2          2009  2671.094047             6  ...            n     y  454764\u001b[0m\n",
      "\u001b[34m3          1986  2431.395225             3  ...            y     y  336859\u001b[0m\n",
      "\u001b[34m4          1987  4341.746533             3  ...            n     n  591761\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        2011  2909.566312             5  ...            y     n  513834\u001b[0m\n",
      "\u001b[34m896        1990  3042.635718             4  ...            y     y  448795\u001b[0m\n",
      "\u001b[34m897        1991  4445.053069             5  ...            n     n  636757\u001b[0m\n",
      "\u001b[34m898        2006  2968.620794             4  ...            n     y  491093\u001b[0m\n",
      "\u001b[34m899        2004  2199.560561             4  ...            n     n  363934\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[34mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 0.05171382 -0.09969332  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.14933344  0.77273038 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.45904469 -0.44174783  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.35038071  1.83069174  0.69750836 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379 -0.06061639 -0.01504275 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.95642652 -1.0457817  -0.01504275 ...  0.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[34mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 0.05171382 -0.09969332  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.14933344  0.77273038 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.45904469 -0.44174783  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.35038071  1.83069174  0.69750836 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379 -0.06061639 -0.01504275 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.95642652 -1.0457817  -0.01504275 ...  0.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:45:32 +0000] \"POST /invocations HTTP/1.1\" 200 128579 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2          2009  2671.094047             6  ...            n     y  454764\u001b[0m\n",
      "\u001b[35m3          1986  2431.395225             3  ...            y     y  336859\u001b[0m\n",
      "\u001b[35m4          1987  4341.746533             3  ...            n     n  591761\u001b[0m\n",
      "\u001b[35m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[35m895        2011  2909.566312             5  ...            y     n  513834\u001b[0m\n",
      "\u001b[35m896        1990  3042.635718             4  ...            y     y  448795\u001b[0m\n",
      "\u001b[35m897        1991  4445.053069             5  ...            n     n  636757\u001b[0m\n",
      "\u001b[35m898        2006  2968.620794             4  ...            n     y  491093\u001b[0m\n",
      "\u001b[35m899        2004  2199.560561             4  ...            n     n  363934\n",
      "\u001b[0m\n",
      "\u001b[35m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[35mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 0.05171382 -0.09969332  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.14933344  0.77273038 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.45904469 -0.44174783  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.35038071  1.83069174  0.69750836 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379 -0.06061639 -0.01504275 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.95642652 -1.0457817  -0.01504275 ...  0.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[35mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 0.05171382 -0.09969332  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.14933344  0.77273038 -0.01504275 ...  1.          1.\n",
      "   0.        ]\n",
      " [ 1.45904469 -0.44174783  1.41005947 ...  0.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.35038071  1.83069174  0.69750836 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.15747379 -0.06061639 -0.01504275 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.95642652 -1.0457817  -0.01504275 ...  0.          1.\n",
      "   0.        ]]\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:45:32 +0000] \"POST /invocations HTTP/1.1\" 200 128579 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-05-28T20:45:32.434:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Launching batch transform job: sagemaker-scikit-learn-2021-05-28-20-40-39-564\n",
      "................................\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=7511 sha256=75e1830fd8e1cbf422c89ac004606a87b2b9ae698a41e774c3e3b8edc2205a8b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uz2v4mc8/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=7511 sha256=75e1830fd8e1cbf422c89ac004606a87b2b9ae698a41e774c3e3b8edc2205a8b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uz2v4mc8/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[35mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [37] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37)\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [37] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [37] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37)\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [37] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:51:00 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:51:00 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m2021-05-28 20:51:03,551 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:51:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2021-05-28 20:51:04,140 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-05-28 20:51:03,551 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:51:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2021-05-28 20:51:04,140 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:51:04 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          2013  4229.118294             4  ...            y     y  735017\u001b[0m\n",
      "\u001b[34m1          1980  2604.804519             6  ...            n     n  312120\u001b[0m\n",
      "\u001b[34m2          1983  2412.771437             3  ...            n     n  275365\u001b[0m\n",
      "\u001b[34m3          1994  4323.470125             5  ...            y     y  678870\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:51:04 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[35m0          2013  4229.118294             4  ...            y     y  735017\u001b[0m\n",
      "\u001b[35m1          1980  2604.804519             6  ...            n     n  312120\u001b[0m\n",
      "\u001b[35m2          1983  2412.771437             3  ...            n     n  275365\u001b[0m\n",
      "\u001b[35m3          1994  4323.470125             5  ...            y     y  678870\u001b[0m\n",
      "\u001b[34m4          2001  2967.067358             4  ...            n     y  491810\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1990  3747.967091             4  ...            n     y  516295\u001b[0m\n",
      "\u001b[34m896        1988  3512.069561             6  ...            y     y  497860\u001b[0m\n",
      "\u001b[34m897        1975  2734.350454             3  ...            y     y  304202\u001b[0m\n",
      "\u001b[34m898        1992  2932.446384             3  ...            n     y  419316\u001b[0m\n",
      "\u001b[34m899        1990  2707.747931             2  ...            n     y  340862\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[34mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 1.86113922  1.55407953 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [-1.45614067 -0.52666464  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [-1.15456977 -0.77265881 -0.72759385 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.95875884 -0.36071644 -0.72759385 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.24985707 -0.10695577 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.45090434 -0.39479424 -1.44014496 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m4          2001  2967.067358             4  ...            n     y  491810\u001b[0m\n",
      "\u001b[35m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[35m895        1990  3747.967091             4  ...            n     y  516295\u001b[0m\n",
      "\u001b[35m896        1988  3512.069561             6  ...            y     y  497860\u001b[0m\n",
      "\u001b[35m897        1975  2734.350454             3  ...            y     y  304202\u001b[0m\n",
      "\u001b[35m898        1992  2932.446384             3  ...            n     y  419316\u001b[0m\n",
      "\u001b[35m899        1990  2707.747931             2  ...            n     y  340862\n",
      "\u001b[0m\n",
      "\u001b[35m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[35mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 1.86113922  1.55407953 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [-1.45614067 -0.52666464  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [-1.15456977 -0.77265881 -0.72759385 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.95875884 -0.36071644 -0.72759385 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.24985707 -0.10695577 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.45090434 -0.39479424 -1.44014496 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[35mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 1.86113922  1.55407953 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [-1.45614067 -0.52666464  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [-1.15456977 -0.77265881 -0.72759385 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.95875884 -0.36071644 -0.72759385 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.24985707 -0.10695577 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.45090434 -0.39479424 -1.44014496 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:51:04 +0000] \"POST /invocations HTTP/1.1\" 200 128523 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[[ 1.86113922  1.55407953 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [-1.45614067 -0.52666464  1.41005947 ...  0.          1.\n",
      "   0.        ]\n",
      " [-1.15456977 -0.77265881 -0.72759385 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.95875884 -0.36071644 -0.72759385 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.24985707 -0.10695577 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.45090434 -0.39479424 -1.44014496 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:51:04 +0000] \"POST /invocations HTTP/1.1\" 200 128523 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-05-28T20:51:04.673:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "Launching batch transform job: sagemaker-scikit-learn-2021-05-28-20-45-52-957\n",
      "..............................\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=7511 sha256=e533553dafa003b768b863969809bd9e4f77dc4f4a4b6c940df9762adcfa6a39\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4lbhzf5o/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=7511 sha256=e533553dafa003b768b863969809bd9e4f77dc4f4a4b6c940df9762adcfa6a39\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4lbhzf5o/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[35mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2021-05-28 20:56:26 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2021-05-28 20:56:26 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m2021-05-28 20:56:29,363 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:56:29 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:56:29 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2021-05-28 20:56:30,031 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-05-28 20:56:29,363 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:56:29 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:56:29 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2021-05-28 20:56:30,031 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          2001  2913.055252             4  ...            y     y  454058\u001b[0m\n",
      "\u001b[34m1          2001  3467.315839             3  ...            n     y  542897\u001b[0m\n",
      "\u001b[34m2          2005  4225.632178             6  ...            y     y  740744\u001b[0m\n",
      "\u001b[34m3          1989  3727.599581             3  ...            y     n  539839\u001b[0m\n",
      "\u001b[34m4          1980  5850.532546             5  ...            y     y  832079\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1981  3321.156107             5  ...            y     n  436973\u001b[0m\n",
      "\u001b[34m896        2002  3669.119899             3  ...            y     n  538767\u001b[0m\n",
      "\u001b[34m897        1992  2991.057982             5  ...            y     n  460008\u001b[0m\n",
      "\u001b[34m898        1974  2396.160427             3  ...            y     n  233174\u001b[0m\n",
      "\u001b[34m899        1984  2222.635278             4  ...            y     y  311845\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[34mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 0.65485562 -0.13179579 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.65485562  0.57821142 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.05695016  1.54961382  1.41005947 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.24985707 -0.03187438  0.69750836 ...  1.          1.\n",
      "   0.        ]\n",
      " [-2.05928247 -0.79393749 -0.72759385 ...  1.          1.\n",
      "   0.        ]\n",
      " [-1.05404614 -1.01622302 -0.01504275 ...  1.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[35m0          2001  2913.055252             4  ...            y     y  454058\u001b[0m\n",
      "\u001b[35m1          2001  3467.315839             3  ...            n     y  542897\u001b[0m\n",
      "\u001b[35m2          2005  4225.632178             6  ...            y     y  740744\u001b[0m\n",
      "\u001b[35m3          1989  3727.599581             3  ...            y     n  539839\u001b[0m\n",
      "\u001b[35m4          1980  5850.532546             5  ...            y     y  832079\u001b[0m\n",
      "\u001b[35m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[35m895        1981  3321.156107             5  ...            y     n  436973\u001b[0m\n",
      "\u001b[35m896        2002  3669.119899             3  ...            y     n  538767\u001b[0m\n",
      "\u001b[35m897        1992  2991.057982             5  ...            y     n  460008\u001b[0m\n",
      "\u001b[35m898        1974  2396.160427             3  ...            y     n  233174\u001b[0m\n",
      "\u001b[35m899        1984  2222.635278             4  ...            y     y  311845\n",
      "\u001b[0m\n",
      "\u001b[35m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros_like(X, dtype=np.int)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[35mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones_like(X, dtype=np.bool)\u001b[0m\n",
      "\u001b[35mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 0.65485562 -0.13179579 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.65485562  0.57821142 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.05695016  1.54961382  1.41005947 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.24985707 -0.03187438  0.69750836 ...  1.          1.\n",
      "   0.        ]\n",
      " [-2.05928247 -0.79393749 -0.72759385 ...  1.          1.\n",
      "   0.        ]\n",
      " [-1.05404614 -1.01622302 -0.01504275 ...  1.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 0.65485562 -0.13179579 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.65485562  0.57821142 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.05695016  1.54961382  1.41005947 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.24985707 -0.03187438  0.69750836 ...  1.          1.\n",
      "   0.        ]\n",
      " [-2.05928247 -0.79393749 -0.72759385 ...  1.          1.\n",
      "   0.        ]\n",
      " [-1.05404614 -1.01622302 -0.01504275 ...  1.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [28/May/2021:20:56:30 +0000] \"POST /invocations HTTP/1.1\" 200 128497 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 0.65485562 -0.13179579 -0.01504275 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.65485562  0.57821142 -0.72759385 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.05695016  1.54961382  1.41005947 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.24985707 -0.03187438  0.69750836 ...  1.          1.\n",
      "   0.        ]\n",
      " [-2.05928247 -0.79393749 -0.72759385 ...  1.          1.\n",
      "   0.        ]\n",
      " [-1.05404614 -1.01622302 -0.01504275 ...  1.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [28/May/2021:20:56:30 +0000] \"POST /invocations HTTP/1.1\" 200 128497 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-05-28T20:56:29.999:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Launching batch transform job: sagemaker-scikit-learn-2021-05-28-20-51-36-648\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training input\n",
    "preprocessed_train_data_path = []\n",
    "\n",
    "for index, transformer in enumerate(preprocessor_transformers):\n",
    "    transformer.transform(train_inputs[index], content_type='text/csv')\n",
    "    print('Launching batch transform job: {}'.format(transformer.latest_transform_job.job_name))\n",
    "    preprocessed_train_data_path.append(transformer.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "robust-painting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:05.604984Z",
     "iopub.status.busy": "2021-05-26T16:18:05.604489Z",
     "iopub.status.idle": "2021-05-26T16:18:05.721527Z",
     "shell.execute_reply": "2021-05-26T16:18:05.721107Z"
    },
    "papermill": {
     "duration": 0.185911,
     "end_time": "2021-05-26T16:18:05.721640",
     "exception": false,
     "start_time": "2021-05-26T16:18:05.535729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job sagemaker-scikit-learn-2021-05-28-20-34-55-699 to complete...\n",
      "Waiting for job sagemaker-scikit-learn-2021-05-28-20-40-39-564 to complete...\n",
      "Waiting for job sagemaker-scikit-learn-2021-05-28-20-45-52-957 to complete...\n",
      "Waiting for job sagemaker-scikit-learn-2021-05-28-20-51-36-648 to complete...\n"
     ]
    }
   ],
   "source": [
    "#Wait for all the batch transform jobs to finish\n",
    "for transformer in preprocessor_transformers: \n",
    "    job_name=transformer.latest_transform_job.job_name\n",
    "    wait_for_batch_transform_job_to_complete(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "prescription-aberdeen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:05.860006Z",
     "iopub.status.busy": "2021-05-26T16:18:05.859239Z",
     "iopub.status.idle": "2021-05-26T16:18:07.110955Z",
     "shell.execute_reply": "2021-05-26T16:18:07.111326Z"
    },
    "papermill": {
     "duration": 1.326047,
     "end_time": "2021-05-26T16:18:07.111466",
     "exception": false,
     "start_time": "2021-05-26T16:18:05.785419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_output_key: sagemaker-scikit-learn-2021-05-28-20-34-55-699/train.csv.out\n",
      "Download directory: preprocessed-data/NewYork_NY/\n",
      "transformer_output_key: sagemaker-scikit-learn-2021-05-28-20-40-39-564/train.csv.out\n",
      "Download directory: preprocessed-data/LosAngeles_CA/\n",
      "transformer_output_key: sagemaker-scikit-learn-2021-05-28-20-45-52-957/train.csv.out\n",
      "Download directory: preprocessed-data/Chicago_IL/\n",
      "transformer_output_key: sagemaker-scikit-learn-2021-05-28-20-51-36-648/train.csv.out\n",
      "Download directory: preprocessed-data/Houston_TX/\n"
     ]
    }
   ],
   "source": [
    "##Download the preprocessed data, split into train and val, upload back to S3 in the same directory as tranformer output path\n",
    "for index, transformer in enumerate(preprocessor_transformers): \n",
    "    transformer_output_key='{}/{}'.format(transformer.latest_transform_job.job_name, 'train.csv.out') \n",
    "    \n",
    "    preprocessed_data_download_dir = '{}/'.format(\"preprocessed-data/\"+LOCATIONS[index])\n",
    "    \n",
    "    sagemaker_session.download_data(\n",
    "        path=preprocessed_data_download_dir, \n",
    "        bucket=BUCKET,\n",
    "        key_prefix=transformer_output_key\n",
    "    )\n",
    "    \n",
    "    print('transformer_output_key: {}'.format(transformer_output_key ))\n",
    "    print('Download directory: {}'.format(preprocessed_data_download_dir ))\n",
    "    \n",
    "    train_df = pd.read_csv('{}/{}'.format(preprocessed_data_download_dir,\"train.csv.out\"))\n",
    "    \n",
    "    #Spliting data into train and test in 70:30 ratio\n",
    "    train, val = train_test_split(train_df, test_size=0.3)\n",
    "    \n",
    "    train.to_csv('{}{}'.format(preprocessed_data_download_dir,\"train.csv\"), sep=',', header=False, index=False)\n",
    "    val.to_csv('{}{}'.format(preprocessed_data_download_dir,\"val.csv\"), sep=',', header=False, index=False)\n",
    "    \n",
    "    \n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path='{}/{}'.format(preprocessed_data_download_dir, 'train.csv'), \n",
    "        bucket=BUCKET,\n",
    "        key_prefix='{}'.format(transformer.latest_transform_job.job_name, 'train.csv'))\n",
    "    \n",
    "    val_input = sagemaker_session.upload_data(\n",
    "        path='{}/{}'.format(preprocessed_data_download_dir, 'val.csv'), \n",
    "        bucket=BUCKET,\n",
    "        key_prefix='{}'.format(transformer.latest_transform_job.job_name, 'val.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "injured-visibility",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:07.244211Z",
     "iopub.status.busy": "2021-05-26T16:18:07.243717Z",
     "iopub.status.idle": "2021-05-26T16:18:07.246005Z",
     "shell.execute_reply": "2021-05-26T16:18:07.246385Z"
    },
    "papermill": {
     "duration": 0.070476,
     "end_time": "2021-05-26T16:18:07.246514",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.176038",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-34-55-699\n",
      "s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-40-39-564\n",
      "s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-45-52-957\n",
      "s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-51-36-648\n"
     ]
    }
   ],
   "source": [
    "##S3 location of the preprocessed data\n",
    "for preprocessed_train_data in preprocessed_train_data_path: \n",
    "    print(preprocessed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "municipal-judges",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:07.380523Z",
     "iopub.status.busy": "2021-05-26T16:18:07.380049Z",
     "iopub.status.idle": "2021-05-26T16:18:07.381884Z",
     "shell.execute_reply": "2021-05-26T16:18:07.382306Z"
    },
    "papermill": {
     "duration": 0.070751,
     "end_time": "2021-05-26T16:18:07.382446",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.311695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    "    preprocessed_data_download_dir = '{}/'.format(\"preprocessed-data/\"+LOCATIONS[index])\n",
    "    path='{}/{}'.format(preprocessed_data_download_dir, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-married",
   "metadata": {
    "papermill": {
     "duration": 0.064853,
     "end_time": "2021-05-26T16:18:07.512422",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.447569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Section 3 : Train house value prediction models for multiple cities <a id='Train-multiple-house-value-prediction-models'></a>\n",
    "\n",
    "In this section, you will use the preprocessed housing data to train multiple linear learner models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "liked-hepatitis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:07.653488Z",
     "iopub.status.busy": "2021-05-26T16:18:07.653015Z",
     "iopub.status.idle": "2021-05-26T16:18:07.662214Z",
     "shell.execute_reply": "2021-05-26T16:18:07.662575Z"
    },
    "papermill": {
     "duration": 0.08553,
     "end_time": "2021-05-26T16:18:07.662708",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.577178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-shell",
   "metadata": {
    "papermill": {
     "duration": 0.064888,
     "end_time": "2021-05-26T16:18:07.792596",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.727708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Launch a single training job for a given housing location\n",
    "There is nothing specific to multi-model endpoints in terms of the models it will host. They are trained in the same way as all other SageMaker models. Here we are using the Linear Learner estimator and not waiting for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "subtle-touch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:07.929420Z",
     "iopub.status.busy": "2021-05-26T16:18:07.928960Z",
     "iopub.status.idle": "2021-05-26T16:18:07.931224Z",
     "shell.execute_reply": "2021-05-26T16:18:07.930811Z"
    },
    "papermill": {
     "duration": 0.074051,
     "end_time": "2021-05-26T16:18:07.931332",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.857281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def launch_training_job(location, transformer):\n",
    "    \"\"\"Launch a linear learner traing job\"\"\"\n",
    "    \n",
    "    train_inputs = '{}/{}'.format(transformer.output_path, \"train.csv\")\n",
    "    val_inputs = '{}/{}'.format(transformer.output_path, \"val.csv\")\n",
    "    \n",
    "    print(\"train_inputs:\", train_inputs)\n",
    "    print(\"val_inputs:\", val_inputs)\n",
    "     \n",
    "    full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, location)\n",
    "    s3_output_path = 's3://{}/{}'.format(BUCKET, full_output_prefix)\n",
    "    \n",
    "    print(\"s3_output_path \", s3_output_path)\n",
    "    \n",
    "    s3_output_path = 's3://{}/{}/model_artifacts/{}'.format(BUCKET, DATA_PREFIX, location)\n",
    "    \n",
    "    linear_estimator = sagemaker.estimator.Estimator(\n",
    "                            container,\n",
    "                            role, \n",
    "                            instance_count=1,\n",
    "                            instance_type='ml.c4.xlarge',\n",
    "                            output_path=s3_output_path,\n",
    "                            sagemaker_session=sagemaker_session)\n",
    "    \n",
    "    linear_estimator.set_hyperparameters(\n",
    "                           feature_dim=10,\n",
    "                           mini_batch_size=100,\n",
    "                           predictor_type='regressor',\n",
    "                           epochs=10,\n",
    "                           num_models=32,\n",
    "                           loss='absolute_loss')\n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    train_input = sagemaker.inputs.TrainingInput(s3_data=train_inputs,\n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='text/csv;label_size=1')\n",
    "    val_input   = sagemaker.inputs.TrainingInput(s3_data=val_inputs,\n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='text/csv;label_size=1')\n",
    "    \n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "     \n",
    "    linear_estimator.fit(remote_inputs, wait=False)\n",
    "   \n",
    "    return linear_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-morning",
   "metadata": {
    "papermill": {
     "duration": 0.065261,
     "end_time": "2021-05-26T16:18:08.061659",
     "exception": false,
     "start_time": "2021-05-26T16:18:07.996398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "trained-portal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T16:18:08.197845Z",
     "iopub.status.busy": "2021-05-26T16:18:08.197048Z",
     "iopub.status.idle": "2021-05-26T16:18:14.576242Z",
     "shell.execute_reply": "2021-05-26T16:18:14.575562Z"
    },
    "papermill": {
     "duration": 6.44915,
     "end_time": "2021-05-26T16:18:14.576441",
     "exception": true,
     "start_time": "2021-05-26T16:18:08.127291",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-34-55-699/train.csv\n",
      "val_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-34-55-699/val.csv\n",
      "s3_output_path  s3://sagemaker-us-west-2-688520471316/DEMO_MME_LINEAR_LEARNER/model_artifacts/NewYork_NY\n",
      "train_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-40-39-564/train.csv\n",
      "val_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-40-39-564/val.csv\n",
      "s3_output_path  s3://sagemaker-us-west-2-688520471316/DEMO_MME_LINEAR_LEARNER/model_artifacts/LosAngeles_CA\n",
      "train_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-45-52-957/train.csv\n",
      "val_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-45-52-957/val.csv\n",
      "s3_output_path  s3://sagemaker-us-west-2-688520471316/DEMO_MME_LINEAR_LEARNER/model_artifacts/Chicago_IL\n",
      "train_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-51-36-648/train.csv\n",
      "val_inputs: s3://sagemaker-us-west-2-688520471316/sagemaker-scikit-learn-2021-05-28-20-51-36-648/val.csv\n",
      "s3_output_path  s3://sagemaker-us-west-2-688520471316/DEMO_MME_LINEAR_LEARNER/model_artifacts/Houston_TX\n",
      "4 training jobs launched: ['linear-learner-2021-05-28-20-56-51-295', 'linear-learner-2021-05-28-20-56-51-472', 'linear-learner-2021-05-28-20-56-53-549', 'linear-learner-2021-05-28-20-56-58-082']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_jobs = []\n",
    "    \n",
    "for transformer, loc in zip(preprocessor_transformers, LOCATIONS[:PARALLEL_TRAINING_JOBS]): \n",
    "    job = launch_training_job(loc, transformer)\n",
    "    training_jobs.append(job)\n",
    "    \n",
    "print('{} training jobs launched: {}'.format(len(training_jobs), training_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-space",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Wait for all  training jobs to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lined-potato",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job linear-learner-2021-05-28-20-56-51-295 to complete...\n",
      "Waiting for job linear-learner-2021-05-28-20-56-51-472 to complete...\n",
      "Waiting for job linear-learner-2021-05-28-20-56-53-549 to complete...\n",
      "Waiting for job linear-learner-2021-05-28-20-56-58-082 to complete...\n"
     ]
    }
   ],
   "source": [
    "#Wait for the jobs to finish\n",
    "for job_name in training_jobs:\n",
    "    wait_for_training_job_to_complete(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-return",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Section 4 - Create Sagemaker model with multi model support <a id='Create-sagemaker-multi-model-support'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "marked-columbus",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    model_name_plus = s3_key[s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    model_name = re.findall('^(.*?)/', model_name_plus)[0]\n",
    "    return s3_key, model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "solar-decline",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_mme(job_name):\n",
    "    print(\"job_name :\", job_name)\n",
    "    response = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    source_s3_key, model_name = parse_model_artifacts(response['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    copy_source = {'Bucket': BUCKET, 'Key': source_s3_key}\n",
    "    key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name, model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(model_name, source_s3_key, key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=copy_source, Key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pediatric-rotation",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old model artifacts from DEMO_MME_LINEAR_LEARNER/multi_model_artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'A6XRAVBHG9P64MJ7',\n",
       "   'HostId': '5OWpvxHM5CKMheg3LTgXaVoiuOifGclHIWObej6Pb3fGE0vV+c8Jl2zA5XEVENCRx4khNj5fB88=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': '5OWpvxHM5CKMheg3LTgXaVoiuOifGclHIWObej6Pb3fGE0vV+c8Jl2zA5XEVENCRx4khNj5fB88=',\n",
       "    'x-amz-request-id': 'A6XRAVBHG9P64MJ7',\n",
       "    'date': 'Fri, 28 May 2021 21:03:00 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/174_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/41_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/38_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Chicago_IL/Chicago_IL.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/NewYork_NY/NewYork_NY.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/127_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/26_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/199_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/185_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/8_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/66_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/70_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/152_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/46_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/57_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/186_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/101_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/149_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/80_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/5_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/119_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/151_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/82_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/109_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/42_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/49_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/133_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/58_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/131_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/67_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/121_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/188_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/153_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/6_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/11_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/23_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/103_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/20_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/86_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/59_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/2_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/134_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/144_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/35_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/116_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/142_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/181_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/3_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/196_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/148_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/78_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/190_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/51_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/132_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/61_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/102_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/162_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/79_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/178_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/84_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/30_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/54_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/1_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/10_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/96_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/124_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/184_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/191_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/LosAngeles_CA/LosAngeles_CA.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/128_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/122_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/52_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/135_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/95_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/110_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/120_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/192_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/90_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/13_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/160_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/50_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/60_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/29_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/167_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/180_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/126_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/139_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/81_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/195_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/27_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/164_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/198_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/65_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/105_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/113_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/44_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/107_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/68_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/175_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/197_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/104_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/123_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/136_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/87_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/73_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/108_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/154_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/173_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/16_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/71_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/155_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/19_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/161_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/189_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/31_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/99_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/47_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/93_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/118_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/98_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/88_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/75_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/22_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/7_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/4_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/55_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/56_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/145_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/150_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/156_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/163_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/138_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/183_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/63_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/32_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/159_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/17_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/158_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/76_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/146_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/187_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/89_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/97_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/12_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/92_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/91_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/182_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/143_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/141_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/165_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/166_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/18_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/74_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/85_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/0_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/111_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/170_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/37_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/140_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/179_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/77_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/33_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/125_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/137_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/39_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/36_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/9_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/168_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/15_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/157_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/25_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/100_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/114_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/24_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/115_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/193_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/172_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/129_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/171_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/194_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/53_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/62_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/147_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/14_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/112_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/64_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/43_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/72_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/28_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/169_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/130_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/177_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/40_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/34_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/45_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/176_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/21_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/94_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/69_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/83_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/117_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/106_Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/48_Houston_TX.tar.gz'}]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, clear out old versions of the model artifacts from previous runs of this notebook\n",
    "s3_bucket = s3.Bucket(BUCKET)\n",
    "full_input_prefix = '{}/multi_model_artifacts'.format(DATA_PREFIX)\n",
    "print('Removing old model artifacts from {}'.format(full_input_prefix))\n",
    "s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "played-contractor",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name : linear-learner-2021-05-28-20-56-51-295\n",
      "Copying NewYork_NY model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/NewYork_NY/linear-learner-2021-05-28-20-56-51-295/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/NewYork_NY/NewYork_NY.tar.gz...\n",
      "job_name : linear-learner-2021-05-28-20-56-51-472\n",
      "Copying LosAngeles_CA model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/LosAngeles_CA/linear-learner-2021-05-28-20-56-51-472/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/LosAngeles_CA/LosAngeles_CA.tar.gz...\n",
      "job_name : linear-learner-2021-05-28-20-56-53-549\n",
      "Copying Chicago_IL model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/Chicago_IL/linear-learner-2021-05-28-20-56-53-549/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Chicago_IL/Chicago_IL.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "## Deploy all but the last model trained to MME\n",
    "## We will use the last model to show how to update an existing MME in Section 7\n",
    "for job_name in training_jobs[:-1]:\n",
    "    deploy_artifacts_to_mme(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "thick-virus",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "\n",
    "_model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "\n",
    "ll_multi_model = MultiDataModel(\n",
    "        name=MODEL_NAME,\n",
    "        model_data_prefix=_model_url,\n",
    "        image_uri=container,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-bankruptcy",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Section 5 : Create an inference pipeline with sklearn model and MME linear learner model <a id='Create-inference-pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-motor",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Set up the inference pipeline using the Pipeline Model API.  This sets up a list of models in a single endpoint; In this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "prerequisite-citation",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inference_model = sklearn_preprocessor.create_model()\n",
    "\n",
    "model_name = '{}-{}'.format('inference-pipeline', timestamp_prefix)\n",
    "endpoint_name = '{}-{}'.format('inference-pipeline-ep', timestamp_prefix)\n",
    "\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, \n",
    "    role=role, \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    models=[\n",
    "        scikit_learn_inference_model, \n",
    "        ll_multi_model])\n",
    "\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-gothic",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Section 6 :  Exercise the inference pipeline - Get predictions from  different  linear learner models. <a id='Exercise-inference-pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exempt-climate",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create Predictor\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "csv_serializer = sagemaker.serializers.CSVSerializer()\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "naughty-placement",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_one_house_value(features, model_name, predictor_to_use):\n",
    "    print('Using model {} to predict price of this house: {}'.format(model_name,\n",
    "                                                                     features))\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    start_time = time.time()\n",
    "     \n",
    "    response = predictor_to_use.predict(features, target_model=model_name)\n",
    "    \n",
    "    response_json = json.loads(response)\n",
    "        \n",
    "    predicted_value = response_json['predictions'][0]['score']    \n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "absent-helping",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1995, 3733.4944153545366, 2, 1.0, 1.37, 0, 'n', 'n']\n",
      "$501,692.75, took 2,214 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [2000, 3093.065104045882, 5, 1.5, 0.79, 3, 'n', 'y']\n",
      "$499,528.06, took 1,761 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1994, 2389.7623356967097, 2, 2.5, 1.02, 0, 'n', 'n']\n",
      "$312,219.31, took 49 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1988, 2649.787223089696, 5, 2.5, 1.12, 0, 'y', 'n']\n",
      "$373,254.16, took 45 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1991, 2513.9827621539066, 3, 1.0, 1.22, 1, 'n', 'y']\n",
      "$323,959.56, took 45 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [2017, 2717.655916259325, 4, 2.5, 1.09, 0, 'y', 'y']\n",
      "$517,623.44, took 40 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1990, 2825.4795167248544, 5, 3.0, 0.63, 3, 'y', 'y']\n",
      "$446,150.50, took 45 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1998, 3652.547470693956, 4, 3.0, 0.83, 1, 'y', 'n']\n",
      "$573,714.50, took 38 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1991, 3181.3082356549266, 5, 2.0, 0.96, 1, 'y', 'n']\n",
      "$468,579.03, took 44 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [1990, 3057.16182407358, 3, 1.0, 0.78, 1, 'n', 'n']\n",
      "$378,565.91, took 43 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    model_name = LOCATIONS[np.random.randint(1, PARALLEL_TRAINING_JOBS - 1)]\n",
    "    full_model_name = '{}/{}.tar.gz'.format(model_name,model_name)\n",
    "    predict_one_house_value(gen_random_house()[:-1], full_model_name, predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-blend",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Section 7 - Add new model to the endpoint, simply by copying the model artifact to the S3 location\n",
    "<a id='update-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "consistent-malaysia",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name : linear-learner-2021-05-28-20-56-58-082\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/Houston_TX/linear-learner-2021-05-28-20-56-58-082/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "## Copy the last model\n",
    "last_training_job=training_jobs[PARALLEL_TRAINING_JOBS-1]\n",
    "deploy_artifacts_to_mme(last_training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sought-procedure",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/Houston_TX.tar.gz to predict price of this house: [1995, 5310.646499516446, 3, 2.5, 0.93, 1, 'y', 'n']\n",
      "$793,874.81, took 1,657 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = LOCATIONS[PARALLEL_TRAINING_JOBS-1]\n",
    "full_model_name = '{}/{}.tar.gz'.format(model_name,model_name)\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name, predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-eclipse",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Section 8 - Endpoint CloudWatch Metrics Analysis <a id='CW-metric-analysis'></a>\n",
    "\n",
    "With MME, the models are dynamically loaded into the containers memory of the instance hosting the endpoint when invoked.  Therefore, the model invocation may take longer when it is invoked for the first time. And after the model is already in the containers memory, the subsequent invocations will be faster. If an instance memory utilization is high and a new model needs to be loaded then unused models are unloaded.  The unloaded models will remain in the instances storage volume and can be loaded into containers memory later without being downloaded from the S3 bucket again.  If the instances storage volume if full, unused models are deleted from storage volume.    \n",
    "Managing the loading/unloading of the models is completely handled by Amazon SageMaker behind the scenes without you having to take any specific actions.  However, it is important to understand this behavior because it has implications on the model invocation latency.\n",
    "\n",
    "Amazon SageMaker provides CloudWatch metrics for multi-model endpoints so you can determine the endpoint usage and the cache hit rate and optimize your endpoint.  To analyze the endpoint and the container behavior, you will invoke multiple models in this order :\n",
    "\n",
    "    a. Create 200 copies of the original model and save with different names.\n",
    "    b. Starting with no models loaded into the container, Invoke the first 100 models\n",
    "    c. Invoke the same 100 models again\n",
    "    d. Invoke all 200 models\n",
    "\n",
    "We use this order of invocations to observe the behavior of the CloudWatch metrics - LoadedModelCount, MemoryUtilization and ModelCacheHit.  You are encouraged to experiment with loading varying number of models to use the CloudWatch charts to help make ongoing decisions on the optimal choice of instance type, instance count, and number of models that a given endpoint should host.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lovely-dinner",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a copy of the model artifacts in S3 bucket with new names so we have multiple models to understand the latency behavior.\n",
    "def copy_additional_artifacts_to_mme(num_copies):\n",
    "    \n",
    "    source_s3_model_key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name, model_name)\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': source_s3_model_key}\n",
    "    for i in range(num_copies):\n",
    "        new_model_name=\"{}_{}\".format(i, model_name)\n",
    "        dest_s3_model_key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name, new_model_name)\n",
    "        print('Copying {} model\\n   from: {}\\n     to: {}...'.format(model_name, source_s3_model_key, dest_s3_model_key))\n",
    "        s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=dest_s3_model_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "collect-profit",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/0_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/1_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/2_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/3_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/4_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/5_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/6_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/7_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/8_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/9_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/10_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/11_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/12_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/13_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/14_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/15_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/16_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/17_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/18_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/19_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/20_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/21_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/22_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/23_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/24_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/25_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/26_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/27_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/28_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/29_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/30_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/31_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/32_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/33_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/34_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/35_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/36_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/37_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/38_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/39_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/40_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/41_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/42_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/43_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/44_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/45_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/46_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/47_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/48_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/49_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/50_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/51_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/52_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/53_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/54_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/55_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/56_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/57_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/58_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/59_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/60_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/61_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/62_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/63_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/64_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/65_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/66_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/67_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/68_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/69_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/70_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/71_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/72_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/73_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/74_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/75_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/76_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/77_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/78_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/79_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/80_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/81_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/82_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/83_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/84_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/85_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/86_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/87_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/88_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/89_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/90_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/91_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/92_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/93_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/94_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/95_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/96_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/97_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/98_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/99_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/100_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/101_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/102_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/103_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/104_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/105_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/106_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/107_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/108_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/109_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/110_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/111_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/112_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/113_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/114_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/115_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/116_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/117_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/118_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/119_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/120_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/121_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/122_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/123_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/124_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/125_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/126_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/127_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/128_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/129_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/130_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/131_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/132_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/133_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/134_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/135_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/136_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/137_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/138_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/139_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/140_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/141_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/142_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/143_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/144_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/145_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/146_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/147_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/148_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/149_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/150_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/151_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/152_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/153_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/154_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/155_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/156_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/157_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/158_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/159_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/160_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/161_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/162_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/163_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/164_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/165_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/166_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/167_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/168_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/169_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/170_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/171_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/172_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/173_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/174_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/175_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/176_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/177_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/178_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/179_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/180_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/181_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/182_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/183_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/184_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/185_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/186_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/187_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/188_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/189_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/190_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/191_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/192_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/193_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/194_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/195_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/196_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/197_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/198_Houston_TX.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/199_Houston_TX.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "##Create 200 copies of the original model and save with different names.\n",
    "copy_additional_artifacts_to_mme(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "forced-modification",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Invoke multiple models in a loop\n",
    "def invoke_multiple_models_mme(model_range_low, model_range_high):\n",
    "    for i in range(model_range_low, model_range_high):\n",
    "        new_model_name=\"{}_{}\".format(i, model_name)\n",
    "        full_model_name = '{}/{}.tar.gz'.format(model_name, new_model_name)\n",
    "        predict_one_house_value(gen_random_house()[:-1], full_model_name, predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "conditional-conversion",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/0_Houston_TX.tar.gz to predict price of this house: [1981, 4000.6605137886877, 5, 2.5, 0.42, 3, 'y', 'n']\n",
      "$564,194.00, took 1,686 ms\n",
      "\n",
      "Using model Houston_TX/1_Houston_TX.tar.gz to predict price of this house: [2009, 2807.5788173200076, 4, 1.5, 1.6, 3, 'y', 'y']\n",
      "$533,546.00, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/2_Houston_TX.tar.gz to predict price of this house: [2003, 3393.5713949484784, 2, 1.5, 1.41, 3, 'n', 'y']\n",
      "$538,403.00, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/3_Houston_TX.tar.gz to predict price of this house: [1974, 4036.868425355425, 2, 2.0, 1.04, 3, 'n', 'y']\n",
      "$481,763.56, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/4_Houston_TX.tar.gz to predict price of this house: [1984, 3581.4609510869664, 5, 1.0, 0.93, 3, 'n', 'y']\n",
      "$477,466.38, took 1,605 ms\n",
      "\n",
      "Using model Houston_TX/5_Houston_TX.tar.gz to predict price of this house: [1986, 2919.4720854301727, 4, 1.5, 1.03, 1, 'n', 'n']\n",
      "$359,295.25, took 1,592 ms\n",
      "\n",
      "Using model Houston_TX/6_Houston_TX.tar.gz to predict price of this house: [2004, 1973.3026514985293, 2, 2.5, 0.49, 2, 'n', 'n']\n",
      "$311,840.81, took 1,651 ms\n",
      "\n",
      "Using model Houston_TX/7_Houston_TX.tar.gz to predict price of this house: [1988, 3208.976135309856, 5, 2.0, 0.98, 1, 'n', 'y']\n",
      "$432,275.16, took 1,603 ms\n",
      "\n",
      "Using model Houston_TX/8_Houston_TX.tar.gz to predict price of this house: [1989, 2476.9078204195403, 5, 1.5, 1.23, 0, 'y', 'n']\n",
      "$334,000.50, took 1,602 ms\n",
      "\n",
      "Using model Houston_TX/9_Houston_TX.tar.gz to predict price of this house: [1990, 3817.662507363658, 2, 1.0, 0.61, 0, 'y', 'y']\n",
      "$481,936.72, took 1,617 ms\n",
      "\n",
      "Using model Houston_TX/10_Houston_TX.tar.gz to predict price of this house: [1993, 4054.0913493823837, 5, 2.5, 0.61, 0, 'y', 'y']\n",
      "$594,101.44, took 1,697 ms\n",
      "\n",
      "Using model Houston_TX/11_Houston_TX.tar.gz to predict price of this house: [1995, 4537.479806080134, 4, 2.0, 0.72, 3, 'y', 'y']\n",
      "$705,473.50, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/12_Houston_TX.tar.gz to predict price of this house: [1980, 3182.248388406053, 3, 3.0, 1.0, 2, 'n', 'y']\n",
      "$399,140.28, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/13_Houston_TX.tar.gz to predict price of this house: [1995, 4277.647354023906, 6, 3.0, 1.13, 3, 'y', 'n']\n",
      "$717,545.00, took 1,648 ms\n",
      "\n",
      "Using model Houston_TX/14_Houston_TX.tar.gz to predict price of this house: [1987, 1533.483303603647, 2, 3.0, 1.05, 2, 'n', 'y']\n",
      "$180,149.20, took 1,611 ms\n",
      "\n",
      "Using model Houston_TX/15_Houston_TX.tar.gz to predict price of this house: [1985, 2040.752500512543, 6, 2.5, 1.23, 1, 'n', 'n']\n",
      "$268,491.06, took 1,586 ms\n",
      "\n",
      "Using model Houston_TX/16_Houston_TX.tar.gz to predict price of this house: [2005, 3567.0429617522814, 3, 1.0, 0.98, 2, 'y', 'y']\n",
      "$574,573.50, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/17_Houston_TX.tar.gz to predict price of this house: [1985, 3944.784689422212, 3, 3.0, 1.14, 2, 'n', 'y']\n",
      "$542,907.25, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/18_Houston_TX.tar.gz to predict price of this house: [2006, 4087.179729851723, 2, 3.0, 1.13, 0, 'n', 'n']\n",
      "$632,828.38, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/19_Houston_TX.tar.gz to predict price of this house: [1991, 3704.206993445823, 4, 2.5, 1.08, 2, 'n', 'y']\n",
      "$538,536.56, took 1,621 ms\n",
      "\n",
      "Using model Houston_TX/20_Houston_TX.tar.gz to predict price of this house: [1999, 2670.122736917456, 2, 1.5, 0.95, 0, 'y', 'y']\n",
      "$375,979.59, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/21_Houston_TX.tar.gz to predict price of this house: [1995, 2482.3070337925674, 5, 2.5, 1.39, 2, 'n', 'y']\n",
      "$395,797.25, took 1,578 ms\n",
      "\n",
      "Using model Houston_TX/22_Houston_TX.tar.gz to predict price of this house: [2006, 2246.7181419012613, 5, 2.5, 1.23, 3, 'n', 'n']\n",
      "$429,881.34, took 1,594 ms\n",
      "\n",
      "Using model Houston_TX/23_Houston_TX.tar.gz to predict price of this house: [1978, 2352.3029597609248, 5, 2.5, 1.2, 3, 'y', 'y']\n",
      "$321,872.50, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/24_Houston_TX.tar.gz to predict price of this house: [2007, 2106.6330405969843, 5, 1.0, 0.98, 0, 'y', 'n']\n",
      "$357,859.03, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/25_Houston_TX.tar.gz to predict price of this house: [2005, 4192.473408785808, 4, 2.0, 1.13, 3, 'n', 'n']\n",
      "$692,370.00, took 1,604 ms\n",
      "\n",
      "Using model Houston_TX/26_Houston_TX.tar.gz to predict price of this house: [1986, 2684.737179181817, 6, 1.0, 1.43, 3, 'n', 'y']\n",
      "$377,522.38, took 1,594 ms\n",
      "\n",
      "Using model Houston_TX/27_Houston_TX.tar.gz to predict price of this house: [1983, 2604.7920681620362, 5, 1.0, 0.83, 0, 'n', 'n']\n",
      "$277,489.81, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/28_Houston_TX.tar.gz to predict price of this house: [2002, 2694.843845038557, 6, 2.0, 0.83, 2, 'y', 'y']\n",
      "$476,301.97, took 1,597 ms\n",
      "\n",
      "Using model Houston_TX/29_Houston_TX.tar.gz to predict price of this house: [2007, 2273.84935809371, 2, 3.0, 0.7, 3, 'y', 'n']\n",
      "$427,159.34, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/30_Houston_TX.tar.gz to predict price of this house: [1997, 3397.0851674012642, 2, 2.5, 1.18, 0, 'y', 'y']\n",
      "$498,698.22, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/31_Houston_TX.tar.gz to predict price of this house: [1991, 4709.520609984447, 2, 2.5, 0.8, 0, 'n', 'y']\n",
      "$629,067.81, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/32_Houston_TX.tar.gz to predict price of this house: [1992, 2637.9554470186404, 2, 2.0, 0.95, 3, 'y', 'n']\n",
      "$389,832.69, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/33_Houston_TX.tar.gz to predict price of this house: [1983, 3631.9236301415026, 2, 1.0, 1.46, 1, 'n', 'n']\n",
      "$429,451.78, took 1,577 ms\n",
      "\n",
      "Using model Houston_TX/34_Houston_TX.tar.gz to predict price of this house: [1993, 2609.852119725409, 2, 3.0, 0.79, 1, 'n', 'n']\n",
      "$350,507.53, took 1,548 ms\n",
      "\n",
      "Using model Houston_TX/35_Houston_TX.tar.gz to predict price of this house: [2009, 2678.1458423119275, 3, 1.5, 0.86, 1, 'y', 'y']\n",
      "$453,932.56, took 1,596 ms\n",
      "\n",
      "Using model Houston_TX/36_Houston_TX.tar.gz to predict price of this house: [1972, 3830.954566122962, 6, 1.5, 0.74, 2, 'n', 'y']\n",
      "$451,637.88, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/37_Houston_TX.tar.gz to predict price of this house: [2019, 3114.0608341023367, 4, 1.5, 1.33, 0, 'y', 'n']\n",
      "$578,942.38, took 1,577 ms\n",
      "\n",
      "Using model Houston_TX/38_Houston_TX.tar.gz to predict price of this house: [2019, 3294.851760966245, 2, 2.0, 1.26, 2, 'y', 'n']\n",
      "$622,667.75, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/39_Houston_TX.tar.gz to predict price of this house: [1988, 3202.852145630624, 3, 2.0, 0.95, 1, 'n', 'n']\n",
      "$408,718.69, took 1,572 ms\n",
      "\n",
      "Using model Houston_TX/40_Houston_TX.tar.gz to predict price of this house: [2002, 2629.918646150344, 2, 3.0, 1.01, 0, 'y', 'y']\n",
      "$415,659.56, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/41_Houston_TX.tar.gz to predict price of this house: [1979, 3268.912671682102, 4, 3.0, 0.69, 3, 'n', 'y']\n",
      "$425,323.78, took 1,549 ms\n",
      "\n",
      "Using model Houston_TX/42_Houston_TX.tar.gz to predict price of this house: [1996, 4604.330309543244, 4, 1.5, 0.75, 3, 'n', 'y']\n",
      "$687,547.62, took 1,583 ms\n",
      "\n",
      "Using model Houston_TX/43_Houston_TX.tar.gz to predict price of this house: [2001, 2943.141730733062, 5, 1.0, 1.39, 0, 'y', 'y']\n",
      "$461,569.38, took 1,541 ms\n",
      "\n",
      "Using model Houston_TX/44_Houston_TX.tar.gz to predict price of this house: [1987, 2208.368211888588, 2, 1.5, 1.16, 2, 'y', 'y']\n",
      "$279,831.00, took 1,648 ms\n",
      "\n",
      "Using model Houston_TX/45_Houston_TX.tar.gz to predict price of this house: [1979, 2461.085086915727, 5, 1.0, 1.19, 2, 'n', 'n']\n",
      "$274,786.44, took 1,629 ms\n",
      "\n",
      "Using model Houston_TX/46_Houston_TX.tar.gz to predict price of this house: [1994, 3291.3120509061037, 2, 2.5, 0.84, 2, 'y', 'y']\n",
      "$489,355.31, took 1,627 ms\n",
      "\n",
      "Using model Houston_TX/47_Houston_TX.tar.gz to predict price of this house: [1999, 3341.1562572632906, 6, 1.0, 0.76, 3, 'n', 'n']\n",
      "$527,169.62, took 1,595 ms\n",
      "\n",
      "Using model Houston_TX/48_Houston_TX.tar.gz to predict price of this house: [1991, 2062.787466018825, 6, 1.0, 0.86, 3, 'n', 'y']\n",
      "$296,674.94, took 1,647 ms\n",
      "\n",
      "Using model Houston_TX/49_Houston_TX.tar.gz to predict price of this house: [1986, 2837.6674691395237, 6, 1.0, 1.16, 3, 'y', 'y']\n",
      "$418,065.88, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/50_Houston_TX.tar.gz to predict price of this house: [1985, 3802.738533650226, 4, 2.5, 0.94, 2, 'n', 'y']\n",
      "$518,106.31, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/51_Houston_TX.tar.gz to predict price of this house: [1989, 3594.4340883048244, 3, 1.5, 0.97, 2, 'y', 'n']\n",
      "$503,411.12, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/52_Houston_TX.tar.gz to predict price of this house: [1993, 2983.4799055630947, 2, 1.5, 1.29, 1, 'y', 'n']\n",
      "$414,986.75, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/53_Houston_TX.tar.gz to predict price of this house: [1987, 3174.756246039871, 3, 3.0, 1.1, 0, 'y', 'y']\n",
      "$431,108.75, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/54_Houston_TX.tar.gz to predict price of this house: [1983, 2565.5450706745114, 2, 1.0, 0.96, 3, 'y', 'n']\n",
      "$312,913.12, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/55_Houston_TX.tar.gz to predict price of this house: [2008, 3206.195047450429, 5, 3.0, 0.46, 2, 'n', 'y']\n",
      "$558,553.12, took 1,627 ms\n",
      "\n",
      "Using model Houston_TX/56_Houston_TX.tar.gz to predict price of this house: [2011, 2801.1750074310444, 4, 1.0, 1.0, 3, 'y', 'y']\n",
      "$518,613.00, took 1,595 ms\n",
      "\n",
      "Using model Houston_TX/57_Houston_TX.tar.gz to predict price of this house: [2012, 2982.5808151503807, 3, 1.0, 0.84, 3, 'n', 'y']\n",
      "$511,589.00, took 1,628 ms\n",
      "\n",
      "Using model Houston_TX/58_Houston_TX.tar.gz to predict price of this house: [1994, 3580.3035219107596, 3, 2.5, 1.09, 3, 'y', 'y']\n",
      "$565,092.00, took 1,596 ms\n",
      "\n",
      "Using model Houston_TX/59_Houston_TX.tar.gz to predict price of this house: [1981, 3830.8589887370094, 5, 1.0, 1.26, 1, 'n', 'n']\n",
      "$476,272.41, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/60_Houston_TX.tar.gz to predict price of this house: [1997, 3311.8798535610968, 2, 2.5, 0.61, 1, 'n', 'y']\n",
      "$462,581.69, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/61_Houston_TX.tar.gz to predict price of this house: [2000, 2156.28609504773, 2, 2.0, 1.32, 1, 'y', 'n']\n",
      "$338,548.31, took 1,605 ms\n",
      "\n",
      "Using model Houston_TX/62_Houston_TX.tar.gz to predict price of this house: [2004, 3361.9337602028822, 6, 1.5, 1.35, 2, 'n', 'n']\n",
      "$565,497.38, took 1,617 ms\n",
      "\n",
      "Using model Houston_TX/63_Houston_TX.tar.gz to predict price of this house: [1978, 2982.95045966464, 2, 2.5, 0.71, 3, 'n', 'n']\n",
      "$346,500.25, took 1,602 ms\n",
      "\n",
      "Using model Houston_TX/64_Houston_TX.tar.gz to predict price of this house: [1999, 3080.409088635621, 4, 2.0, 0.98, 2, 'y', 'y']\n",
      "$500,058.19, took 1,674 ms\n",
      "\n",
      "Using model Houston_TX/65_Houston_TX.tar.gz to predict price of this house: [1985, 1922.262855382029, 4, 2.0, 0.88, 0, 'n', 'n']\n",
      "$195,364.05, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/66_Houston_TX.tar.gz to predict price of this house: [1995, 3768.2035702500807, 5, 2.0, 1.06, 0, 'y', 'n']\n",
      "$563,684.62, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/67_Houston_TX.tar.gz to predict price of this house: [2011, 4065.2010480471317, 5, 1.5, 1.14, 2, 'n', 'n']\n",
      "$691,276.00, took 1,697 ms\n",
      "\n",
      "Using model Houston_TX/68_Houston_TX.tar.gz to predict price of this house: [1997, 2321.6596108512967, 5, 3.0, 0.58, 2, 'y', 'n']\n",
      "$395,837.31, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/69_Houston_TX.tar.gz to predict price of this house: [2002, 2373.7031122269636, 2, 1.0, 0.91, 3, 'n', 'n']\n",
      "$358,688.75, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/70_Houston_TX.tar.gz to predict price of this house: [1986, 3212.512692769039, 4, 1.5, 0.8, 1, 'n', 'n']\n",
      "$397,288.28, took 1,602 ms\n",
      "\n",
      "Using model Houston_TX/71_Houston_TX.tar.gz to predict price of this house: [1992, 3810.250186255341, 3, 2.5, 0.93, 3, 'y', 'y']\n",
      "$584,875.25, took 1,597 ms\n",
      "\n",
      "Using model Houston_TX/72_Houston_TX.tar.gz to predict price of this house: [1985, 2935.7910760082577, 6, 2.5, 0.93, 1, 'y', 'y']\n",
      "$419,228.22, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/73_Houston_TX.tar.gz to predict price of this house: [1986, 1816.9257860001376, 5, 3.0, 1.0, 0, 'y', 'y']\n",
      "$242,303.38, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/74_Houston_TX.tar.gz to predict price of this house: [2006, 2301.2883716614742, 2, 1.0, 1.05, 2, 'n', 'y']\n",
      "$357,177.75, took 1,640 ms\n",
      "\n",
      "Using model Houston_TX/75_Houston_TX.tar.gz to predict price of this house: [1995, 2796.17977120671, 4, 2.5, 0.84, 1, 'y', 'n']\n",
      "$426,946.62, took 1,585 ms\n",
      "\n",
      "Using model Houston_TX/76_Houston_TX.tar.gz to predict price of this house: [1992, 3804.546145805397, 4, 3.0, 0.61, 1, 'y', 'n']\n",
      "$565,418.75, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/77_Houston_TX.tar.gz to predict price of this house: [1991, 2228.540638316763, 5, 1.5, 0.49, 0, 'y', 'y']\n",
      "$288,972.75, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/78_Houston_TX.tar.gz to predict price of this house: [2018, 2379.9643217918656, 4, 3.0, 1.09, 3, 'y', 'y']\n",
      "$532,681.12, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/79_Houston_TX.tar.gz to predict price of this house: [1987, 2623.8689663290743, 5, 1.0, 0.83, 3, 'y', 'y']\n",
      "$372,227.34, took 1,602 ms\n",
      "\n",
      "Using model Houston_TX/80_Houston_TX.tar.gz to predict price of this house: [2003, 2798.665062949795, 3, 2.5, 0.69, 2, 'n', 'y']\n",
      "$445,863.94, took 1,595 ms\n",
      "\n",
      "Using model Houston_TX/81_Houston_TX.tar.gz to predict price of this house: [1998, 2262.089574005673, 6, 1.0, 0.82, 1, 'n', 'n']\n",
      "$331,426.94, took 1,675 ms\n",
      "\n",
      "Using model Houston_TX/82_Houston_TX.tar.gz to predict price of this house: [1987, 3010.7895953114535, 6, 2.5, 1.1, 3, 'y', 'y']\n",
      "$476,051.06, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/83_Houston_TX.tar.gz to predict price of this house: [1993, 2885.3347306636547, 5, 2.0, 0.79, 1, 'n', 'n']\n",
      "$405,418.75, took 1,603 ms\n",
      "\n",
      "Using model Houston_TX/84_Houston_TX.tar.gz to predict price of this house: [1995, 2857.5420732142, 5, 1.5, 1.05, 3, 'n', 'n']\n",
      "$439,694.56, took 1,699 ms\n",
      "\n",
      "Using model Houston_TX/85_Houston_TX.tar.gz to predict price of this house: [1994, 3123.561598249204, 2, 2.0, 0.92, 0, 'n', 'y']\n",
      "$401,564.62, took 1,705 ms\n",
      "\n",
      "Using model Houston_TX/86_Houston_TX.tar.gz to predict price of this house: [1982, 3111.1092911824776, 5, 1.5, 0.9, 2, 'n', 'n']\n",
      "$389,861.62, took 1,665 ms\n",
      "\n",
      "Using model Houston_TX/87_Houston_TX.tar.gz to predict price of this house: [1996, 2839.4496874772035, 2, 3.0, 1.15, 0, 'y', 'y']\n",
      "$418,839.00, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/88_Houston_TX.tar.gz to predict price of this house: [1986, 2049.898843434318, 5, 1.5, 1.23, 0, 'y', 'n']\n",
      "$254,406.94, took 1,646 ms\n",
      "\n",
      "Using model Houston_TX/89_Houston_TX.tar.gz to predict price of this house: [1989, 2990.111161347708, 6, 2.0, 1.11, 0, 'n', 'n']\n",
      "$403,502.72, took 1,701 ms\n",
      "\n",
      "Using model Houston_TX/90_Houston_TX.tar.gz to predict price of this house: [1998, 3118.8036929022883, 2, 3.0, 1.05, 0, 'y', 'y']\n",
      "$468,595.97, took 1,672 ms\n",
      "\n",
      "Using model Houston_TX/91_Houston_TX.tar.gz to predict price of this house: [1996, 2988.90801237786, 6, 2.5, 1.37, 3, 'n', 'y']\n",
      "$502,528.09, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/92_Houston_TX.tar.gz to predict price of this house: [1990, 2402.6627986206317, 3, 1.5, 1.02, 3, 'y', 'y']\n",
      "$347,446.53, took 1,674 ms\n",
      "\n",
      "Using model Houston_TX/93_Houston_TX.tar.gz to predict price of this house: [1981, 3920.551709342602, 5, 3.0, 0.78, 3, 'n', 'y']\n",
      "$546,347.19, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/94_Houston_TX.tar.gz to predict price of this house: [1991, 1255.0915130050562, 3, 1.0, 0.76, 3, 'n', 'n']\n",
      "$140,747.19, took 1,653 ms\n",
      "\n",
      "Using model Houston_TX/95_Houston_TX.tar.gz to predict price of this house: [2013, 2637.659956797664, 5, 1.5, 1.31, 2, 'y', 'n']\n",
      "$517,353.50, took 1,619 ms\n",
      "\n",
      "Using model Houston_TX/96_Houston_TX.tar.gz to predict price of this house: [1999, 2881.6798621124485, 5, 1.0, 0.83, 3, 'y', 'y']\n",
      "$473,996.72, took 1,673 ms\n",
      "\n",
      "Using model Houston_TX/97_Houston_TX.tar.gz to predict price of this house: [1999, 3585.13101223853, 6, 1.5, 0.82, 0, 'n', 'y']\n",
      "$528,463.56, took 1,675 ms\n",
      "\n",
      "Using model Houston_TX/98_Houston_TX.tar.gz to predict price of this house: [1973, 3350.2043577516797, 5, 2.5, 0.61, 2, 'y', 'y']\n",
      "$414,339.69, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/99_Houston_TX.tar.gz to predict price of this house: [1988, 3713.7598853631507, 3, 2.5, 0.83, 2, 'y', 'n']\n",
      "$531,354.50, took 1,649 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Starting with no models loaded into the container\n",
    "##Invoke the first 100 models\n",
    "invoke_multiple_models_mme(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "decreased-liechtenstein",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/0_Houston_TX.tar.gz to predict price of this house: [1993, 2059.212348811203, 4, 2.5, 0.83, 3, 'n', 'n']\n",
      "$312,405.47, took 1,645 ms\n",
      "\n",
      "Using model Houston_TX/1_Houston_TX.tar.gz to predict price of this house: [1973, 3274.6551252758586, 4, 1.0, 0.89, 0, 'y', 'n']\n",
      "$339,918.25, took 1,654 ms\n",
      "\n",
      "Using model Houston_TX/2_Houston_TX.tar.gz to predict price of this house: [1997, 2677.4526874919975, 3, 1.5, 0.81, 2, 'y', 'y']\n",
      "$404,714.91, took 1,646 ms\n",
      "\n",
      "Using model Houston_TX/3_Houston_TX.tar.gz to predict price of this house: [1981, 1454.9114542448885, 4, 1.5, 1.14, 3, 'y', 'n']\n",
      "$172,242.12, took 1,608 ms\n",
      "\n",
      "Using model Houston_TX/4_Houston_TX.tar.gz to predict price of this house: [2001, 3487.4531104647854, 2, 2.5, 1.44, 0, 'n', 'y']\n",
      "$515,330.12, took 1,695 ms\n",
      "\n",
      "Using model Houston_TX/5_Houston_TX.tar.gz to predict price of this house: [2016, 2710.329286374311, 3, 1.5, 1.18, 1, 'n', 'y']\n",
      "$479,196.41, took 1,647 ms\n",
      "\n",
      "Using model Houston_TX/6_Houston_TX.tar.gz to predict price of this house: [1990, 1830.2896996667596, 2, 1.0, 1.37, 3, 'n', 'n']\n",
      "$225,812.66, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/7_Houston_TX.tar.gz to predict price of this house: [1988, 3387.17457686301, 2, 2.5, 1.03, 0, 'y', 'y']\n",
      "$446,004.25, took 1,707 ms\n",
      "\n",
      "Using model Houston_TX/8_Houston_TX.tar.gz to predict price of this house: [1973, 3044.836851534573, 5, 2.5, 1.24, 1, 'y', 'n']\n",
      "$369,039.31, took 1,684 ms\n",
      "\n",
      "Using model Houston_TX/9_Houston_TX.tar.gz to predict price of this house: [1999, 3097.152403690697, 6, 2.0, 0.61, 1, 'n', 'n']\n",
      "$475,058.53, took 1,727 ms\n",
      "\n",
      "Using model Houston_TX/10_Houston_TX.tar.gz to predict price of this house: [1993, 3741.8937525685205, 5, 1.0, 1.11, 3, 'y', 'y']\n",
      "$577,877.25, took 1,685 ms\n",
      "\n",
      "Using model Houston_TX/11_Houston_TX.tar.gz to predict price of this house: [1985, 2281.349126017937, 4, 3.0, 1.14, 0, 'n', 'y']\n",
      "$274,540.06, took 1,788 ms\n",
      "\n",
      "Using model Houston_TX/12_Houston_TX.tar.gz to predict price of this house: [2000, 2871.732142147089, 2, 1.5, 0.74, 3, 'y', 'y']\n",
      "$452,303.47, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/13_Houston_TX.tar.gz to predict price of this house: [1981, 1615.9051718983058, 5, 2.5, 1.21, 0, 'n', 'y']\n",
      "$157,256.81, took 1,651 ms\n",
      "\n",
      "Using model Houston_TX/14_Houston_TX.tar.gz to predict price of this house: [1986, 3862.4415623860764, 5, 2.0, 0.62, 0, 'n', 'n']\n",
      "$494,782.56, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/15_Houston_TX.tar.gz to predict price of this house: [2004, 2886.847780789631, 4, 1.0, 0.82, 0, 'y', 'n']\n",
      "$443,666.91, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/16_Houston_TX.tar.gz to predict price of this house: [2002, 2282.2506414296413, 3, 1.5, 1.2, 0, 'y', 'n']\n",
      "$350,925.66, took 1,701 ms\n",
      "\n",
      "Using model Houston_TX/17_Houston_TX.tar.gz to predict price of this house: [1986, 3788.4945922318416, 5, 2.0, 1.31, 0, 'y', 'y']\n",
      "$525,651.75, took 1,604 ms\n",
      "\n",
      "Using model Houston_TX/18_Houston_TX.tar.gz to predict price of this house: [1993, 2795.97774842785, 2, 2.5, 0.79, 1, 'n', 'n']\n",
      "$368,853.00, took 1,568 ms\n",
      "\n",
      "Using model Houston_TX/19_Houston_TX.tar.gz to predict price of this house: [1995, 3451.36777289651, 3, 3.0, 1.06, 2, 'y', 'y']\n",
      "$544,402.69, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/20_Houston_TX.tar.gz to predict price of this house: [2007, 3449.8873942034065, 3, 2.0, 1.07, 0, 'y', 'n']\n",
      "$557,872.62, took 1,602 ms\n",
      "\n",
      "Using model Houston_TX/21_Houston_TX.tar.gz to predict price of this house: [2000, 3526.9559628910506, 2, 1.0, 0.95, 2, 'n', 'y']\n",
      "$506,100.38, took 1,596 ms\n",
      "\n",
      "Using model Houston_TX/22_Houston_TX.tar.gz to predict price of this house: [1990, 2435.9366035332932, 6, 2.5, 1.24, 2, 'n', 'y']\n",
      "$369,636.53, took 1,679 ms\n",
      "\n",
      "Using model Houston_TX/23_Houston_TX.tar.gz to predict price of this house: [2003, 3953.287330827305, 6, 2.0, 1.24, 3, 'n', 'n']\n",
      "$670,646.06, took 1,547 ms\n",
      "\n",
      "Using model Houston_TX/24_Houston_TX.tar.gz to predict price of this house: [1986, 2091.8903830414515, 6, 2.0, 1.1, 0, 'n', 'n']\n",
      "$253,278.11, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/25_Houston_TX.tar.gz to predict price of this house: [1985, 2588.5831403703874, 6, 2.0, 1.02, 1, 'n', 'y']\n",
      "$335,693.47, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/26_Houston_TX.tar.gz to predict price of this house: [2006, 3097.2014192636575, 6, 1.0, 1.01, 0, 'n', 'n']\n",
      "$487,700.72, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/27_Houston_TX.tar.gz to predict price of this house: [1997, 2283.8699755098005, 4, 3.0, 0.92, 0, 'n', 'y']\n",
      "$332,655.16, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/28_Houston_TX.tar.gz to predict price of this house: [2005, 2469.5679158576986, 5, 1.0, 0.94, 3, 'y', 'y']\n",
      "$446,837.47, took 1,548 ms\n",
      "\n",
      "Using model Houston_TX/29_Houston_TX.tar.gz to predict price of this house: [1977, 4347.550843239336, 6, 1.0, 0.91, 2, 'y', 'n']\n",
      "$574,346.19, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/30_Houston_TX.tar.gz to predict price of this house: [1994, 1989.2296510856268, 4, 3.0, 1.16, 1, 'n', 'n']\n",
      "$294,156.81, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/31_Houston_TX.tar.gz to predict price of this house: [1991, 1028.6549313704738, 2, 1.5, 0.49, 2, 'y', 'y']\n",
      "$107,893.41, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/32_Houston_TX.tar.gz to predict price of this house: [1987, 2540.2280893827083, 2, 2.0, 0.97, 3, 'n', 'n']\n",
      "$324,897.16, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/33_Houston_TX.tar.gz to predict price of this house: [1994, 1935.3541630600334, 5, 3.0, 1.27, 2, 'n', 'y']\n",
      "$315,273.69, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/34_Houston_TX.tar.gz to predict price of this house: [2006, 949.3615420269757, 2, 3.0, 1.0, 2, 'y', 'n']\n",
      "$216,191.45, took 1,548 ms\n",
      "\n",
      "Using model Houston_TX/35_Houston_TX.tar.gz to predict price of this house: [1999, 2336.3075080610497, 2, 2.5, 0.85, 2, 'y', 'y']\n",
      "$373,327.00, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/36_Houston_TX.tar.gz to predict price of this house: [2002, 3208.778294613591, 2, 2.5, 1.24, 1, 'n', 'n']\n",
      "$489,255.47, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/37_Houston_TX.tar.gz to predict price of this house: [1988, 1896.8227042187768, 3, 2.0, 1.12, 0, 'y', 'y']\n",
      "$227,103.06, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/38_Houston_TX.tar.gz to predict price of this house: [1996, 2891.701270099355, 4, 2.0, 1.15, 3, 'n', 'n']\n",
      "$451,142.44, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/39_Houston_TX.tar.gz to predict price of this house: [1989, 3643.647690703157, 3, 2.5, 1.26, 1, 'y', 'y']\n",
      "$521,653.72, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/40_Houston_TX.tar.gz to predict price of this house: [1993, 3297.648801588921, 2, 1.5, 0.84, 3, 'y', 'n']\n",
      "$481,419.75, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/41_Houston_TX.tar.gz to predict price of this house: [1986, 4864.794193251792, 5, 1.5, 1.25, 3, 'n', 'y']\n",
      "$697,177.62, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/42_Houston_TX.tar.gz to predict price of this house: [2008, 2649.716415118003, 4, 3.0, 0.52, 1, 'y', 'y']\n",
      "$475,130.62, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/43_Houston_TX.tar.gz to predict price of this house: [1996, 2573.0548802549815, 4, 2.5, 0.97, 1, 'y', 'y']\n",
      "$402,263.09, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/44_Houston_TX.tar.gz to predict price of this house: [1990, 2938.653249792627, 4, 1.0, 1.75, 2, 'n', 'y']\n",
      "$407,383.38, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/45_Houston_TX.tar.gz to predict price of this house: [2002, 3514.168502141511, 5, 2.5, 0.63, 3, 'n', 'n']\n",
      "$583,033.25, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/46_Houston_TX.tar.gz to predict price of this house: [1975, 3370.20455038835, 2, 2.0, 0.66, 1, 'y', 'y']\n",
      "$371,594.16, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/47_Houston_TX.tar.gz to predict price of this house: [1991, 2505.3811109172393, 2, 2.0, 0.6, 3, 'n', 'n']\n",
      "$331,489.25, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/48_Houston_TX.tar.gz to predict price of this house: [1992, 3786.3991409548726, 5, 1.0, 1.33, 1, 'n', 'n']\n",
      "$529,380.62, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/49_Houston_TX.tar.gz to predict price of this house: [1984, 2694.9423592151043, 6, 2.0, 0.65, 0, 'y', 'y']\n",
      "$346,093.94, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/50_Houston_TX.tar.gz to predict price of this house: [1985, 1452.0350452173761, 5, 3.0, 0.86, 2, 'n', 'n']\n",
      "$185,236.95, took 1,604 ms\n",
      "\n",
      "Using model Houston_TX/51_Houston_TX.tar.gz to predict price of this house: [2006, 1476.4548302510998, 6, 1.0, 0.57, 1, 'y', 'y']\n",
      "$274,570.38, took 1,593 ms\n",
      "\n",
      "Using model Houston_TX/52_Houston_TX.tar.gz to predict price of this house: [1998, 3195.406253995879, 4, 3.0, 0.88, 2, 'n', 'n']\n",
      "$503,771.75, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/53_Houston_TX.tar.gz to predict price of this house: [2000, 2574.385170775523, 5, 2.0, 0.48, 0, 'n', 'y']\n",
      "$372,796.44, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/54_Houston_TX.tar.gz to predict price of this house: [1977, 2362.687818884111, 3, 2.0, 1.06, 1, 'n', 'y']\n",
      "$228,109.42, took 1,595 ms\n",
      "\n",
      "Using model Houston_TX/55_Houston_TX.tar.gz to predict price of this house: [2007, 4287.8283844848775, 4, 1.5, 1.15, 3, 'n', 'n']\n",
      "$708,204.06, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/56_Houston_TX.tar.gz to predict price of this house: [1998, 2823.397162897926, 2, 3.0, 1.12, 0, 'y', 'y']\n",
      "$426,231.91, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/57_Houston_TX.tar.gz to predict price of this house: [1998, 2394.359011698427, 3, 3.0, 1.12, 0, 'y', 'y']\n",
      "$373,042.22, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/58_Houston_TX.tar.gz to predict price of this house: [1985, 3013.8799403586113, 2, 3.0, 0.92, 3, 'n', 'y']\n",
      "$402,856.62, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/59_Houston_TX.tar.gz to predict price of this house: [2004, 3058.1649394615074, 2, 1.5, 0.55, 2, 'n', 'n']\n",
      "$456,473.59, took 1,607 ms\n",
      "\n",
      "Using model Houston_TX/60_Houston_TX.tar.gz to predict price of this house: [1984, 3750.3723386350034, 3, 3.0, 1.26, 2, 'n', 'n']\n",
      "$511,505.19, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/61_Houston_TX.tar.gz to predict price of this house: [2001, 3578.8913256058313, 5, 2.5, 1.24, 0, 'n', 'n']\n",
      "$556,532.81, took 1,665 ms\n",
      "\n",
      "Using model Houston_TX/62_Houston_TX.tar.gz to predict price of this house: [2013, 1950.881647276348, 4, 2.5, 0.91, 3, 'n', 'y']\n",
      "$403,778.28, took 1,553 ms\n",
      "\n",
      "Using model Houston_TX/63_Houston_TX.tar.gz to predict price of this house: [1973, 2757.483059814013, 4, 3.0, 0.94, 2, 'n', 'n']\n",
      "$308,072.72, took 1,595 ms\n",
      "\n",
      "Using model Houston_TX/64_Houston_TX.tar.gz to predict price of this house: [2005, 3469.629249665083, 2, 3.0, 1.6, 1, 'y', 'n']\n",
      "$587,007.31, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/65_Houston_TX.tar.gz to predict price of this house: [2005, 2826.910065173545, 5, 1.0, 0.82, 0, 'n', 'n']\n",
      "$426,396.06, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/66_Houston_TX.tar.gz to predict price of this house: [1987, 2578.1643200134354, 2, 3.0, 0.53, 2, 'y', 'n']\n",
      "$347,500.50, took 1,549 ms\n",
      "\n",
      "Using model Houston_TX/67_Houston_TX.tar.gz to predict price of this house: [1984, 1364.0592933820715, 5, 3.0, 1.07, 3, 'y', 'n']\n",
      "$211,984.19, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/68_Houston_TX.tar.gz to predict price of this house: [1984, 2584.62500783994, 2, 1.5, 0.85, 0, 'y', 'n']\n",
      "$281,514.00, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/69_Houston_TX.tar.gz to predict price of this house: [1968, 3478.9662127914457, 6, 3.0, 0.86, 1, 'y', 'y']\n",
      "$418,430.59, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/70_Houston_TX.tar.gz to predict price of this house: [2009, 3334.829584998688, 4, 1.0, 1.32, 1, 'n', 'y']\n",
      "$540,520.56, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/71_Houston_TX.tar.gz to predict price of this house: [1995, 1949.7243082927537, 2, 1.5, 1.02, 3, 'y', 'n']\n",
      "$295,158.12, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/72_Houston_TX.tar.gz to predict price of this house: [1999, 2646.1615748451118, 2, 2.0, 1.28, 1, 'y', 'y']\n",
      "$405,545.62, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/73_Houston_TX.tar.gz to predict price of this house: [1979, 3707.062468949787, 6, 2.5, 0.68, 2, 'n', 'n']\n",
      "$487,332.41, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/74_Houston_TX.tar.gz to predict price of this house: [1979, 3376.396616499863, 3, 1.0, 1.18, 0, 'n', 'y']\n",
      "$358,760.88, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/75_Houston_TX.tar.gz to predict price of this house: [2007, 3122.1976054345987, 6, 2.5, 0.99, 3, 'y', 'n']\n",
      "$595,260.88, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/76_Houston_TX.tar.gz to predict price of this house: [2010, 1174.6965452928146, 5, 1.5, 1.05, 0, 'n', 'y']\n",
      "$221,317.47, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/77_Houston_TX.tar.gz to predict price of this house: [2000, 2806.5275418007222, 6, 3.0, 0.81, 2, 'n', 'y']\n",
      "$476,363.59, took 1,525 ms\n",
      "\n",
      "Using model Houston_TX/78_Houston_TX.tar.gz to predict price of this house: [1989, 2905.248568609915, 3, 1.0, 0.76, 3, 'y', 'y']\n",
      "$401,257.72, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/79_Houston_TX.tar.gz to predict price of this house: [1975, 3712.669870149828, 4, 3.0, 0.82, 3, 'n', 'n']\n",
      "$473,673.78, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/80_Houston_TX.tar.gz to predict price of this house: [1997, 1791.4163542897961, 5, 1.0, 0.5, 3, 'n', 'n']\n",
      "$267,737.56, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/81_Houston_TX.tar.gz to predict price of this house: [1997, 3286.5750268561014, 4, 1.5, 1.31, 0, 'n', 'n']\n",
      "$463,743.12, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/82_Houston_TX.tar.gz to predict price of this house: [2000, 2001.6070530222864, 2, 2.5, 1.4, 2, 'n', 'n']\n",
      "$317,831.12, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/83_Houston_TX.tar.gz to predict price of this house: [1982, 2046.4195221913565, 4, 1.0, 1.25, 1, 'n', 'y']\n",
      "$203,985.11, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/84_Houston_TX.tar.gz to predict price of this house: [2003, 2104.2080828147496, 6, 1.0, 0.68, 3, 'y', 'y']\n",
      "$386,086.22, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/85_Houston_TX.tar.gz to predict price of this house: [2004, 3427.309594042864, 6, 2.5, 0.72, 0, 'n', 'n']\n",
      "$547,542.19, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/86_Houston_TX.tar.gz to predict price of this house: [1994, 2811.418949785749, 5, 1.5, 0.94, 1, 'y', 'y']\n",
      "$418,550.81, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/87_Houston_TX.tar.gz to predict price of this house: [1988, 2199.5023555745506, 6, 2.0, 1.03, 3, 'n', 'n']\n",
      "$324,349.91, took 1,535 ms\n",
      "\n",
      "Using model Houston_TX/88_Houston_TX.tar.gz to predict price of this house: [1989, 3276.052503023212, 4, 3.0, 1.21, 1, 'n', 'n']\n",
      "$461,251.97, took 1,613 ms\n",
      "\n",
      "Using model Houston_TX/89_Houston_TX.tar.gz to predict price of this house: [1975, 2305.030804258927, 4, 1.5, 1.11, 2, 'y', 'n']\n",
      "$251,426.47, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/90_Houston_TX.tar.gz to predict price of this house: [2002, 2648.0206378046323, 4, 1.5, 1.36, 0, 'n', 'n']\n",
      "$395,984.03, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/91_Houston_TX.tar.gz to predict price of this house: [2000, 2341.702396130479, 6, 2.5, 0.68, 2, 'n', 'y']\n",
      "$394,219.03, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/92_Houston_TX.tar.gz to predict price of this house: [2004, 3346.9028401594533, 4, 3.0, 0.88, 3, 'y', 'y']\n",
      "$598,022.69, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/93_Houston_TX.tar.gz to predict price of this house: [1992, 2804.3960749549083, 5, 3.0, 0.61, 2, 'y', 'y']\n",
      "$442,438.41, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/94_Houston_TX.tar.gz to predict price of this house: [1984, 3932.3174229849265, 6, 1.0, 1.23, 0, 'n', 'y']\n",
      "$502,081.56, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/95_Houston_TX.tar.gz to predict price of this house: [2004, 4643.468434941777, 2, 2.0, 1.11, 0, 'n', 'y']\n",
      "$686,067.12, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/96_Houston_TX.tar.gz to predict price of this house: [1992, 3425.2857495236226, 4, 2.0, 1.1, 1, 'n', 'y']\n",
      "$477,792.72, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/97_Houston_TX.tar.gz to predict price of this house: [1992, 3988.6565533670587, 6, 1.5, 1.11, 2, 'n', 'y']\n",
      "$589,921.12, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/98_Houston_TX.tar.gz to predict price of this house: [1987, 2992.522318395771, 3, 3.0, 0.78, 1, 'y', 'n']\n",
      "$411,153.31, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/99_Houston_TX.tar.gz to predict price of this house: [1994, 2745.1578687241217, 5, 2.0, 1.35, 0, 'y', 'n']\n",
      "$412,893.12, took 1,599 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Invoke the same 100 models again\n",
    "invoke_multiple_models_mme(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ahead-tenant",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/0_Houston_TX.tar.gz to predict price of this house: [2018, 3725.099880583392, 5, 2.5, 1.61, 2, 'y', 'n']\n",
      "$732,573.88, took 1,628 ms\n",
      "\n",
      "Using model Houston_TX/1_Houston_TX.tar.gz to predict price of this house: [1995, 2335.421903559468, 2, 3.0, 1.14, 1, 'y', 'y']\n",
      "$353,438.97, took 1,591 ms\n",
      "\n",
      "Using model Houston_TX/2_Houston_TX.tar.gz to predict price of this house: [1990, 3374.6527811756982, 2, 1.0, 1.28, 0, 'y', 'n']\n",
      "$432,486.84, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/3_Houston_TX.tar.gz to predict price of this house: [2001, 2916.743932407746, 5, 3.0, 1.06, 1, 'n', 'y']\n",
      "$478,079.44, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/4_Houston_TX.tar.gz to predict price of this house: [1977, 1904.4289231718374, 3, 2.5, 0.89, 1, 'n', 'n']\n",
      "$164,743.47, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/5_Houston_TX.tar.gz to predict price of this house: [1988, 3470.80307702986, 2, 1.0, 0.89, 1, 'n', 'n']\n",
      "$417,433.56, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/6_Houston_TX.tar.gz to predict price of this house: [2016, 2990.8261352230043, 5, 1.0, 1.01, 1, 'n', 'n']\n",
      "$529,044.69, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/7_Houston_TX.tar.gz to predict price of this house: [1982, 3682.751941292027, 6, 1.0, 0.37, 1, 'n', 'n']\n",
      "$447,966.16, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/8_Houston_TX.tar.gz to predict price of this house: [2004, 3335.6054903183035, 4, 1.5, 0.9, 0, 'n', 'n']\n",
      "$497,674.41, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/9_Houston_TX.tar.gz to predict price of this house: [1978, 3241.300159416146, 4, 1.5, 1.13, 3, 'n', 'n']\n",
      "$398,507.66, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/10_Houston_TX.tar.gz to predict price of this house: [2004, 3149.540896599566, 5, 1.0, 1.03, 3, 'n', 'y']\n",
      "$520,904.19, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/11_Houston_TX.tar.gz to predict price of this house: [1999, 1655.5690211023216, 2, 1.0, 1.28, 0, 'n', 'n']\n",
      "$198,690.84, took 1,572 ms\n",
      "\n",
      "Using model Houston_TX/12_Houston_TX.tar.gz to predict price of this house: [2000, 3055.415815326563, 3, 2.5, 0.98, 3, 'n', 'n']\n",
      "$490,976.59, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/13_Houston_TX.tar.gz to predict price of this house: [2009, 2061.3062205469414, 6, 2.0, 0.75, 1, 'n', 'y']\n",
      "$376,685.62, took 1,580 ms\n",
      "\n",
      "Using model Houston_TX/14_Houston_TX.tar.gz to predict price of this house: [1991, 1770.4006684763299, 6, 2.0, 0.7, 1, 'y', 'y']\n",
      "$261,568.00, took 1,618 ms\n",
      "\n",
      "Using model Houston_TX/15_Houston_TX.tar.gz to predict price of this house: [1981, 2524.205564945238, 6, 1.0, 0.96, 2, 'n', 'y']\n",
      "$299,979.81, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/16_Houston_TX.tar.gz to predict price of this house: [1992, 3236.3967212704297, 3, 3.0, 0.82, 3, 'y', 'n']\n",
      "$505,750.78, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/17_Houston_TX.tar.gz to predict price of this house: [1991, 2088.0386291571635, 3, 3.0, 0.71, 3, 'y', 'y']\n",
      "$326,304.62, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/18_Houston_TX.tar.gz to predict price of this house: [1994, 2681.317205922456, 5, 1.0, 0.51, 1, 'y', 'y']\n",
      "$378,867.28, took 1,603 ms\n",
      "\n",
      "Using model Houston_TX/19_Houston_TX.tar.gz to predict price of this house: [2015, 2068.5568269186588, 6, 2.5, 0.96, 2, 'y', 'n']\n",
      "$463,907.28, took 1,596 ms\n",
      "\n",
      "Using model Houston_TX/20_Houston_TX.tar.gz to predict price of this house: [1990, 2778.0338995753514, 3, 2.5, 0.93, 1, 'y', 'n']\n",
      "$389,245.53, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/21_Houston_TX.tar.gz to predict price of this house: [2004, 3190.1790574087754, 3, 1.5, 0.62, 2, 'n', 'n']\n",
      "$488,840.06, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/22_Houston_TX.tar.gz to predict price of this house: [2013, 3333.752575377086, 6, 1.5, 1.21, 2, 'y', 'n']\n",
      "$629,701.69, took 1,597 ms\n",
      "\n",
      "Using model Houston_TX/23_Houston_TX.tar.gz to predict price of this house: [2009, 2636.4767924874, 2, 1.0, 0.94, 0, 'y', 'y']\n",
      "$413,968.06, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/24_Houston_TX.tar.gz to predict price of this house: [1999, 2689.191614229415, 2, 3.0, 0.61, 3, 'n', 'n']\n",
      "$420,277.78, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/25_Houston_TX.tar.gz to predict price of this house: [2006, 4668.54289453335, 4, 2.0, 0.72, 1, 'y', 'n']\n",
      "$752,125.38, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/26_Houston_TX.tar.gz to predict price of this house: [1987, 2473.5245450866496, 2, 2.0, 0.82, 2, 'n', 'y']\n",
      "$295,869.59, took 1,557 ms\n",
      "\n",
      "Using model Houston_TX/27_Houston_TX.tar.gz to predict price of this house: [1991, 2314.800308295597, 5, 3.0, 1.18, 2, 'y', 'y']\n",
      "$378,353.62, took 1,564 ms\n",
      "\n",
      "Using model Houston_TX/28_Houston_TX.tar.gz to predict price of this house: [1987, 2529.4086301151683, 4, 3.0, 0.75, 0, 'n', 'n']\n",
      "$312,234.81, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/29_Houston_TX.tar.gz to predict price of this house: [1992, 2707.4579433709237, 5, 1.0, 0.89, 2, 'y', 'n']\n",
      "$397,067.97, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/30_Houston_TX.tar.gz to predict price of this house: [1965, 2539.9230878783655, 3, 1.0, 1.26, 2, 'n', 'y']\n",
      "$192,836.31, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/31_Houston_TX.tar.gz to predict price of this house: [1997, 2913.9979667777193, 6, 2.5, 1.06, 1, 'y', 'y']\n",
      "$482,500.44, took 1,595 ms\n",
      "\n",
      "Using model Houston_TX/32_Houston_TX.tar.gz to predict price of this house: [1987, 3419.289858799808, 4, 3.0, 0.95, 2, 'n', 'y']\n",
      "$481,083.69, took 1,628 ms\n",
      "\n",
      "Using model Houston_TX/33_Houston_TX.tar.gz to predict price of this house: [2002, 3007.3303016975633, 4, 2.0, 0.75, 0, 'y', 'n']\n",
      "$468,268.91, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/34_Houston_TX.tar.gz to predict price of this house: [1977, 3562.8505823542473, 2, 2.0, 0.96, 1, 'n', 'n']\n",
      "$393,856.81, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/35_Houston_TX.tar.gz to predict price of this house: [1989, 2815.396275296747, 6, 2.0, 1.13, 2, 'n', 'n']\n",
      "$408,718.16, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/36_Houston_TX.tar.gz to predict price of this house: [1983, 3142.067856965782, 2, 2.0, 0.83, 3, 'n', 'n']\n",
      "$390,184.56, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/37_Houston_TX.tar.gz to predict price of this house: [2005, 2930.192424774834, 6, 1.0, 1.05, 1, 'y', 'n']\n",
      "$498,374.81, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/38_Houston_TX.tar.gz to predict price of this house: [2004, 3203.6760492791113, 5, 1.5, 0.86, 2, 'n', 'n']\n",
      "$518,664.88, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/39_Houston_TX.tar.gz to predict price of this house: [2000, 3157.255748123725, 4, 2.5, 1.16, 1, 'n', 'y']\n",
      "$490,895.91, took 1,606 ms\n",
      "\n",
      "Using model Houston_TX/40_Houston_TX.tar.gz to predict price of this house: [1988, 4006.4996583557877, 5, 1.5, 1.04, 3, 'y', 'n']\n",
      "$598,629.50, took 1,593 ms\n",
      "\n",
      "Using model Houston_TX/41_Houston_TX.tar.gz to predict price of this house: [1990, 3572.0987784710715, 3, 3.0, 0.85, 0, 'y', 'y']\n",
      "$499,993.78, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/42_Houston_TX.tar.gz to predict price of this house: [1990, 2587.5791243830968, 4, 2.5, 0.91, 1, 'n', 'n']\n",
      "$346,706.59, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/43_Houston_TX.tar.gz to predict price of this house: [1996, 3826.3524348949295, 6, 2.0, 0.71, 2, 'n', 'n']\n",
      "$586,071.06, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/44_Houston_TX.tar.gz to predict price of this house: [2006, 2622.1138469555917, 5, 2.0, 1.06, 1, 'n', 'y']\n",
      "$441,524.84, took 1,551 ms\n",
      "\n",
      "Using model Houston_TX/45_Houston_TX.tar.gz to predict price of this house: [2004, 2755.6956862719944, 4, 1.5, 1.23, 3, 'y', 'y']\n",
      "$490,146.41, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/46_Houston_TX.tar.gz to predict price of this house: [1994, 2962.370891388052, 4, 2.0, 1.02, 2, 'n', 'n']\n",
      "$432,485.19, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/47_Houston_TX.tar.gz to predict price of this house: [2000, 2879.186335461646, 3, 1.5, 0.89, 2, 'y', 'y']\n",
      "$452,670.28, took 1,647 ms\n",
      "\n",
      "Using model Houston_TX/48_Houston_TX.tar.gz to predict price of this house: [1993, 4119.799277665883, 2, 2.0, 0.78, 3, 'y', 'y']\n",
      "$612,269.31, took 1,577 ms\n",
      "\n",
      "Using model Houston_TX/49_Houston_TX.tar.gz to predict price of this house: [2006, 2338.7969958461763, 4, 1.0, 1.47, 1, 'y', 'y']\n",
      "$404,186.19, took 1,572 ms\n",
      "\n",
      "Using model Houston_TX/50_Houston_TX.tar.gz to predict price of this house: [1998, 3683.5579394427386, 3, 3.0, 1.04, 2, 'n', 'n']\n",
      "$569,808.25, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/51_Houston_TX.tar.gz to predict price of this house: [1996, 3351.180350835616, 5, 2.0, 0.7, 2, 'y', 'n']\n",
      "$528,439.00, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/52_Houston_TX.tar.gz to predict price of this house: [2005, 1946.7117260437224, 6, 2.0, 1.12, 0, 'n', 'n']\n",
      "$332,262.72, took 1,525 ms\n",
      "\n",
      "Using model Houston_TX/53_Houston_TX.tar.gz to predict price of this house: [1996, 3268.5182435067836, 4, 3.0, 1.23, 1, 'n', 'n']\n",
      "$497,532.22, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/54_Houston_TX.tar.gz to predict price of this house: [2003, 4319.642109768701, 3, 2.5, 0.87, 2, 'n', 'n']\n",
      "$677,450.06, took 1,598 ms\n",
      "\n",
      "Using model Houston_TX/55_Houston_TX.tar.gz to predict price of this house: [2003, 3152.7856161109994, 4, 2.5, 1.05, 2, 'y', 'y']\n",
      "$543,167.81, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/56_Houston_TX.tar.gz to predict price of this house: [2004, 3426.2439453103307, 6, 1.0, 1.13, 1, 'n', 'y']\n",
      "$544,826.12, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/57_Houston_TX.tar.gz to predict price of this house: [1994, 2941.7720407782253, 3, 3.0, 0.68, 3, 'n', 'n']\n",
      "$444,292.44, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/58_Houston_TX.tar.gz to predict price of this house: [1996, 2752.9800360669865, 3, 2.5, 1.06, 1, 'y', 'n']\n",
      "$420,399.28, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/59_Houston_TX.tar.gz to predict price of this house: [1981, 1648.44646715802, 6, 3.0, 0.5, 1, 'n', 'n']\n",
      "$179,932.48, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/60_Houston_TX.tar.gz to predict price of this house: [1996, 2853.1201814544097, 5, 1.5, 0.87, 0, 'n', 'n']\n",
      "$393,573.31, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/61_Houston_TX.tar.gz to predict price of this house: [1997, 2450.763973480215, 4, 1.5, 0.75, 2, 'y', 'y']\n",
      "$380,241.41, took 1,547 ms\n",
      "\n",
      "Using model Houston_TX/62_Houston_TX.tar.gz to predict price of this house: [1999, 4120.169808405924, 5, 3.0, 1.05, 0, 'n', 'y']\n",
      "$631,623.31, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/63_Houston_TX.tar.gz to predict price of this house: [1984, 3617.0046042046893, 3, 1.0, 1.3, 0, 'n', 'y']\n",
      "$424,070.72, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/64_Houston_TX.tar.gz to predict price of this house: [1997, 2839.171265813998, 2, 3.0, 1.05, 3, 'n', 'n']\n",
      "$443,183.38, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/65_Houston_TX.tar.gz to predict price of this house: [1998, 3403.6552571667703, 4, 2.5, 1.4, 3, 'y', 'y']\n",
      "$578,470.94, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/66_Houston_TX.tar.gz to predict price of this house: [1984, 4540.458671709725, 6, 3.0, 0.96, 0, 'y', 'n']\n",
      "$648,325.75, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/67_Houston_TX.tar.gz to predict price of this house: [1998, 3874.8298409255526, 6, 1.5, 1.18, 2, 'y', 'y']\n",
      "$630,790.88, took 1,549 ms\n",
      "\n",
      "Using model Houston_TX/68_Houston_TX.tar.gz to predict price of this house: [1993, 2837.5654887470205, 5, 1.0, 1.1, 1, 'y', 'n']\n",
      "$411,641.53, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/69_Houston_TX.tar.gz to predict price of this house: [1999, 4518.458499212982, 2, 1.5, 0.87, 2, 'n', 'n']\n",
      "$656,258.00, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/70_Houston_TX.tar.gz to predict price of this house: [2003, 3515.1151066437274, 2, 3.0, 0.79, 3, 'y', 'n']\n",
      "$593,726.44, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/71_Houston_TX.tar.gz to predict price of this house: [1985, 1590.1159860973637, 6, 2.0, 0.98, 1, 'n', 'n']\n",
      "$185,452.50, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/72_Houston_TX.tar.gz to predict price of this house: [1976, 2950.6136840553513, 6, 2.0, 0.97, 3, 'n', 'y']\n",
      "$371,872.06, took 1,549 ms\n",
      "\n",
      "Using model Houston_TX/73_Houston_TX.tar.gz to predict price of this house: [1982, 3418.758915340038, 6, 2.0, 0.91, 1, 'n', 'n']\n",
      "$441,004.19, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/74_Houston_TX.tar.gz to predict price of this house: [1988, 3049.355615296962, 4, 1.5, 0.96, 2, 'n', 'y']\n",
      "$402,991.03, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/75_Houston_TX.tar.gz to predict price of this house: [1989, 2717.8513157204206, 2, 2.5, 1.05, 2, 'y', 'n']\n",
      "$382,511.09, took 1,630 ms\n",
      "\n",
      "Using model Houston_TX/76_Houston_TX.tar.gz to predict price of this house: [1997, 4402.585326306754, 5, 3.0, 0.88, 2, 'n', 'y']\n",
      "$689,800.75, took 1,619 ms\n",
      "\n",
      "Using model Houston_TX/77_Houston_TX.tar.gz to predict price of this house: [2009, 3540.628759137551, 6, 2.5, 0.88, 2, 'y', 'n']\n",
      "$650,134.94, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/78_Houston_TX.tar.gz to predict price of this house: [1996, 3421.8463299551745, 3, 1.5, 0.99, 2, 'y', 'n']\n",
      "$515,039.06, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/79_Houston_TX.tar.gz to predict price of this house: [1987, 3254.351119495228, 2, 1.0, 1.16, 0, 'n', 'y']\n",
      "$371,314.06, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/80_Houston_TX.tar.gz to predict price of this house: [1992, 3622.561972868046, 3, 1.0, 1.43, 0, 'n', 'y']\n",
      "$470,340.38, took 1,873 ms\n",
      "\n",
      "Using model Houston_TX/81_Houston_TX.tar.gz to predict price of this house: [1996, 2686.7203029037037, 4, 2.5, 0.7, 3, 'y', 'n']\n",
      "$443,162.81, took 1,610 ms\n",
      "\n",
      "Using model Houston_TX/82_Houston_TX.tar.gz to predict price of this house: [2006, 3624.9674106531693, 5, 2.0, 1.31, 1, 'y', 'y']\n",
      "$622,069.81, took 1,630 ms\n",
      "\n",
      "Using model Houston_TX/83_Houston_TX.tar.gz to predict price of this house: [2003, 2883.7606738969484, 6, 3.0, 0.77, 0, 'y', 'y']\n",
      "$496,383.53, took 1,632 ms\n",
      "\n",
      "Using model Houston_TX/84_Houston_TX.tar.gz to predict price of this house: [2005, 3781.301585634691, 4, 3.0, 1.19, 2, 'y', 'n']\n",
      "$660,450.00, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/85_Houston_TX.tar.gz to predict price of this house: [2007, 3053.893565689354, 6, 3.0, 0.68, 2, 'y', 'n']\n",
      "$571,324.69, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/86_Houston_TX.tar.gz to predict price of this house: [1995, 2545.094799991517, 6, 2.5, 0.89, 3, 'y', 'n']\n",
      "$443,291.19, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/87_Houston_TX.tar.gz to predict price of this house: [1988, 3082.699082285498, 6, 1.0, 0.94, 1, 'n', 'y']\n",
      "$404,392.97, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/88_Houston_TX.tar.gz to predict price of this house: [1988, 3712.1761652529212, 2, 3.0, 1.26, 3, 'n', 'y']\n",
      "$531,507.00, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/89_Houston_TX.tar.gz to predict price of this house: [1983, 3993.476002106338, 3, 2.0, 0.73, 2, 'n', 'y']\n",
      "$510,432.41, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/90_Houston_TX.tar.gz to predict price of this house: [1998, 3172.568914014831, 3, 2.0, 1.1, 1, 'y', 'n']\n",
      "$485,162.94, took 1,573 ms\n",
      "\n",
      "Using model Houston_TX/91_Houston_TX.tar.gz to predict price of this house: [2000, 3137.3533674716214, 5, 2.0, 0.79, 0, 'y', 'n']\n",
      "$489,040.91, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/92_Houston_TX.tar.gz to predict price of this house: [1982, 3073.812839445355, 3, 1.5, 0.59, 2, 'n', 'y']\n",
      "$354,830.47, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/93_Houston_TX.tar.gz to predict price of this house: [2001, 3726.263077751921, 2, 2.5, 1.33, 3, 'n', 'y']\n",
      "$594,449.38, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/94_Houston_TX.tar.gz to predict price of this house: [1977, 1740.558717979032, 3, 2.0, 0.92, 3, 'y', 'n']\n",
      "$186,857.30, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/95_Houston_TX.tar.gz to predict price of this house: [1995, 2708.6425927248292, 6, 1.5, 0.86, 2, 'n', 'y']\n",
      "$408,277.03, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/96_Houston_TX.tar.gz to predict price of this house: [2012, 2195.2831687286875, 6, 1.0, 1.02, 1, 'y', 'n']\n",
      "$424,759.19, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/97_Houston_TX.tar.gz to predict price of this house: [1992, 2635.9587424886936, 2, 2.0, 1.62, 3, 'n', 'n']\n",
      "$381,876.69, took 1,603 ms\n",
      "\n",
      "Using model Houston_TX/98_Houston_TX.tar.gz to predict price of this house: [2002, 2758.0417513164375, 6, 1.0, 0.67, 2, 'n', 'n']\n",
      "$438,227.44, took 1,594 ms\n",
      "\n",
      "Using model Houston_TX/99_Houston_TX.tar.gz to predict price of this house: [1998, 2992.2444437245504, 6, 1.5, 1.42, 2, 'y', 'n']\n",
      "$504,888.81, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/100_Houston_TX.tar.gz to predict price of this house: [1997, 3368.369062086795, 2, 3.0, 0.98, 3, 'n', 'y']\n",
      "$520,572.44, took 1,678 ms\n",
      "\n",
      "Using model Houston_TX/101_Houston_TX.tar.gz to predict price of this house: [2005, 2715.951091693028, 2, 2.0, 1.2, 1, 'n', 'y']\n",
      "$421,110.25, took 1,645 ms\n",
      "\n",
      "Using model Houston_TX/102_Houston_TX.tar.gz to predict price of this house: [1990, 3731.081857859741, 5, 3.0, 1.36, 0, 'n', 'y']\n",
      "$533,846.38, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/103_Houston_TX.tar.gz to predict price of this house: [2005, 2404.786315500849, 2, 1.5, 0.84, 1, 'n', 'n']\n",
      "$356,034.91, took 1,698 ms\n",
      "\n",
      "Using model Houston_TX/104_Houston_TX.tar.gz to predict price of this house: [2003, 3349.3816156371868, 3, 1.5, 0.58, 3, 'y', 'y']\n",
      "$546,334.19, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/105_Houston_TX.tar.gz to predict price of this house: [1979, 1708.2345335150735, 4, 2.5, 1.23, 0, 'y', 'y']\n",
      "$174,595.97, took 1,674 ms\n",
      "\n",
      "Using model Houston_TX/106_Houston_TX.tar.gz to predict price of this house: [1982, 2605.7831539850326, 4, 1.5, 1.18, 2, 'n', 'n']\n",
      "$310,526.03, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/107_Houston_TX.tar.gz to predict price of this house: [1978, 3368.031433307581, 2, 1.5, 1.03, 0, 'y', 'n']\n",
      "$371,411.59, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/108_Houston_TX.tar.gz to predict price of this house: [1991, 3243.1365463499237, 5, 2.5, 1.25, 1, 'n', 'y']\n",
      "$469,427.00, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/109_Houston_TX.tar.gz to predict price of this house: [1992, 3701.1938628131297, 4, 2.5, 1.35, 0, 'n', 'y']\n",
      "$519,329.66, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/110_Houston_TX.tar.gz to predict price of this house: [1993, 1560.963211046615, 3, 1.0, 1.01, 3, 'y', 'y']\n",
      "$227,841.17, took 1,725 ms\n",
      "\n",
      "Using model Houston_TX/111_Houston_TX.tar.gz to predict price of this house: [1991, 2534.7726548689775, 2, 3.0, 0.86, 0, 'y', 'n']\n",
      "$339,585.03, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/112_Houston_TX.tar.gz to predict price of this house: [1994, 3616.516010455412, 2, 1.5, 1.01, 1, 'y', 'n']\n",
      "$507,777.69, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/113_Houston_TX.tar.gz to predict price of this house: [1995, 3580.473189942575, 3, 2.5, 0.95, 2, 'n', 'n']\n",
      "$526,881.75, took 1,698 ms\n",
      "\n",
      "Using model Houston_TX/114_Houston_TX.tar.gz to predict price of this house: [1982, 2839.2048264725013, 2, 1.5, 0.8, 0, 'n', 'y']\n",
      "$283,362.94, took 1,651 ms\n",
      "\n",
      "Using model Houston_TX/115_Houston_TX.tar.gz to predict price of this house: [1995, 3937.1862865591047, 6, 1.5, 1.19, 2, 'y', 'n']\n",
      "$624,433.75, took 1,675 ms\n",
      "\n",
      "Using model Houston_TX/116_Houston_TX.tar.gz to predict price of this house: [1991, 4014.9335488955976, 6, 2.0, 0.97, 2, 'n', 'y']\n",
      "$594,512.62, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/117_Houston_TX.tar.gz to predict price of this house: [1988, 4069.3778930287253, 3, 1.5, 1.3, 2, 'n', 'y']\n",
      "$552,987.31, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/118_Houston_TX.tar.gz to predict price of this house: [1984, 3296.840607359306, 4, 3.0, 0.96, 0, 'n', 'y']\n",
      "$416,421.56, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/119_Houston_TX.tar.gz to predict price of this house: [1986, 1961.0801687468438, 5, 1.0, 0.5, 2, 'y', 'n']\n",
      "$244,164.92, took 1,725 ms\n",
      "\n",
      "Using model Houston_TX/120_Houston_TX.tar.gz to predict price of this house: [2008, 2804.125965011638, 6, 2.0, 0.94, 1, 'y', 'y']\n",
      "$511,613.56, took 1,648 ms\n",
      "\n",
      "Using model Houston_TX/121_Houston_TX.tar.gz to predict price of this house: [1989, 2766.5252708997455, 4, 1.5, 1.03, 1, 'n', 'y']\n",
      "$352,373.00, took 1,700 ms\n",
      "\n",
      "Using model Houston_TX/122_Houston_TX.tar.gz to predict price of this house: [1990, 4285.3083013738515, 6, 2.5, 1.18, 1, 'y', 'n']\n",
      "$653,322.44, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/123_Houston_TX.tar.gz to predict price of this house: [1992, 3195.890635384975, 6, 1.5, 1.2, 1, 'y', 'y']\n",
      "$482,851.38, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/124_Houston_TX.tar.gz to predict price of this house: [1971, 2752.8104939425807, 4, 2.0, 1.18, 3, 'y', 'n']\n",
      "$323,835.12, took 2,472 ms\n",
      "\n",
      "Using model Houston_TX/125_Houston_TX.tar.gz to predict price of this house: [1969, 1759.8627566073062, 4, 1.0, 0.97, 3, 'y', 'n']\n",
      "$140,803.72, took 1,676 ms\n",
      "\n",
      "Using model Houston_TX/126_Houston_TX.tar.gz to predict price of this house: [1986, 2778.068804293796, 3, 1.0, 1.31, 3, 'y', 'y']\n",
      "$380,257.19, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/127_Houston_TX.tar.gz to predict price of this house: [1986, 4316.721770062063, 3, 3.0, 1.02, 0, 'n', 'n']\n",
      "$569,803.62, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/128_Houston_TX.tar.gz to predict price of this house: [2002, 2978.5213805083026, 3, 2.5, 1.12, 2, 'n', 'n']\n",
      "$478,146.66, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/129_Houston_TX.tar.gz to predict price of this house: [1999, 2587.2387386160376, 6, 3.0, 1.63, 3, 'n', 'n']\n",
      "$474,227.50, took 1,698 ms\n",
      "\n",
      "Using model Houston_TX/130_Houston_TX.tar.gz to predict price of this house: [2000, 3217.5057124459586, 4, 2.0, 1.38, 1, 'y', 'n']\n",
      "$520,340.56, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/131_Houston_TX.tar.gz to predict price of this house: [1991, 2998.623916540494, 3, 1.5, 0.78, 2, 'n', 'y']\n",
      "$395,818.44, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/132_Houston_TX.tar.gz to predict price of this house: [1992, 3094.695101992494, 4, 2.5, 0.75, 3, 'y', 'y']\n",
      "$484,371.69, took 1,620 ms\n",
      "\n",
      "Using model Houston_TX/133_Houston_TX.tar.gz to predict price of this house: [1983, 3711.7700655610483, 2, 2.5, 1.06, 1, 'y', 'n']\n",
      "$484,177.69, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/134_Houston_TX.tar.gz to predict price of this house: [2002, 4560.554985200182, 3, 1.0, 0.69, 1, 'y', 'n']\n",
      "$684,355.88, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/135_Houston_TX.tar.gz to predict price of this house: [1979, 2488.669504669597, 3, 2.0, 0.78, 3, 'y', 'y']\n",
      "$305,729.19, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/136_Houston_TX.tar.gz to predict price of this house: [1980, 3516.9957201059697, 6, 3.0, 0.66, 2, 'y', 'n']\n",
      "$497,649.72, took 1,596 ms\n",
      "\n",
      "Using model Houston_TX/137_Houston_TX.tar.gz to predict price of this house: [1984, 2014.940044306224, 6, 3.0, 1.31, 1, 'y', 'y']\n",
      "$295,414.56, took 1,700 ms\n",
      "\n",
      "Using model Houston_TX/138_Houston_TX.tar.gz to predict price of this house: [2001, 2060.2190908745597, 6, 2.0, 1.11, 1, 'y', 'y']\n",
      "$367,869.44, took 1,648 ms\n",
      "\n",
      "Using model Houston_TX/139_Houston_TX.tar.gz to predict price of this house: [1990, 1764.1542325617, 6, 1.0, 0.92, 3, 'n', 'y']\n",
      "$248,305.95, took 1,548 ms\n",
      "\n",
      "Using model Houston_TX/140_Houston_TX.tar.gz to predict price of this house: [2006, 2697.6144590273257, 6, 1.5, 0.86, 1, 'y', 'n']\n",
      "$473,592.22, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/141_Houston_TX.tar.gz to predict price of this house: [1984, 2416.141012528769, 5, 2.0, 1.3, 2, 'n', 'n']\n",
      "$316,103.59, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/142_Houston_TX.tar.gz to predict price of this house: [2009, 2884.9708292568866, 3, 3.0, 0.51, 2, 'n', 'n']\n",
      "$495,214.22, took 1,576 ms\n",
      "\n",
      "Using model Houston_TX/143_Houston_TX.tar.gz to predict price of this house: [1993, 2558.5680106694886, 4, 1.0, 0.98, 0, 'n', 'y']\n",
      "$316,286.25, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/144_Houston_TX.tar.gz to predict price of this house: [2002, 3316.4299113844454, 3, 3.0, 1.21, 3, 'n', 'y']\n",
      "$555,840.94, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/145_Houston_TX.tar.gz to predict price of this house: [1989, 2833.050485042475, 3, 1.0, 0.9, 0, 'n', 'n']\n",
      "$323,189.09, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/146_Houston_TX.tar.gz to predict price of this house: [1999, 2059.7595835726393, 3, 1.5, 1.03, 1, 'n', 'y']\n",
      "$288,638.69, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/147_Houston_TX.tar.gz to predict price of this house: [1983, 3405.734558761972, 5, 3.0, 0.92, 0, 'n', 'n']\n",
      "$437,194.72, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/148_Houston_TX.tar.gz to predict price of this house: [1994, 4025.355482052036, 3, 1.0, 1.43, 3, 'y', 'y']\n",
      "$611,739.75, took 1,550 ms\n",
      "\n",
      "Using model Houston_TX/149_Houston_TX.tar.gz to predict price of this house: [1992, 3212.7594684670457, 3, 1.5, 1.07, 0, 'y', 'y']\n",
      "$434,028.62, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/150_Houston_TX.tar.gz to predict price of this house: [1990, 2332.9576411055186, 6, 2.5, 1.16, 0, 'n', 'y']\n",
      "$321,438.25, took 1,675 ms\n",
      "\n",
      "Using model Houston_TX/151_Houston_TX.tar.gz to predict price of this house: [1993, 2328.1465944350207, 5, 1.5, 1.1, 1, 'n', 'n']\n",
      "$320,527.47, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/152_Houston_TX.tar.gz to predict price of this house: [1999, 3134.0189121728076, 6, 2.5, 0.9, 1, 'n', 'y']\n",
      "$497,407.12, took 1,650 ms\n",
      "\n",
      "Using model Houston_TX/153_Houston_TX.tar.gz to predict price of this house: [1983, 2906.7810297653396, 6, 1.5, 1.26, 1, 'n', 'n']\n",
      "$369,141.81, took 1,652 ms\n",
      "\n",
      "Using model Houston_TX/154_Houston_TX.tar.gz to predict price of this house: [2001, 3508.343974921593, 3, 2.5, 1.01, 2, 'n', 'n']\n",
      "$549,246.62, took 1,571 ms\n",
      "\n",
      "Using model Houston_TX/155_Houston_TX.tar.gz to predict price of this house: [1983, 1825.1655600776526, 3, 2.0, 1.12, 2, 'y', 'n']\n",
      "$220,743.36, took 1,675 ms\n",
      "\n",
      "Using model Houston_TX/156_Houston_TX.tar.gz to predict price of this house: [1997, 3478.3782940415044, 5, 3.0, 0.86, 3, 'n', 'n']\n",
      "$566,557.69, took 1,675 ms\n",
      "\n",
      "Using model Houston_TX/157_Houston_TX.tar.gz to predict price of this house: [1997, 2439.9722805839674, 3, 1.5, 1.36, 1, 'y', 'y']\n",
      "$367,651.34, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/158_Houston_TX.tar.gz to predict price of this house: [1993, 3785.9224117646945, 5, 3.0, 1.13, 3, 'y', 'n']\n",
      "$622,666.12, took 1,651 ms\n",
      "\n",
      "Using model Houston_TX/159_Houston_TX.tar.gz to predict price of this house: [1992, 1441.7431534660623, 2, 2.5, 1.1, 3, 'n', 'y']\n",
      "$200,012.11, took 1,646 ms\n",
      "\n",
      "Using model Houston_TX/160_Houston_TX.tar.gz to predict price of this house: [2010, 1904.7589819503428, 4, 1.5, 1.04, 3, 'n', 'n']\n",
      "$365,323.66, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/161_Houston_TX.tar.gz to predict price of this house: [1974, 2716.5893978210293, 6, 2.0, 1.2, 2, 'n', 'y']\n",
      "$316,745.59, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/162_Houston_TX.tar.gz to predict price of this house: [1999, 3397.173919159928, 3, 2.0, 0.71, 0, 'y', 'n']\n",
      "$498,784.44, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/163_Houston_TX.tar.gz to predict price of this house: [2019, 4068.8327957917368, 6, 2.0, 0.72, 0, 'n', 'y']\n",
      "$713,090.25, took 1,579 ms\n",
      "\n",
      "Using model Houston_TX/164_Houston_TX.tar.gz to predict price of this house: [1979, 3670.25411835014, 3, 1.0, 1.41, 1, 'y', 'y']\n",
      "$448,312.62, took 1,645 ms\n",
      "\n",
      "Using model Houston_TX/165_Houston_TX.tar.gz to predict price of this house: [1999, 4545.102719080794, 5, 2.0, 0.99, 0, 'n', 'n']\n",
      "$674,570.06, took 1,648 ms\n",
      "\n",
      "Using model Houston_TX/166_Houston_TX.tar.gz to predict price of this house: [1997, 4434.601179405223, 4, 1.0, 1.25, 3, 'n', 'n']\n",
      "$670,463.75, took 1,605 ms\n",
      "\n",
      "Using model Houston_TX/167_Houston_TX.tar.gz to predict price of this house: [1989, 3144.4874292063187, 4, 1.5, 1.09, 2, 'y', 'n']\n",
      "$450,111.75, took 1,620 ms\n",
      "\n",
      "Using model Houston_TX/168_Houston_TX.tar.gz to predict price of this house: [2003, 2942.82307190152, 2, 2.5, 1.01, 0, 'n', 'y']\n",
      "$433,730.84, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/169_Houston_TX.tar.gz to predict price of this house: [1982, 3250.253776144228, 3, 2.0, 0.77, 2, 'y', 'n']\n",
      "$419,533.28, took 1,622 ms\n",
      "\n",
      "Using model Houston_TX/170_Houston_TX.tar.gz to predict price of this house: [1992, 2994.9041801438734, 2, 1.0, 1.3, 1, 'n', 'y']\n",
      "$377,842.47, took 1,673 ms\n",
      "\n",
      "Using model Houston_TX/171_Houston_TX.tar.gz to predict price of this house: [2014, 1446.592194559973, 3, 3.0, 0.98, 1, 'y', 'n']\n",
      "$327,617.56, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/172_Houston_TX.tar.gz to predict price of this house: [1992, 3468.1721740752096, 2, 2.5, 1.2, 1, 'y', 'n']\n",
      "$498,756.62, took 1,627 ms\n",
      "\n",
      "Using model Houston_TX/173_Houston_TX.tar.gz to predict price of this house: [2003, 3942.132920597447, 3, 1.0, 1.03, 2, 'y', 'y']\n",
      "$621,308.44, took 1,551 ms\n",
      "\n",
      "Using model Houston_TX/174_Houston_TX.tar.gz to predict price of this house: [1994, 2487.8348539704443, 2, 2.0, 0.95, 3, 'n', 'y']\n",
      "$353,578.94, took 1,619 ms\n",
      "\n",
      "Using model Houston_TX/175_Houston_TX.tar.gz to predict price of this house: [1984, 2745.4139184474393, 6, 2.5, 0.28, 1, 'y', 'n']\n",
      "$369,093.94, took 1,575 ms\n",
      "\n",
      "Using model Houston_TX/176_Houston_TX.tar.gz to predict price of this house: [2009, 1771.2861718292415, 6, 2.5, 0.98, 1, 'y', 'n']\n",
      "$372,973.59, took 1,601 ms\n",
      "\n",
      "Using model Houston_TX/177_Houston_TX.tar.gz to predict price of this house: [1999, 2089.4884932714604, 3, 2.0, 1.07, 2, 'n', 'n']\n",
      "$318,835.78, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/178_Houston_TX.tar.gz to predict price of this house: [1983, 2586.058460376966, 3, 2.0, 1.23, 2, 'n', 'n']\n",
      "$312,670.34, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/179_Houston_TX.tar.gz to predict price of this house: [1986, 3375.859540117509, 3, 2.0, 0.66, 2, 'n', 'n']\n",
      "$432,137.47, took 1,600 ms\n",
      "\n",
      "Using model Houston_TX/180_Houston_TX.tar.gz to predict price of this house: [2010, 3453.190967839877, 5, 1.0, 0.69, 1, 'n', 'n']\n",
      "$558,436.56, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/181_Houston_TX.tar.gz to predict price of this house: [1986, 3221.491547291837, 3, 2.5, 0.58, 3, 'y', 'y']\n",
      "$456,518.53, took 1,628 ms\n",
      "\n",
      "Using model Houston_TX/182_Houston_TX.tar.gz to predict price of this house: [1994, 4222.802179444209, 6, 2.0, 1.48, 2, 'n', 'y']\n",
      "$654,181.75, took 1,594 ms\n",
      "\n",
      "Using model Houston_TX/183_Houston_TX.tar.gz to predict price of this house: [2018, 4264.291433842246, 6, 2.5, 0.83, 3, 'n', 'y']\n",
      "$795,441.12, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/184_Houston_TX.tar.gz to predict price of this house: [1986, 2532.690968720601, 3, 2.5, 1.37, 3, 'y', 'y']\n",
      "$373,476.78, took 1,626 ms\n",
      "\n",
      "Using model Houston_TX/185_Houston_TX.tar.gz to predict price of this house: [1983, 3608.5593435417277, 5, 3.0, 1.46, 0, 'n', 'y']\n",
      "$481,154.16, took 1,648 ms\n",
      "\n",
      "Using model Houston_TX/186_Houston_TX.tar.gz to predict price of this house: [1993, 3389.8487160476943, 3, 1.0, 0.94, 3, 'n', 'n']\n",
      "$474,657.06, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/187_Houston_TX.tar.gz to predict price of this house: [1991, 2984.7506704556736, 4, 2.0, 1.41, 0, 'n', 'n']\n",
      "$398,996.56, took 1,674 ms\n",
      "\n",
      "Using model Houston_TX/188_Houston_TX.tar.gz to predict price of this house: [1993, 1627.1474288952081, 2, 2.0, 0.93, 3, 'y', 'n']\n",
      "$243,628.64, took 1,581 ms\n",
      "\n",
      "Using model Houston_TX/189_Houston_TX.tar.gz to predict price of this house: [2006, 3616.023804381903, 5, 2.5, 0.9, 1, 'y', 'y']\n",
      "$619,892.94, took 1,668 ms\n",
      "\n",
      "Using model Houston_TX/190_Houston_TX.tar.gz to predict price of this house: [1991, 3601.816275819371, 4, 3.0, 0.87, 1, 'n', 'n']\n",
      "$511,914.19, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/191_Houston_TX.tar.gz to predict price of this house: [2004, 2575.294151147105, 6, 1.5, 0.84, 3, 'y', 'n']\n",
      "$475,085.06, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/192_Houston_TX.tar.gz to predict price of this house: [2005, 4593.026716912943, 4, 3.0, 0.58, 3, 'n', 'n']\n",
      "$757,294.12, took 1,625 ms\n",
      "\n",
      "Using model Houston_TX/193_Houston_TX.tar.gz to predict price of this house: [1978, 3417.224719237198, 6, 2.5, 1.02, 3, 'n', 'y']\n",
      "$462,818.34, took 1,623 ms\n",
      "\n",
      "Using model Houston_TX/194_Houston_TX.tar.gz to predict price of this house: [1995, 2033.9675306944796, 4, 1.0, 1.17, 2, 'n', 'n']\n",
      "$283,949.50, took 1,624 ms\n",
      "\n",
      "Using model Houston_TX/195_Houston_TX.tar.gz to predict price of this house: [1972, 3166.7518884807346, 2, 2.0, 1.29, 1, 'n', 'y']\n",
      "$316,728.62, took 1,574 ms\n",
      "\n",
      "Using model Houston_TX/196_Houston_TX.tar.gz to predict price of this house: [2003, 4394.544663864366, 4, 1.0, 1.32, 0, 'n', 'n']\n",
      "$651,657.25, took 1,649 ms\n",
      "\n",
      "Using model Houston_TX/197_Houston_TX.tar.gz to predict price of this house: [2003, 4214.705213925773, 5, 2.5, 0.82, 1, 'y', 'n']\n",
      "$691,381.12, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/198_Houston_TX.tar.gz to predict price of this house: [1981, 2752.696401099814, 3, 1.0, 0.84, 0, 'n', 'n']\n",
      "$267,505.59, took 1,599 ms\n",
      "\n",
      "Using model Houston_TX/199_Houston_TX.tar.gz to predict price of this house: [1996, 2183.426971911059, 3, 1.5, 1.33, 0, 'n', 'y']\n",
      "$283,422.69, took 1,626 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##This time invoke all 200 models to observe behavior\n",
    "invoke_multiple_models_mme(0, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-stress",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### CloudWatch charts for LoadedModelCount,MemoryUtilization and ModelCacheHit metrics will be similar to charts below.\n",
    "\n",
    "![](cw_charts/ModelCountMemUtilization.png)\n",
    "\n",
    "LoadedModelCount continuously increases, as more models are invoked, till it levels off at 121.  MemoryUtilization of the container also increased correspondingly to around 79%.  This shows that the instance chosen to host the endpoint, could only maintain 121 models in memory, when 200 model invocations are made.  \n",
    "\n",
    "![](cw_charts/ModelCountMemUtilizationCacheHit.png)\n",
    "\n",
    "As the number of models loaded to the container memory increase, the ModelCacheHit improves.  When the same 100 models are invoked the second time, the ModelCacheHit reaches 1.  When new models, not yet loaded are invoked the ModelCacheHit decreases again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-cooling",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Section 9 - Explore granular access to the target models of MME <a id='Finegrain-control-invoke-models'></a>\n",
    "\n",
    "If the role attached to this notebook instance allows invoking SageMaker endpoints, it is able to invoke all models hosted on the MME.  Using IAM conditional keys, you can restrict this model invocation access to specific models.  To explore this, you will create a new IAM role and IAM policy with conditional key to restrict access to a single model.  Assume this new role and verify that only a single target model can be invoked.\n",
    "\n",
    "Note that to execute this section, the role attached to the notebook instance should allow the following actions :\n",
    "    \"iam:CreateRole\",\n",
    "    \"iam:CreatePolicy\",\n",
    "    \"iam:AttachRolePolicy\",\n",
    "    \"iam:UpdateAssumeRolePolicy\"\n",
    "    \n",
    "If this is not the case, please work with the Administrator of this AWS account to ensure this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "outside-nomination",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_client = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "spectacular-darwin",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a new role that can be assumed by this notebook.  The roles should allow access to only a single model.\n",
    "\n",
    "path='/'\n",
    "\n",
    "role_name=\"{}{}\".format('allow_invoke_ny_model_role', strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "description='Role that allows invoking a single model'\n",
    "\n",
    "action_string = \"sts:AssumeRole\"\n",
    "    \n",
    "trust_policy={\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"statement1\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": role\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "operating-album",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = iam_client.create_role(\n",
    "    Path=path,\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "    Description=description,\n",
    "    MaxSessionDuration=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "framed-compilation",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role arn is : arn:aws:iam::688520471316:role/allow_invoke_ny_model_role2021-05-28-21-23-36\n"
     ]
    }
   ],
   "source": [
    "role_arn=response['Role']['Arn']\n",
    "print(\"Role arn is :\", role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "owned-worth",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint arn is : arn:aws:sagemaker:us-west-2:688520471316:endpoint/inference-pipeline-ep-2021-05-28-21-03-00\n"
     ]
    }
   ],
   "source": [
    "endpoint_resource_arn = \"arn:aws:sagemaker:{}:{}:endpoint/{}\".format(REGION, ACCOUNT_ID, endpoint_name)\n",
    "print(\"Endpoint arn is :\", endpoint_resource_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "earned-stomach",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Create the IAM policy with the IAM condition key\n",
    "policy_name = \"{}{}\".format('allow_invoke_ny_model_policy', strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "managed_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"SageMakerAccess\",\n",
    "            \"Action\": \"sagemaker:InvokeEndpoint\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\":endpoint_resource_arn,\n",
    "            \"Condition\": {\n",
    "                \"StringLike\": {\n",
    "                    \"sagemaker:TargetModel\": [\"NewYork_NY/*\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = iam_client.create_policy(\n",
    "  PolicyName=policy_name,\n",
    "  PolicyDocument=json.dumps(managed_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "provincial-haiti",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_arn=response['Policy']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "southwest-helena",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4619d603-f251-495e-9e1d-15c09fed6a92',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4619d603-f251-495e-9e1d-15c09fed6a92',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Fri, 28 May 2021 21:23:36 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Attach policy to role\n",
    "iam_client.attach_role_policy(\n",
    "    PolicyArn=policy_arn,\n",
    "    RoleName=role_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "incorporated-radiation",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::688520471316:assumed-role/hongshan-sagemaker-experiment/i-01b049c9062e751b4 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::688520471316:role/allow_invoke_ny_model_role2021-05-28-21-23-36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-99623cdc35b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m assumed_role_limited_access = sts_connection.assume_role(\n\u001b[1;32m      4\u001b[0m     \u001b[0mRoleArn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrole_arn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mRoleSessionName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MME_Invoke_NY_Model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0massumed_role_limited_access\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AssumedRoleUser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Arn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::688520471316:assumed-role/hongshan-sagemaker-experiment/i-01b049c9062e751b4 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::688520471316:role/allow_invoke_ny_model_role2021-05-28-21-23-36"
     ]
    }
   ],
   "source": [
    "## Invoke with the role that has access to only NY model\n",
    "sts_connection = boto3.client('sts')\n",
    "assumed_role_limited_access = sts_connection.assume_role(\n",
    "    RoleArn=role_arn,\n",
    "    RoleSessionName=\"MME_Invoke_NY_Model\"\n",
    ")\n",
    "assumed_role_limited_access['AssumedRoleUser']['Arn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-novelty",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trust_policy={\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"statement1\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": role\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    },\n",
    "    {\n",
    "      \"Sid\": \"statement2\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "          \"AWS\": assumed_role_limited_access['AssumedRoleUser']['Arn']\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }  \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-sampling",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_client.update_assume_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyDocument=json.dumps(trust_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-separate",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY = assumed_role_limited_access['Credentials']['AccessKeyId']\n",
    "SECRET_KEY = assumed_role_limited_access['Credentials']['SecretAccessKey']\n",
    "SESSION_TOKEN = assumed_role_limited_access['Credentials']['SessionToken']\n",
    "\n",
    "runtime_sm_client_with_assumed_role = boto3.client(\n",
    "    service_name='sagemaker-runtime', \n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-macintosh",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " sagemakerSessionAssumedRole = sagemaker.Session(sagemaker_runtime_client=runtime_sm_client_with_assumed_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-participant",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictorAssumedRole = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemakerSessionAssumedRole,\n",
    "    serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-zoning",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_model_name = 'NewYork_NY/NewYork_NY.tar.gz'\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name,predictorAssumedRole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-explorer",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "##This should fail with \"AccessDeniedException\" since the assumed role does not have access to Chicago model\n",
    "full_model_name = 'Chicago_IL/Chicago_IL.tar.gz'\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name,predictorAssumedRole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-retirement",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Clean up<a id='CleanUp'></a>\n",
    "Clean up the endpoint to avoid unneccessary costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-costume",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Delete the endpoint and underlying model\n",
    "predictor.delete_model() \n",
    "predictor.delete_endpoint()\n",
    "for t in preprocessor_transformers:\n",
    "    t.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-throat",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Delete the IAM Role\n",
    "iam_client.detach_role_policy(\n",
    "    PolicyArn=policy_arn,\n",
    "    RoleName=role_name\n",
    ")\n",
    "iam_client.delete_role(RoleName=role_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-inspiration",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Delete the IAM Policy\n",
    "iam_client.delete_policy(PolicyArn=policy_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-order",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1666.174512,
   "end_time": "2021-05-26T16:18:15.082012",
   "environment_variables": {},
   "exception": true,
   "input_path": "linear_learner_multi_model_endpoint_inf_pipeline.ipynb",
   "output_path": "/opt/ml/processing/output/linear_learner_multi_model_endpoint_inf_pipeline-2021-05-26-15-46-44.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-05-26T15:50:28.907500",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
