{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering using Embeddings\n",
    "\n",
    "Many use cases require text (text2text) generation models like **BloomZ** and **Flan T5** to respond to user questions with insightful answers. For example, a customer support chatbot may need to provide answers to common questions. The **BloomZ** and **Flan T5** models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.\n",
    "\n",
    "In this notebook we will demonstrate a method for enabling **Flan T5** to answer questions using a library of text as a reference, by using document embeddings and retrieval. We prepared the database based on the answers here - https://aws.amazon.com/sagemaker/faqs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deploy Flan T5 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"*\"\n",
    "\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name_flan_t5 = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "model_inference = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name_flan_t5,\n",
    ")\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor_inference = model_inference.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name_flan_t5,\n",
    "    volume_size=30,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Responses from the flan T5 XL without providing the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type='application/json'):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json)\n",
    "    return response\n",
    "\n",
    "def parse_response_model_flan_t5(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text_inputs\": question, \n",
    "    \"max_length\":50, \n",
    "    \"num_return_sequences\":1, \n",
    "    \"top_k\":50, \n",
    "    \"top_p\":0.95, \n",
    "    \"do_sample\":True\n",
    "}\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'), endpoint_name=endpoint_name_flan_t5)\n",
    "\n",
    "generated_texts = parse_response_model_flan_t5(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieving the most relevant context from the database for the question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Deploying the model endpoint for getting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textembedding-gpt-j-6b\", \"*\"\n",
    "\n",
    "endpoint_name_embed = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "\n",
    "embed_model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=\"s3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/textembedding/infer-huggingface-textembedding-huggingface-textembedding-gpt-j-6b-20230320-2050-repack.tar.gz\",\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name_embed,\n",
    ")\n",
    "\n",
    "model_predictor_embed = embed_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name_embed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response_text_embed(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text.append(model_predictions['embedding'])\n",
    "    return generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Preprocess the document library\n",
    "We plan to use document embeddings to fetch the most relevant parts of our document library and insert them into the prompt that we provide to **Flan T5 Xl**.\n",
    "\n",
    "Sections should be large enough to contain enough information to answer a question; but small enough to fit one or several into the **Flan T5 Xl** prompt. We find that approximately a paragraph of text is usually a good length, but you should experiment for your particular use case. In this example, we prepared the database based on the answers here - https://aws.amazon.com/sagemaker/faqs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the Database\n",
    "!aws s3 cp s3://hemamsin-jump-test-pdx/data/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv Amazon_SageMaker_FAQs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_answers = pd.read_csv('Amazon_SageMaker_FAQs.csv',names =[\"Question\", \"Answer\"])\n",
    "df_answers.set_index([\"Question\"])\n",
    "\n",
    "res_embed = []\n",
    "for idx, row in df_answers.iterrows():\n",
    "    query_response = query_endpoint_with_json_payload(row[\"Answer\"], endpoint_name_embed, content_type=\"application/x-text\")\n",
    "    generated_embed = parse_response_text_embed(query_response)[0][0]\n",
    "    res_embed.append(generated_embed)\n",
    "res_embed_df = pd.DataFrame(res_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "train_features = np.array(res_embed)\n",
    "\n",
    "# Providing each answer embedding label\n",
    "train_labels = np.array([i for i in range(len(train_features))])\n",
    "\n",
    "print(\"train_features shape = \", train_features.shape)\n",
    "print(\"train_labels shape = \", train_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "\n",
    "bucket = sess.default_bucket()  # modify to your bucket name\n",
    "prefix = \"Database\"\n",
    "key = \"Amazon-SageMaker-FAQs\"\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"train\", key)).upload_fileobj(buf)\n",
    "s3_train_data = f\"s3://{bucket}/{prefix}/train/{key}\"\n",
    "print(f\"uploaded training data location: {s3_train_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data,\n",
    "    and return a deployed predictor\n",
    "\n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(\n",
    "        get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        aws_role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "\n",
    "    # train a model. fit_input contains the locations of the train data\n",
    "    fit_input = {\"train\": s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "\n",
    "hyperparams = {\"feature_dim\": train_features.shape[1], \"k\": 5,\"sample_size\": train_features.shape[0], \"predictor_type\": \"classifier\"}\n",
    "output_path = f\"s3://{bucket}/{prefix}/default_example/output\"\n",
    "knn_estimator = trained_estimator_from_hyperparams(\n",
    "    s3_train_data, hyperparams, output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "def predictor_from_estimator(knn_estimator, instance_type, endpoint_name=None):\n",
    "    knn_predictor = knn_estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name\n",
    "    )\n",
    "    knn_predictor.serializer = CSVSerializer()\n",
    "    knn_predictor.deserializer = JSONDeserializer()\n",
    "    return knn_predictor\n",
    "\n",
    "\n",
    "instance_type = \"ml.m4.xlarge\"\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-knn\")\n",
    "\n",
    "knn_predictor = predictor_from_estimator(\n",
    "    knn_estimator, instance_type, endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Doing the inference with context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Getting the Context For the Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "def construct_context(context_predictions_arr) -> str:\n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "     \n",
    "    for index in context_predictions_arr:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df_answers[\"Answer\"][index]\n",
    "        chosen_sections_len += len(document_section) + 2 \n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.replace(\"\\n\", \" \"))\n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")    \n",
    "        \n",
    "    return \"\".join(chosen_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query_endpoint_with_json_payload(question, endpoint_name_embed, content_type=\"application/x-text\")\n",
    "question_embedding = parse_response_text_embed(query_response)[0][0]\n",
    "\n",
    "#Getting the most relevant context using KNN\n",
    "context_predictions_arr = knn_predictor.predict(np.array(question_embedding), initial_args={\"ContentType\": \"text/csv\",\"Accept\": \"application/json; verbose=true\"})['predictions'][0][\"labels\"]\n",
    "\n",
    "context_embed_retrieve = construct_context(context_predictions_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Doing the Inference with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_flan_t5 = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n",
    "\n",
    "input_flan_t5 = prompts_flan_t5.replace(\"{context}\", context_embed_retrieve)\n",
    "input_flan_t5 = input_flan_t5.replace(\"{question}\", question)\n",
    "\n",
    "payload = {\n",
    "    \"text_inputs\": input_flan_t5, \n",
    "    \"max_length\":500, \n",
    "    \"num_return_sequences\":1, \n",
    "    \"top_k\":50, \n",
    "    \"top_p\":0.95, \n",
    "    \"do_sample\":True\n",
    "}\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'), endpoint_name=endpoint_name_flan_t5)\n",
    "\n",
    "generated_texts = parse_response_model_flan_t5(query_response)\n",
    "print(generated_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
