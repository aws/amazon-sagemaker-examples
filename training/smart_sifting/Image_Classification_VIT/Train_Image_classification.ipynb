{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Image Classification Model using VIT and Smart Sifting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train a image classification model using Vision Transformer (VIT) and Smart Sifting library. VIT is a transformer encoder model pretrained on large collection of images from ImageNEt at a resolution of 224X224 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Introduction to Smart Sifting \n",
    "\n",
    "Smart Sifting is a framework to speed up training of PyTorch models. The framework implements a set of algorithms that filter out inconsequential training examples during training, reducing the computational cost and accelerating the training process. It is configuration-driven and extensible, allowing users to add custom logic to transform their training examples into a filterable format. Smart sifting provides a generic utility for any DNN model, and can reduce the training cost by up to 35% in infrastructure cost.\n",
    "\n",
    "![image](sifting_flow.png)\n",
    "\n",
    "\n",
    "\n",
    "Smart sifting’s task is to sift through your training data during the training process and only feed the more informative samples to the model. During typical training with PyTorch, data is iteratively sent in batches to the training loop and to accelerator devices (e.g. GPUs or Trainium chips) by the PyTorch data loader. Smart sifting is implemented at this data loading stage and is thus independent of any upstream data preprocessing in your training pipeline. Smart sifting uses your live model and a user specified loss function to do an evaluative forward pass of each data sample as it is loaded. Samples which are high loss will materially impact model training and thus are included in training data; meanwhile data samples which are relatively low loss are already well represented by the model and so are set aside and excluded from training. A key input to smart sifting is the proportion of data to exclude: for example, by setting the proportion to 25%, samples in approximately the bottom quartile of loss of each batch will be excluded from training. Once enough high-loss samples have been identified to complete a batch, the data is sent through the full training loop and the model learns and trains normally. Customers don’t need to make any downstream changes to their training loop when smart sifting is enabled.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    ! pip install datasets transformers --quiet\n",
    "    ! pip install -U sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset\n",
    "\n",
    "For this training we will be using [Caltech-101 dataset](https://data.caltech.edu/records/mzrjq-6wc02). Caltech-101 consists of pictures of objects belonging to 101 classes. Each class contains roughly 40 to 800 images, totalling around 9k images. Images are of variable sizes, with typical edge lengths of 200-300 pixels. \n",
    "\n",
    "Lets start by downloading and extracting the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp --recursive s3://sagemaker-example-files-prod-us-west-2/datasets/image/caltech-101/ ./caltech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xf ./caltech/101_ObjectCategories.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the downloaded data into huggingface datasets arrow format. Note: This is done for convenience not a requirement for sifting library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"101_ObjectCategories\")\n",
    "ds_train_devtest = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_splits = DatasetDict(\n",
    "    {\"train\": ds_train_devtest[\"train\"], \"validation\": ds_train_devtest[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we should have a dataset with train and validation splits. Lets print the dataset to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Splits: \\n {ds_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataset to S3 for Training\n",
    "\n",
    "Lets upload the dataset to S3 , for this we will leverage the dataset API S3 integration to directly save DataSet object to s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f\"s3://{sess.default_bucket()}/dataset/caltech101\"\n",
    "print(f\"uploading training dataset to: {training_input_path}\")  # save train_dataset to s3\n",
    "ds_splits.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run training Job using SageMaker Training.\n",
    "\n",
    "\n",
    "Adding Sifting library to the Image classification code involves following the below steps\n",
    "\n",
    "1. **Define Loss Function** - For Image classification we use CrossEntropy loss defined as below\n",
    "    ````\n",
    "    class ImageLoss(Loss):\n",
    "    \"\"\"\n",
    "    This is an implementation of the Loss interface for the model \n",
    "    required for Smart Sifting. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.celoss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def loss(\n",
    "            self,\n",
    "            model: torch.nn.Module,\n",
    "            transformed_batch: SiftingBatch,\n",
    "            original_batch: Any = None,\n",
    "    ) -> torch.Tensor:\n",
    "        device = next(model.parameters()).device\n",
    "        batch = {k: v.to(device) for k, v, in original_batch.items()}\n",
    "\n",
    "        # compute loss\n",
    "        outputs = model(**batch)\n",
    "        return self.celoss(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "    ````\n",
    "2. **Define Transformation Function** to convert input batch to sifting format.\n",
    "\n",
    "````\n",
    "class ImageListBatchTransform(SiftingBatchTransform):\n",
    "    \"\"\"\n",
    "    This is an implementation of the data transforms for the model \n",
    "    required for Smart Sifting. Transform to and from ListBatch\n",
    "    \"\"\"\n",
    "    def transform(self, batch: Any):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(batch[\"pixel_values\"])):\n",
    "            inputs.append(batch[\"pixel_values\"][i])\n",
    "\n",
    "        for i in range(len(batch[\"labels\"])):\n",
    "            labels.append(batch[\"labels\"][i])\n",
    "\n",
    "        return ListBatch(inputs, labels)\n",
    "    \n",
    "    def reverse_transform(self, list_batch: ListBatch):\n",
    "        a_batch = {}\n",
    "        a_batch[\"pixel_values\"] = self.stack_tensors(list_batch.inputs)\n",
    "        a_batch[\"labels\"] = self.stack_tensors(list_batch.labels)\n",
    "  \n",
    "        return a_batch\n",
    "\n",
    "    def stack_tensors(self,list_of_tensors):\n",
    "        if list_of_tensors:\n",
    "            t = torch.stack(list_of_tensors)\n",
    "        else:\n",
    "            t = torch.tensor([])\n",
    "        return t\n",
    "````\n",
    "\n",
    "\n",
    "3. **Define sifting config** - Define configuration for sifting. \n",
    "\n",
    "    Beta_value depicts the proportion of samples to keep , higher the value more samples are sifted. \n",
    "    loss_history_length - Depicts the window of samples to include when evaluating relative loss.\n",
    "\n",
    "````\n",
    "   sift_config = RelativeProbabilisticSiftConfig(\n",
    "            beta_value=3,\n",
    "            loss_history_length=500,\n",
    "            loss_based_sift_config=LossConfig(\n",
    "                 sift_config=SiftingBaseConfig(sift_delay=10)\n",
    "            )\n",
    "        )\n",
    "````\n",
    "4. **Wrap the Pytorch Data Loader with Sifting Data loader.\n",
    "   As a last step we wrap the Pytroch Dataloader with siftingDataLoader passing config, transformation and loss functions.\n",
    "\n",
    "```\n",
    "  train_dataloader = SiftingDataloader(\n",
    "                sift_config=sift_config,\n",
    "                orig_dataloader=train_dataloader,\n",
    "                batch_transforms=ImageListBatchTransform(),\n",
    "                loss_impl=ImageLoss(),\n",
    "                model=model\n",
    "        )  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define few metrics to be tracked inorder to monitor sifting. This are optional metrics useful to debug and understand sifting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAGEMAKER_METRIC_DEFINITIONS = [\n",
    "    # Sifting Metrics\n",
    "    {\"Name\": \"num_samples_batch:count\", \"Regex\": \"num_orig_samples_seen: ([0-9.]+)\"},\n",
    "    {\"Name\": \"num_kept_batch:count\", \"Regex\": \"num_orig_samples_kept: ([0-9.]+)\"},\n",
    "    {\"Name\": \"batch_size:count\", \"Regex\": \"Batch size for next sifted batch: ([0-9.]+)\"},\n",
    "    {\n",
    "        \"Name\": \"sifted_batch_creation:latency\",\n",
    "        \"Regex\": \"sifted_batch_creation_latency: ([0-9.e+\\-]+)\",\n",
    "    },\n",
    "    {\"Name\": \"sifting_batch_transform:latency\", \"Regex\": \"func:transform latency: ([0-9.e+\\-]+)\"},\n",
    "    {\n",
    "        \"Name\": \"calc_rel_vals:latency\",\n",
    "        \"Regex\": \"func:calculate_relevance_values latency: ([0-9.e+\\-]+)\",\n",
    "    },\n",
    "    {\"Name\": \"should_sift:latency\", \"Regex\": \"func:should_sift latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"sift:latency\", \"Regex\": \"func:sift latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"accumulate:latency\", \"Regex\": \"func:accumulate latency: ([0-9.e+\\-]+)\"},\n",
    "    {\n",
    "        \"Name\": \"orig_batch_transform:latency\",\n",
    "        \"Regex\": \"func:reverse_transform latency: ([0-9.e+\\-]+)\",\n",
    "    },\n",
    "    # The following two metrics (rel_vals:number, kept_rel_vals:number) have multiple data points per log line, but\n",
    "    # this metric capturing method can only capture one data point per log line.\n",
    "    # TODO: capture all data points once we have a custom reporting API for sifting.\n",
    "    {\"Name\": \"rel_vals:number\", \"Regex\": \"relevance_values: \\[([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"kept_rel_vals:number\", \"Regex\": \"relevance_values_kept: \\[([0-9.e+\\-]+)\"},\n",
    "    # Training Metrics\n",
    "    {\"Name\": \"train_step:count\", \"Regex\": \"train step count: ([0-9.]+)\"},\n",
    "    {\"Name\": \"train_fp:latency\", \"Regex\": \"train forward pass latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"train_bp:latency\", \"Regex\": \"train backprop latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"train_optim:latency\", \"Regex\": \"train optimizer step latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"train_total_step:latency\", \"Regex\": \"train total step latency: ([0-9.e+\\-]+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "\n",
    "# change the model name/path here to switch between resnet: \"microsoft/resnet-101\" and vit: \"google/vit-base-patch16-224-in21k\"\n",
    "# hyperparameters[\"model_name_or_path\"] = \"microsoft/resnet-101\"\n",
    "hyperparameters[\"model_name_or_path\"] = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "hyperparameters[\"seed\"] = 100\n",
    "hyperparameters[\"per_device_train_batch_size\"] = 64\n",
    "hyperparameters[\"per_device_eval_batch_size\"] = 64\n",
    "hyperparameters[\"learning_rate\"] = 5e-5\n",
    "\n",
    "hyperparameters[\"max_train_steps\"] = 1000  # use 10000\n",
    "hyperparameters[\"num_train_epochs\"] = 4\n",
    "\n",
    "hyperparameters[\"use_sifting\"] = 1  # param for enabling sifting 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will launch the training job using G5.2xlarge instance. Smart Sifting library is part of the SageMaker Pytorch Deep Learning containers starting version 2.0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_job_name = \"vit-img-classification-sifting\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=base_job_name,\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"train_images.py\",\n",
    "    role=role,\n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,\n",
    "    metric_definitions=SAGEMAKER_METRIC_DEFINITIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the training job with Data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"train\": training_input_path}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we looked at how to use smart sifting library to train an Image classification model. Smart sifting helps in reducing training time upto 40% without any reduction in Model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/training|smart_sifting|Image_Classification_VIT|Train_Image_classification.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
