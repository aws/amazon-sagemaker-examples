{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Governance - unified model card and model registry\n",
    "\n",
    "This notebook walks you through the features of Amazon SageMaker Model Registry. For more information, see [Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-models.html) in the _Amazon SageMaker Developer Guide_.\n",
    "\n",
    "Amazon SageMaker integrates Model Cards into Model Registry, making it easier for customers to manage governance information for specific model versions directly in Model Registry in just a few clicks. Customers register ML models in Model Registry to manage their models. Additionally, they can register ML model versions early in the development lifecycle, including essential business details and technical metadata. This integration allows customers to seamlessly review and govern models across their lifecycle from a single place. Customers have greater visibility into the model lifecycle from experimentation and training to evaluation and deployment. This streamlined experience ensures that model governance is consistent and easily accessible throughout the development.\n",
    "\n",
    "In this example, you create a binary classification model along with persisting model card information into registry to manage unified governance information and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2 in /opt/conda/lib/python3.10/site-packages (2.232.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (1.35.23)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (2.2.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (4.25.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (6.0.0)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (1.0.7)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2) (2.2.3)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.23 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2) (1.35.23)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2) (0.10.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2) (3.20.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2) (2.5.1)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2) (13.8.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2) (2024.8.30)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2) (0.70.16)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2) (4.12.2)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/pytz-2023.3.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"sagemaker>=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-core in /opt/conda/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (1.35.23)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (2.5.1)\n",
      "Requirement already satisfied: PyYAML<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (6.0.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (4.23.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (4.3.6)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (13.8.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (4.0.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core) (6.11.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.23 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->sagemaker-core) (1.35.23)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->sagemaker-core) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->sagemaker-core) (0.10.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker-core) (3.20.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0->sagemaker-core) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core) (2.18.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.23->boto3<2.0.0,>=1.34.0->sagemaker-core) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.23->boto3<2.0.0,>=1.34.0->sagemaker-core) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.23->boto3<2.0.0,>=1.34.0->sagemaker-core) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for pytz: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/pytz-2023.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datazone project\n",
    "This is optional step, however encouraged to help with governance. Amazon DataZone is a data management service that makes it faster and easier for you to catalog, discover, share, and govern data stored across AWS, on-premises, and third-party sources. With Amazon DataZone, administrators who oversee organization’s data assets can manage and govern access to data using fine-grained controls. These controls help ensure access with the right level of privileges and context. Amazon DataZone makes it easy for engineers, data scientists, product managers, analysts, and business users to share and access data throughout an organization so they can discover, use, and collaborate to derive data-driven insights.\n",
    "\n",
    "First, [create a new datazone domain](https://us-east-1.console.aws.amazon.com/datazone/home?region=us-east-1#/) if it doesnt already exists.Then, [create a new DataZone Project](https://docs.aws.amazon.com/datazone/latest/userguide/create-new-project.html). Use the project id in the below check and run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture project id from DataZone project\n",
    "project_id = \"5rn1teh0tv85rb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data/Model Quality, Bias, and Model Explainability Checks in SageMaker Pipelines\n",
    "\n",
    "This notebook introduces two new step types in SageMaker Pipelines -\n",
    "* `QualityCheckStep`\n",
    "* `ClarifyCheckStep`\n",
    "\n",
    "With these two steps, the pipeline is able to perform baseline calculations that are needed as a standard against which data/model quality issues can be detected (including bias and explainability).\n",
    "\n",
    "These steps leverage SageMaker pre-built containers:\n",
    "\n",
    "* `QualityCheckStep` (for Data/Model Quality): [sagemaker-model-monitor-analyzer](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-built-container.html)\n",
    "* `ClarifyCheckStep` (for Data/Model Bias and Model Explainability): [sagemaker-clarify-processing](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html#clarify-processing-job-configure-container)\n",
    "\n",
    "The training dataset that you used to train the model is usually a good baseline dataset. The training dataset data schema and the inference dataset schema should exactly match (the number and order of the features). Note that the prediction/output columns are assumed to be the first columns in the training dataset. From the training dataset, you can ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data.\n",
    "\n",
    "These two new steps will always calculate new baselines using the dataset provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Check Baselines in the Model Registry\n",
    "\n",
    "The `RegisterStep` has a new parameter called `drift_check_baselines`. This refers to the baseline files associated with the model. When deployed, these baseline files are used by Model Monitor for Model Quality/Data Quality checks. In addition, these baselines can be used in `QualityCheckStep` and `ClarifyCheckStep` to compare newly trained models against models that have already been registered in the Model Registry.\n",
    "\n",
    "### Step Properties\n",
    "\n",
    "The `QualityCheckStep` has the following properties -\n",
    "\n",
    "* `CalculatedBaselineStatistics` : The baseline statistics file calculated by the underlying Model Monitor container.\n",
    "* `CalculatedBaselineConstraints` : The baseline constraints file calculated by the underlying Model Monitor container.\n",
    "* `BaselineUsedForDriftCheckStatistics` and `BaselineUsedForDriftCheckConstraints` : These are the two properties used to set `drift_check_baseline` in the Model Registry. The values set in these properties vary depending on the parameters passed to the step. The different behaviors are described in the table below.\n",
    "\n",
    "The `ClarifyCheckStep` has the following properties -\n",
    "\n",
    "* `CalculatedBaselineConstraints` : The baseline constraints file calculated by the underlying Clarify container.\n",
    "* `BaselineUsedForDriftCheckConstraints` : This property is used to set `drift_check_baseline` in the Model Registry. The values set in this property will vary depending on the parameters passed to the step. The different behaviors are described in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Overview\n",
    "\n",
    "This notebook should be run with `Python 3.9` using the SageMaker Studio `Python3 (Data Science)` kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the SageMaker Python SDK, boto, and AWS CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.10/site-packages (1.35.23)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.35.25-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.35.23)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.35.25-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: awscli in /opt/conda/lib/python3.10/site-packages (1.34.23)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.34.25-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore) (2.2.3)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (6.0.2)\n",
      "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.4.6)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.1)\n",
      "Downloading botocore-1.35.25-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.35.25-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading awscli-1.34.25-py3-none-any.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing requirements for pytz: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/pytz-2023.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: botocore, boto3, awscli\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.23\n",
      "    Uninstalling botocore-1.35.23:\n",
      "      Successfully uninstalled botocore-1.35.23\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.23\n",
      "    Uninstalling boto3-1.35.23:\n",
      "      Successfully uninstalled boto3-1.35.23\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.34.23\n",
      "    Uninstalling awscli-1.34.23:\n",
      "      Successfully uninstalled awscli-1.34.23\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.35.25 which is incompatible.\n",
      "amazon-sagemaker-jupyter-scheduler 3.0.6 requires pydantic==1.*, but you have pydantic 2.5.1 which is incompatible.\n",
      "autogluon-common 0.8.2 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-common 0.8.2 requires psutil<6,>=5.7.3, but you have psutil 6.0.0 which is incompatible.\n",
      "autogluon-core 0.8.2 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-features 0.8.2 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.2 requires jsonschema<4.18,>=4.14, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.2 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-tabular 0.8.2 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.2 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed awscli-1.34.25 boto3-1.35.25 botocore-1.35.25\n",
      "Requirement already satisfied: sagemaker>=2.99.0 in /opt/conda/lib/python3.10/site-packages (2.232.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (1.35.25)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (2.2.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (4.25.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (1.0.7)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.99.0) (2.2.3)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.25 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2.99.0) (1.35.25)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2.99.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2.99.0) (0.10.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.99.0) (3.20.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2.99.0) (2.5.1)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2.99.0) (13.8.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2.99.0) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.99.0) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.99.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.99.0) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.99.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.99.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.99.0) (2024.8.30)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.99.0) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.99.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.99.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.99.0) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.99.0) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.99.0) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.99.0) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.99.0) (0.70.16)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2.99.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2.99.0) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker>=2.99.0) (4.12.2)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/pytz-2023.3.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install botocore boto3 awscli --upgrade\n",
    "! pip install \"sagemaker>=2.99.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker import utils\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput, CreateModelInput, TransformInput\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics, FileSource\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterBoolean,\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    CreateModelStep,\n",
    "    TransformStep,\n",
    ")\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "# Importing new steps and helper functions\n",
    "\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    DataBiasCheckConfig,\n",
    "    ClarifyCheckStep,\n",
    "    ModelBiasCheckConfig,\n",
    "    ModelPredictedLabelConfig,\n",
    "    ModelExplainabilityCheckConfig,\n",
    "    SHAPConfig,\n",
    ")\n",
    "from sagemaker.workflow.quality_check_step import (\n",
    "    DataQualityCheckConfig,\n",
    "    ModelQualityCheckConfig,\n",
    "    QualityCheckStep,\n",
    ")\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.model_monitor import DatasetFormat, model_monitoring\n",
    "from sagemaker.clarify import BiasConfig, DataConfig, ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = sagemaker.Session().boto_region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto_session, sagemaker_client=sm_client)\n",
    "pipeline_session = PipelineSession()\n",
    "prefix = \"model-monitor-clarify-step-pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables and parameters needed for the Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "base_job_prefix = \"model-monitor-clarify\"\n",
    "model_package_group_name = \"ExamplePackage\"\n",
    "pipeline_name = \"examplepipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline parameters\n",
    "\n",
    "Both `QualityCheckStep` and `ClarifyCheckStep` use two boolean flags `skip_check` and `register_new_baseline` to control their behavior.\n",
    "\n",
    "* `skip_check` : This determines if a drift check is executed or not.\n",
    "* `register_new_baseline` : This determines if the newly calculated baselines (in the step property `CalculatedBaselines`) should be set in the step property `BaselineUsedForDriftCheck`.\n",
    "* `supplied_baseline_statistics` and `supplied_baseline_constraints` : If `skip_check` is set to False, baselines can be provided to this step through this parameter. If provided, the step will compare the newly calculated baselines (`CalculatedBaselines`) against those provided here instead of finding the latest baselines from the Model Registry. In the case of `ClarifyCheckStep`, only `supplied_baseline_constraints` is a valid parameter, for `QualityCheckStep`, both parameters are used.\n",
    "* `model_package_group_name` : The step will use the `drift_check_baselines` from the latest approved model in the model package group for the drift check. If `supplied_baseline_*` is provided, this field will be ignored.\n",
    "\n",
    "The first time the pipeline is run, the `skip_check` value should be set to True using the pipeline execution parameters so that new baselines are registered and no drift check is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Pipeline parameters\n",
    "\n",
    "This table summarizes how the pipeline parameters work when combined.\n",
    "\n",
    "The parameter `drift_check_baselines` is used to supply baselines to the `RegisterStep` that will be used for all drift checks involving the model.\n",
    "\n",
    "Newly calculated baselines can be reference by the properties `CalculatedBaselineStatistics` and `CalculatedBaselineConstraints` on the `QualityCheckStep` and `CalculatedBaselineConstraints` on the `ClarifyCheckStep`.\n",
    "\n",
    "For example, `data_quality_check_step.properties.CalculatedBaselineStatistics` and `data_quality_check_step.properties.CalculatedBaselineConstraints`. This property refers to the baseline that is calculated when the data quality check step is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| `skip_check` / `register_new_baseline` | Does step do a drift check?                              | Value of step property `CalculatedBaseline`                                                           | Value of step property `BaselineUsedForDriftCheck`      | Possible Circumstances for this parameter combination|\n",
    "| -------------------------------------- | ---------------------------------------------------------|------------------------------------------------------------                                           |------------------------------------------------- | -----------------------------------------------------|\n",
    "| F / F                                  | Drift Check executed against existing baselines.         | New baselines calculated by step execution                                                            |  Baseline from latest approved model in Model Registry or baseline supplied as step parameter                                    | Regular re-training with checks enabled to get a new model version, but carry over previous baselines as DriftCheckBaselines in Registry for new model version.                                                                                                                                                             |\n",
    "| F / T                                  | Drift Check executed against existing baselines.         | New baselines calculated by step execution                  | Newly calculated baseline by step execution (value of property `CalculatedBaseline`)                                     | Regular re-training with checks enabled to get a new model version, but refresh DriftCheckBaselines in Registry with newly calculated baselines for the new model version.                                                                                                                                                  |\n",
    "| T / F                                  | No Drift Check.                                          | New baselines calculated by step execution          | Baseline from latest approved model in Model Registry or baseline supplied as step parameter                                     | Violation detected by the model monitor on endpoint for a particular type of check and the pipeline is triggered for retraining a new model. Skip the check against previous baselines, but carry over previous baselines as DriftCheckBaselines in Registry for new model version.             |\n",
    "| T / T                                  | No Drift Check.                                          | New baselines calculated by step execution                  | Newly calculated baseline by step execution (value of property `CalculatedBaseline`)                                     | a. Initial run of the pipeline, building the first model version and generate initial baselines. <br>b. Violation detected by the model monitor on endpoint for a particular type of check and the pipeline is triggered for retraining a new model. Skip the check against previous baselines and refresh DriftCheckBaselines with newly calculated baselines in Registry directly.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "# The dataset used here is the open source Abalone dataset that can be found\n",
    "# here - https://archive.ics.uci.edu/ml/datasets/abalone\n",
    "input_data = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=f\"s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv\",\n",
    ")\n",
    "\n",
    "# for data quality check step\n",
    "skip_check_data_quality = ParameterBoolean(name=\"SkipDataQualityCheck\", default_value=False)\n",
    "register_new_baseline_data_quality = ParameterBoolean(\n",
    "    name=\"RegisterNewDataQualityBaseline\", default_value=False\n",
    ")\n",
    "supplied_baseline_statistics_data_quality = ParameterString(\n",
    "    name=\"DataQualitySuppliedStatistics\", default_value=\"\"\n",
    ")\n",
    "supplied_baseline_constraints_data_quality = ParameterString(\n",
    "    name=\"DataQualitySuppliedConstraints\", default_value=\"\"\n",
    ")\n",
    "\n",
    "# for data bias check step\n",
    "skip_check_data_bias = ParameterBoolean(name=\"SkipDataBiasCheck\", default_value=False)\n",
    "register_new_baseline_data_bias = ParameterBoolean(\n",
    "    name=\"RegisterNewDataBiasBaseline\", default_value=False\n",
    ")\n",
    "supplied_baseline_constraints_data_bias = ParameterString(\n",
    "    name=\"DataBiasSuppliedBaselineConstraints\", default_value=\"\"\n",
    ")\n",
    "\n",
    "# for model quality check step\n",
    "skip_check_model_quality = ParameterBoolean(name=\"SkipModelQualityCheck\", default_value=False)\n",
    "register_new_baseline_model_quality = ParameterBoolean(\n",
    "    name=\"RegisterNewModelQualityBaseline\", default_value=False\n",
    ")\n",
    "supplied_baseline_statistics_model_quality = ParameterString(\n",
    "    name=\"ModelQualitySuppliedStatistics\", default_value=\"\"\n",
    ")\n",
    "supplied_baseline_constraints_model_quality = ParameterString(\n",
    "    name=\"ModelQualitySuppliedConstraints\", default_value=\"\"\n",
    ")\n",
    "\n",
    "# for model bias check step\n",
    "skip_check_model_bias = ParameterBoolean(name=\"SkipModelBiasCheck\", default_value=False)\n",
    "register_new_baseline_model_bias = ParameterBoolean(\n",
    "    name=\"RegisterNewModelBiasBaseline\", default_value=False\n",
    ")\n",
    "supplied_baseline_constraints_model_bias = ParameterString(\n",
    "    name=\"ModelBiasSuppliedBaselineConstraints\", default_value=\"\"\n",
    ")\n",
    "\n",
    "# for model explainability check step\n",
    "skip_check_model_explainability = ParameterBoolean(\n",
    "    name=\"SkipModelExplainabilityCheck\", default_value=False\n",
    ")\n",
    "register_new_baseline_model_explainability = ParameterBoolean(\n",
    "    name=\"RegisterNewModelExplainabilityBaseline\", default_value=False\n",
    ")\n",
    "supplied_baseline_constraints_model_explainability = ParameterString(\n",
    "    name=\"ModelExplainabilitySuppliedBaselineConstraints\", default_value=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing step for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocess.py\n",
    "\n",
    "\"\"\"Feature engineers the abalone dataset.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "# Since we get a headerless CSV file we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Merges two dicts, returning a new copy.\"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.debug(\"Starting preprocessing.\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\n",
    "    input_data = args.input_data\n",
    "    bucket = input_data.split(\"/\")[2]\n",
    "    key = \"/\".join(input_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from bucket: %s, key: %s\", bucket, key)\n",
    "    fn = f\"{base_dir}/data/abalone-dataset.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, fn)\n",
    "\n",
    "    logger.debug(\"Reading downloaded data.\")\n",
    "    df = pd.read_csv(\n",
    "        fn,\n",
    "        header=None,\n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "    )\n",
    "    os.unlink(fn)\n",
    "\n",
    "    logger.debug(\"Defining transformers.\")\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger.info(\"Applying transforms.\")\n",
    "    y = df.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    logger.info(\"Splitting %d rows of data into train, validation, test datasets.\", len(X))\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    logger.info(\"Writing out datasets to %s.\", base_dir)\n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        f\"{base_dir}/validation/validation.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=f\"{base_job_prefix}/sklearn-abalone-preprocess\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "processor_args = sklearn_processor.run(\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocess.py\",\n",
    "    arguments=[\"--input-data\", input_data],\n",
    ")\n",
    "step_process = ProcessingStep(name=\"PreprocessAbaloneData\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Data Quality\n",
    "\n",
    "`CheckJobConfig` is a helper function that's used to define the job configurations used by the `QualityCheckStep`. By separating the job configuration from the step parameters, the same `CheckJobConfig` can be used across multiple steps for quality checks.\n",
    "\n",
    "The `DataQualityCheckConfig` is used to define the Quality Check job by specifying the dataset used to calculate the baseline, in this case, the training dataset from the data processing step, the dataset format, in this case, a csv file with no headers, and the output path for the results of the data quality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "check_job_config = CheckJobConfig(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    volume_size_in_gb=120,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "data_quality_check_config = DataQualityCheckConfig(\n",
    "    baseline_dataset=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False, output_columns_position=\"START\"),\n",
    "    output_s3_uri=Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            \"s3:/\",\n",
    "            default_bucket,\n",
    "            base_job_prefix,\n",
    "            ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "            \"dataqualitycheckstep\",\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "data_quality_check_step = QualityCheckStep(\n",
    "    name=\"DataQualityCheckStep\",\n",
    "    skip_check=skip_check_data_quality,\n",
    "    register_new_baseline=register_new_baseline_data_quality,\n",
    "    quality_check_config=data_quality_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    supplied_baseline_statistics=supplied_baseline_statistics_data_quality,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_data_quality,\n",
    "    model_package_group_name=model_package_group_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Data Bias\n",
    "\n",
    "The job configuration from the previous step is used here and the `DataConfig` class is used to define how the `ClarifyCheckStep` should compute the data bias. The training dataset is used again for the bias evaluation, the column representing the label is specified through the `label` parameter, and a `BiasConfig` is provided.\n",
    "\n",
    "In the `BiasConfig`, we specify a facet name (the column that is the focal point of the bias calculation), the value of the facet that determines the range of values it can hold, and the threshold value for the label.\n",
    "\n",
    "More details on `BiasConfig` can be found [here](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.clarify.BiasConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.clarify_model_monitoring:Uploading analysis config to {s3_uri}.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "data_bias_analysis_cfg_output_path = (\n",
    "    f\"s3://{default_bucket}/{base_job_prefix}/databiascheckstep/analysis_cfg\"\n",
    ")\n",
    "\n",
    "data_bias_data_config = DataConfig(\n",
    "    s3_data_input_path=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "        \"train\"\n",
    "    ].S3Output.S3Uri,\n",
    "    s3_output_path=Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            \"s3:/\",\n",
    "            default_bucket,\n",
    "            base_job_prefix,\n",
    "            ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "            \"databiascheckstep\",\n",
    "        ],\n",
    "    ),\n",
    "    label=0,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=data_bias_analysis_cfg_output_path,\n",
    ")\n",
    "\n",
    "\n",
    "data_bias_config = BiasConfig(\n",
    "    label_values_or_threshold=[15.0], facet_name=[8], facet_values_or_threshold=[[0.5]]\n",
    ")\n",
    "\n",
    "data_bias_check_config = DataBiasCheckConfig(\n",
    "    data_config=data_bias_data_config,\n",
    "    data_bias_config=data_bias_config,\n",
    ")\n",
    "\n",
    "data_bias_check_step = ClarifyCheckStep(\n",
    "    name=\"DataBiasCheckStep\",\n",
    "    clarify_check_config=data_bias_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_data_bias,\n",
    "    register_new_baseline=register_new_baseline_data_bias,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_data_bias,\n",
    "    model_package_group_name=model_package_group_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_path = f\"s3://{sagemaker_session.default_bucket()}/{base_job_prefix}/AbaloneTrain\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"{base_job_prefix}/abalone-train\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    ")\n",
    "\n",
    "train_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainAbaloneModel\",\n",
    "    step_args=train_args,\n",
    "    depends_on=[data_bias_check_step.name, data_quality_check_step.name],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "The model is created so that a batch transform job can be used to get predictions from the model on a test dataset. These predictions are used when calculating model quality, model bias, and model explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"AbaloneCreateModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Output\n",
    "\n",
    "The output of the transform step combines the prediction and the input label. The output format is <br>\n",
    "`prediction, original label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=f\"s3://{default_bucket}/AbaloneTransform\",\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AbaloneTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(\n",
    "        data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "        input_filter=\"$[1:]\",\n",
    "        join_source=\"Input\",\n",
    "        output_filter=\"$[0,-1]\",\n",
    "        content_type=\"text/csv\",\n",
    "        split_type=\"Line\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Model Quality\n",
    "\n",
    "In this `QualityCheckStep` we calculate the baselines for statistics and constraints using the predictions that the model generates from the test dataset (output from the TransformStep). We define the problem type as 'Regression' in the `ModelQualityCheckConfig` along with specifying the columns which represent the input and output. Since the dataset has no headers, `_c0`, `_c1` are auto-generated header names that should be used in the `ModelQualityCheckConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "model_quality_check_config = ModelQualityCheckConfig(\n",
    "    baseline_dataset=step_transform.properties.TransformOutput.S3OutputPath,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            \"s3:/\",\n",
    "            default_bucket,\n",
    "            base_job_prefix,\n",
    "            ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "            \"modelqualitycheckstep\",\n",
    "        ],\n",
    "    ),\n",
    "    problem_type=\"Regression\",\n",
    "    inference_attribute=\"_c0\",  # use auto-populated headers since we don't have headers in the dataset\n",
    "    ground_truth_attribute=\"_c1\",  # use auto-populated headers since we don't have headers in the dataset\n",
    ")\n",
    "\n",
    "model_quality_check_step = QualityCheckStep(\n",
    "    name=\"ModelQualityCheckStep\",\n",
    "    skip_check=skip_check_model_quality,\n",
    "    register_new_baseline=register_new_baseline_model_quality,\n",
    "    quality_check_config=model_quality_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    supplied_baseline_statistics=supplied_baseline_statistics_model_quality,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_quality,\n",
    "    model_package_group_name=model_package_group_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Model Bias\n",
    "\n",
    "Similar to the Data Bias check step, a `BiasConfig` is defined and Clarify is used to calculate the model bias using the training dataset and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.clarify_model_monitoring:Uploading analysis config to {s3_uri}.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "model_bias_analysis_cfg_output_path = (\n",
    "    f\"s3://{default_bucket}/{base_job_prefix}/modelbiascheckstep/analysis_cfg\"\n",
    ")\n",
    "\n",
    "model_bias_data_config = DataConfig(\n",
    "    s3_data_input_path=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "        \"train\"\n",
    "    ].S3Output.S3Uri,\n",
    "    s3_output_path=Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            \"s3:/\",\n",
    "            default_bucket,\n",
    "            base_job_prefix,\n",
    "            ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "            \"modelbiascheckstep\",\n",
    "        ],\n",
    "    ),\n",
    "    s3_analysis_config_output_path=model_bias_analysis_cfg_output_path,\n",
    "    label=0,\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "# We are using this bias config to configure Clarify to detect bias based on the first feature in the featurized vector for Sex\n",
    "model_bias_config = BiasConfig(\n",
    "    label_values_or_threshold=[15.0], facet_name=[8], facet_values_or_threshold=[[0.5]]\n",
    ")\n",
    "\n",
    "model_bias_check_config = ModelBiasCheckConfig(\n",
    "    data_config=model_bias_data_config,\n",
    "    data_bias_config=model_bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=ModelPredictedLabelConfig(),\n",
    ")\n",
    "\n",
    "model_bias_check_step = ClarifyCheckStep(\n",
    "    name=\"ModelBiasCheckStep\",\n",
    "    clarify_check_config=model_bias_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_bias,\n",
    "    register_new_baseline=register_new_baseline_model_bias,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_bias,\n",
    "    model_package_group_name=model_package_group_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Explainability\n",
    "\n",
    "SageMaker Clarify uses a model-agnostic feature attribution approach, which you can use to understand why a model made a prediction after training and to provide per-instance explanation during inference. The implementation includes a scalable and efficient implementation of SHAP, based on the concept of a Shapley value from the field of cooperative game theory that assigns each feature an importance value for a particular prediction.\n",
    "\n",
    "For Model Explainability, Clarify requires an explainability configuration to be provided. In this example, we use `SHAPConfig`. For more information of `explainability_config`, visit the [Clarify documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.clarify_model_monitoring:Uploading analysis config to {s3_uri}.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "model_explainability_analysis_cfg_output_path = \"s3://{}/{}/{}/{}\".format(\n",
    "    default_bucket, base_job_prefix, \"modelexplainabilitycheckstep\", \"analysis_cfg\"\n",
    ")\n",
    "\n",
    "model_explainability_data_config = DataConfig(\n",
    "    s3_data_input_path=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "        \"train\"\n",
    "    ].S3Output.S3Uri,\n",
    "    s3_output_path=Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            \"s3:/\",\n",
    "            default_bucket,\n",
    "            base_job_prefix,\n",
    "            ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "            \"modelexplainabilitycheckstep\",\n",
    "        ],\n",
    "    ),\n",
    "    s3_analysis_config_output_path=model_explainability_analysis_cfg_output_path,\n",
    "    label=0,\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "shap_config = SHAPConfig(seed=123, num_samples=10)\n",
    "model_explainability_check_config = ModelExplainabilityCheckConfig(\n",
    "    data_config=model_explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")\n",
    "model_explainability_check_step = ClarifyCheckStep(\n",
    "    name=\"ModelExplainabilityCheckStep\",\n",
    "    clarify_check_config=model_explainability_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_explainability,\n",
    "    register_new_baseline=register_new_baseline_model_explainability,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_explainability,\n",
    "    model_package_group_name=model_package_group_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of the model\n",
    "\n",
    "Using a processing job, evaluate the performance of the model. The performance is used in the Condition Step to determine if the model should be registered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluate.py\n",
    "\n",
    "\"\"\"Evaluation script for measuring mean squared error.\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.debug(\"Starting evaluation.\")\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    logger.debug(\"Calculating mean squared error.\")\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Writing out evaluation report with mse: %f\", mse)\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_prefix}/script-abalone-eval\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"AbaloneEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluate.py\",\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EvaluateAbaloneModel\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the metrics to be registered with the model in the Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_data_statistics=MetricsSource(\n",
    "        s3_uri=data_quality_check_step.properties.CalculatedBaselineStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_data_constraints=MetricsSource(\n",
    "        s3_uri=data_quality_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    bias_pre_training=MetricsSource(\n",
    "        s3_uri=data_bias_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=model_quality_check_step.properties.CalculatedBaselineStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_constraints=MetricsSource(\n",
    "        s3_uri=model_quality_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    bias_post_training=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability=MetricsSource(\n",
    "        s3_uri=model_explainability_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "drift_check_baselines = DriftCheckBaselines(\n",
    "    model_data_statistics=MetricsSource(\n",
    "        s3_uri=data_quality_check_step.properties.BaselineUsedForDriftCheckStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_data_constraints=MetricsSource(\n",
    "        s3_uri=data_quality_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    bias_pre_training_constraints=MetricsSource(\n",
    "        s3_uri=data_bias_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    bias_config_file=FileSource(\n",
    "        s3_uri=model_bias_check_config.monitoring_analysis_config_uri,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=model_quality_check_step.properties.BaselineUsedForDriftCheckStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_constraints=MetricsSource(\n",
    "        s3_uri=model_quality_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    bias_post_training_constraints=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_constraints=MetricsSource(\n",
    "        s3_uri=model_explainability_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_config_file=FileSource(\n",
    "        s3_uri=model_explainability_check_config.monitoring_analysis_config_uri,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "The two parameters in `RegisterModel` that hold the metrics calculated by the `ClarifyCheckStep` and `QualityCheckStep` are `model_metrics` and `drift_check_baselines`.\n",
    "\n",
    "`drift_check_baselines` - these are the baseline files that will be used for drift checks in `QualityCheckStep` or `ClarifyCheckStep` and model monitoring jobs that are set up on endpoints hosting this model.\n",
    "\n",
    "`model_metrics` - these should be the latest baselines calculated in the pipeline run. This can be set using the step property `CalculatedBaseline`\n",
    "\n",
    "The intention behind these parameters is to give users a way to configure the baselines associated with a model so they can be used in drift checks or model monitoring jobs. Each time a pipeline is executed, users can choose to update the `drift_check_baselines` with newly calculated baselines. The `model_metrics` can be used to register the newly calculated baselines or any other metrics associated with the model.\n",
    "\n",
    "Every time a baseline is calculated, it is not necessary that the baselines used for drift checks are updated to the newly calculated baselines. In some cases, users may retain an older version of the baseline file to be used for drift checks and not register new baselines that are calculated in the Pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves.urllib.parse import urlparse\n",
    "from pprint import pprint\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.model_card import (\n",
    "    ModelCard,\n",
    "    ModelOverview,\n",
    "    IntendedUses,\n",
    "    BusinessDetails,\n",
    "    AdditionalInformation,\n",
    "    ModelCardStatusEnum,\n",
    "    ObjectiveFunctionEnum,\n",
    "    RiskRatingEnum,\n",
    "    MetricTypeEnum,\n",
    "    TrainingDetails\n",
    ")\n",
    "from sagemaker.model import ModelPackage as MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_overview = ModelOverview(\n",
    "    #model_description=\"This is an example model used for a Python SDK demo of unified Amazon SageMaker Model Registry and Model Cards.\",\n",
    "    #problem_type=\"Binary Classification\",\n",
    "    #algorithm_type=\"Logistic Regression\",\n",
    "    model_creator=\"DEMO-Model-Registry-ModelCard-Unification\",\n",
    "    #model_owner=\"datascienceteam\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "intended_uses = IntendedUses(\n",
    "    purpose_of_model=\"Test model card.\",\n",
    "    intended_uses=\"Not used except this test.\",\n",
    "    factors_affecting_model_efficiency=\"No.\",\n",
    "    risk_rating=RiskRatingEnum.LOW,\n",
    "    explanations_for_risk_rating=\"Just an example.\",\n",
    ")\n",
    "business_details = BusinessDetails(\n",
    "    business_problem=\"The business problem that your model is used to solve.\",\n",
    "    business_stakeholders=\"The stakeholders who have the interest in the business that your model is used for.\",\n",
    "    line_of_business=\"Services that the business is offering.\",\n",
    ")\n",
    "additional_information = AdditionalInformation(\n",
    "    ethical_considerations=\"Your model ethical consideration.\",\n",
    "    caveats_and_recommendations=\"Your model's caveats and recommendations.\",\n",
    "    custom_details={\"Datazone projects\": project_id},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_card = ModelCard(\n",
    "    name=\"MyModelCard\",\n",
    "    status=ModelCardStatusEnum.DRAFT,\n",
    "    model_overview=model_overview,\n",
    "    intended_uses=intended_uses,\n",
    "    business_details=business_details,\n",
    "    additional_information=additional_information,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    "    drift_check_baselines=drift_check_baselines,\n",
    "    model_card=my_card\n",
    ")\n",
    "\n",
    "step_register = ModelStep(name=\"RegisterAbaloneModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# condition step for evaluating model quality and branching execution\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\",\n",
    "    ),\n",
    "    right=6.0,\n",
    ")\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckMSEAbaloneEvaluation\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline instance\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        skip_check_data_quality,\n",
    "        register_new_baseline_data_quality,\n",
    "        supplied_baseline_statistics_data_quality,\n",
    "        supplied_baseline_constraints_data_quality,\n",
    "        skip_check_data_bias,\n",
    "        register_new_baseline_data_bias,\n",
    "        supplied_baseline_constraints_data_bias,\n",
    "        skip_check_model_quality,\n",
    "        register_new_baseline_model_quality,\n",
    "        supplied_baseline_statistics_model_quality,\n",
    "        supplied_baseline_constraints_model_quality,\n",
    "        skip_check_model_bias,\n",
    "        register_new_baseline_model_bias,\n",
    "        supplied_baseline_constraints_model_bias,\n",
    "        skip_check_model_explainability,\n",
    "        register_new_baseline_model_explainability,\n",
    "        supplied_baseline_constraints_model_explainability,\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        data_quality_check_step,\n",
    "        data_bias_check_step,\n",
    "        step_train,\n",
    "        step_create_model,\n",
    "        step_transform,\n",
    "        model_quality_check_step,\n",
    "        model_bias_check_step,\n",
    "        model_explainability_check_step,\n",
    "        step_eval,\n",
    "        step_cond,\n",
    "    ],\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputDataUrl',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv'},\n",
       "  {'Name': 'SkipDataQualityCheck', 'Type': 'Boolean', 'DefaultValue': False},\n",
       "  {'Name': 'RegisterNewDataQualityBaseline',\n",
       "   'Type': 'Boolean',\n",
       "   'DefaultValue': False},\n",
       "  {'Name': 'DataQualitySuppliedStatistics',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''},\n",
       "  {'Name': 'DataQualitySuppliedConstraints',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''},\n",
       "  {'Name': 'SkipDataBiasCheck', 'Type': 'Boolean', 'DefaultValue': False},\n",
       "  {'Name': 'RegisterNewDataBiasBaseline',\n",
       "   'Type': 'Boolean',\n",
       "   'DefaultValue': False},\n",
       "  {'Name': 'DataBiasSuppliedBaselineConstraints',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''},\n",
       "  {'Name': 'SkipModelQualityCheck', 'Type': 'Boolean', 'DefaultValue': False},\n",
       "  {'Name': 'RegisterNewModelQualityBaseline',\n",
       "   'Type': 'Boolean',\n",
       "   'DefaultValue': False},\n",
       "  {'Name': 'ModelQualitySuppliedStatistics',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''},\n",
       "  {'Name': 'ModelQualitySuppliedConstraints',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''},\n",
       "  {'Name': 'SkipModelBiasCheck', 'Type': 'Boolean', 'DefaultValue': False},\n",
       "  {'Name': 'RegisterNewModelBiasBaseline',\n",
       "   'Type': 'Boolean',\n",
       "   'DefaultValue': False},\n",
       "  {'Name': 'ModelBiasSuppliedBaselineConstraints',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''},\n",
       "  {'Name': 'SkipModelExplainabilityCheck',\n",
       "   'Type': 'Boolean',\n",
       "   'DefaultValue': False},\n",
       "  {'Name': 'RegisterNewModelExplainabilityBaseline',\n",
       "   'Type': 'Boolean',\n",
       "   'DefaultValue': False},\n",
       "  {'Name': 'ModelExplainabilitySuppliedBaselineConstraints',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': ''}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'PreprocessAbaloneData',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--input-data',\n",
       "      {'Get': 'Parameters.InputDataUrl'}],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocess.py']},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/examplepipeline/code/2d6df10ec89b5513bb118b8c4af0c648/preprocess.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'examplepipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAbaloneData',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'examplepipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAbaloneData',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'examplepipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAbaloneData',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'DataQualityCheckStep',\n",
       "   'Type': 'QualityCheck',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 120}},\n",
       "    'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'baseline_dataset_input',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'quality_check_output',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'model-monitor-clarify',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'dataqualitycheckstep']}},\n",
       "        'LocalPath': '/opt/ml/processing/output',\n",
       "        'S3UploadMode': 'EndOfJob'}}]},\n",
       "    'Environment': {'output_path': '/opt/ml/processing/output',\n",
       "     'publish_cloudwatch_metrics': 'Disabled',\n",
       "     'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}',\n",
       "     'dataset_source': '/opt/ml/processing/input/baseline_dataset_input'}},\n",
       "   'CheckType': 'DATA_QUALITY',\n",
       "   'ModelPackageGroupName': 'ExamplePackage',\n",
       "   'SkipCheck': {'Get': 'Parameters.SkipDataQualityCheck'},\n",
       "   'FailOnViolation': True,\n",
       "   'RegisterNewBaseline': {'Get': 'Parameters.RegisterNewDataQualityBaseline'},\n",
       "   'SuppliedBaselineStatistics': {'Get': 'Parameters.DataQualitySuppliedStatistics'},\n",
       "   'SuppliedBaselineConstraints': {'Get': 'Parameters.DataQualitySuppliedConstraints'}},\n",
       "  {'Name': 'DataBiasCheckStep',\n",
       "   'Type': 'ClarifyCheck',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 120}},\n",
       "    'AppSpecification': {'ImageUri': '205585389593.dkr.ecr.us-east-1.amazonaws.com/sagemaker-clarify-processing:1.0'},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'analysis_config',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/databiascheckstep/analysis_cfg/analysis_config.json',\n",
       "       'LocalPath': '/opt/ml/processing/input/config',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'dataset',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/data',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'analysis_result',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'model-monitor-clarify',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'databiascheckstep']}},\n",
       "        'LocalPath': '/opt/ml/processing/output',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CheckType': 'DATA_BIAS',\n",
       "   'ModelPackageGroupName': 'ExamplePackage',\n",
       "   'SkipCheck': {'Get': 'Parameters.SkipDataBiasCheck'},\n",
       "   'FailOnViolation': True,\n",
       "   'RegisterNewBaseline': {'Get': 'Parameters.RegisterNewDataBiasBaseline'},\n",
       "   'SuppliedBaselineConstraints': {'Get': 'Parameters.DataBiasSuppliedBaselineConstraints'}},\n",
       "  {'Name': 'TrainAbaloneModel',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/AbaloneTrain'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'}},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'validation'}],\n",
       "    'HyperParameters': {'objective': 'reg:linear',\n",
       "     'num_round': '50',\n",
       "     'max_depth': '5',\n",
       "     'eta': '0.2',\n",
       "     'gamma': '4',\n",
       "     'min_child_weight': '6',\n",
       "     'subsample': '0.7',\n",
       "     'silent': '0'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/AbaloneTrain',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/AbaloneTrain',\n",
       "     'DisableProfiler': False}},\n",
       "   'DependsOn': ['DataBiasCheckStep', 'DataQualityCheckStep']},\n",
       "  {'Name': 'AbaloneCreateModel-CreateModel',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'PrimaryContainer': {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "     'Environment': {},\n",
       "     'ModelDataUrl': {'Get': 'Steps.TrainAbaloneModel.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "  {'Name': 'AbaloneTransform',\n",
       "   'Type': 'Transform',\n",
       "   'Arguments': {'ModelName': {'Get': 'Steps.AbaloneCreateModel-CreateModel.ModelName'},\n",
       "    'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "       'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"}}},\n",
       "     'ContentType': 'text/csv',\n",
       "     'SplitType': 'Line'},\n",
       "    'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-east-1-340280328827/AbaloneTransform',\n",
       "     'AssembleWith': 'Line',\n",
       "     'Accept': 'text/csv'},\n",
       "    'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge'},\n",
       "    'DataProcessing': {'InputFilter': '$[1:]',\n",
       "     'OutputFilter': '$[0,-1]',\n",
       "     'JoinSource': 'Input'}}},\n",
       "  {'Name': 'ModelQualityCheckStep',\n",
       "   'Type': 'QualityCheck',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 120}},\n",
       "    'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'baseline_dataset_input',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.AbaloneTransform.TransformOutput.S3OutputPath'},\n",
       "       'LocalPath': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'quality_check_output',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'model-monitor-clarify',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'modelqualitycheckstep']}},\n",
       "        'LocalPath': '/opt/ml/processing/output',\n",
       "        'S3UploadMode': 'EndOfJob'}}]},\n",
       "    'Environment': {'output_path': '/opt/ml/processing/output',\n",
       "     'publish_cloudwatch_metrics': 'Disabled',\n",
       "     'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}',\n",
       "     'dataset_source': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "     'analysis_type': 'MODEL_QUALITY',\n",
       "     'problem_type': 'Regression',\n",
       "     'inference_attribute': '_c0',\n",
       "     'ground_truth_attribute': '_c1'}},\n",
       "   'CheckType': 'MODEL_QUALITY',\n",
       "   'ModelPackageGroupName': 'ExamplePackage',\n",
       "   'SkipCheck': {'Get': 'Parameters.SkipModelQualityCheck'},\n",
       "   'FailOnViolation': True,\n",
       "   'RegisterNewBaseline': {'Get': 'Parameters.RegisterNewModelQualityBaseline'},\n",
       "   'SuppliedBaselineStatistics': {'Get': 'Parameters.ModelQualitySuppliedStatistics'},\n",
       "   'SuppliedBaselineConstraints': {'Get': 'Parameters.ModelQualitySuppliedConstraints'}},\n",
       "  {'Name': 'ModelBiasCheckStep',\n",
       "   'Type': 'ClarifyCheck',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 120}},\n",
       "    'AppSpecification': {'ImageUri': '205585389593.dkr.ecr.us-east-1.amazonaws.com/sagemaker-clarify-processing:1.0'},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'analysis_config',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/modelbiascheckstep/analysis_cfg/analysis_config.json',\n",
       "       'LocalPath': '/opt/ml/processing/input/config',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'dataset',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/data',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'analysis_result',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'model-monitor-clarify',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'modelbiascheckstep']}},\n",
       "        'LocalPath': '/opt/ml/processing/output',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CheckType': 'MODEL_BIAS',\n",
       "   'ModelPackageGroupName': 'ExamplePackage',\n",
       "   'SkipCheck': {'Get': 'Parameters.SkipModelBiasCheck'},\n",
       "   'FailOnViolation': True,\n",
       "   'RegisterNewBaseline': {'Get': 'Parameters.RegisterNewModelBiasBaseline'},\n",
       "   'SuppliedBaselineConstraints': {'Get': 'Parameters.ModelBiasSuppliedBaselineConstraints'},\n",
       "   'ModelName': {'Get': 'Steps.AbaloneCreateModel-CreateModel.ModelName'}},\n",
       "  {'Name': 'ModelExplainabilityCheckStep',\n",
       "   'Type': 'ClarifyCheck',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 120}},\n",
       "    'AppSpecification': {'ImageUri': '205585389593.dkr.ecr.us-east-1.amazonaws.com/sagemaker-clarify-processing:1.0'},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'analysis_config',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/modelexplainabilitycheckstep/analysis_cfg/analysis_config.json',\n",
       "       'LocalPath': '/opt/ml/processing/input/config',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'dataset',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/data',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'analysis_result',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'model-monitor-clarify',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'modelexplainabilitycheckstep']}},\n",
       "        'LocalPath': '/opt/ml/processing/output',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CheckType': 'MODEL_EXPLAINABILITY',\n",
       "   'ModelPackageGroupName': 'ExamplePackage',\n",
       "   'SkipCheck': {'Get': 'Parameters.SkipModelExplainabilityCheck'},\n",
       "   'FailOnViolation': True,\n",
       "   'RegisterNewBaseline': {'Get': 'Parameters.RegisterNewModelExplainabilityBaseline'},\n",
       "   'SuppliedBaselineConstraints': {'Get': 'Parameters.ModelExplainabilitySuppliedBaselineConstraints'},\n",
       "   'ModelName': {'Get': 'Steps.AbaloneCreateModel-CreateModel.ModelName'}},\n",
       "  {'Name': 'EvaluateAbaloneModel',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluate.py']},\n",
       "    'RoleArn': 'arn:aws:iam::340280328827:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1TSLA2QB1DJKS',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.TrainAbaloneModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.PreprocessAbaloneData.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/examplepipeline/code/3384adc475a3da31615b9ff745f950e3/evaluate.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-340280328827',\n",
       "           'examplepipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'EvaluateAbaloneModel',\n",
       "           'output',\n",
       "           'evaluation']}},\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'AbaloneEvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'CheckMSEAbaloneEvaluation',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.EvaluateAbaloneModel.PropertyFiles.AbaloneEvaluationReport'},\n",
       "        'Path': 'regression_metrics.mse.value'}},\n",
       "      'RightValue': 6.0}],\n",
       "    'IfSteps': [{'Name': 'RegisterAbaloneModel-RegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'ExamplePackage',\n",
       "       'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelQualityCheckStep.CalculatedBaselineStatistics'}},\n",
       "         'Constraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelQualityCheckStep.CalculatedBaselineConstraints'}}},\n",
       "        'ModelDataQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.DataQualityCheckStep.CalculatedBaselineStatistics'}},\n",
       "         'Constraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.DataQualityCheckStep.CalculatedBaselineConstraints'}}},\n",
       "        'Bias': {'PreTrainingReport': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.DataBiasCheckStep.CalculatedBaselineConstraints'}},\n",
       "         'PostTrainingReport': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelBiasCheckStep.CalculatedBaselineConstraints'}}},\n",
       "        'Explainability': {'Report': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelExplainabilityCheckStep.CalculatedBaselineConstraints'}}}},\n",
       "       'DriftCheckBaselines': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelQualityCheckStep.BaselineUsedForDriftCheckStatistics'}},\n",
       "         'Constraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelQualityCheckStep.BaselineUsedForDriftCheckConstraints'}}},\n",
       "        'ModelDataQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.DataQualityCheckStep.BaselineUsedForDriftCheckStatistics'}},\n",
       "         'Constraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.DataQualityCheckStep.BaselineUsedForDriftCheckConstraints'}}},\n",
       "        'Bias': {'ConfigFile': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/modelbiascheckstep/analysis_cfg/bias-monitoring-configuration/bias-monitoring-config-2024-09-23-21-05-03-909/c568d056-6fa3-4a63-acbf-2781cfc96700/analysis_config.json',\n",
       "          'ContentType': 'application/json'},\n",
       "         'PreTrainingConstraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.DataBiasCheckStep.BaselineUsedForDriftCheckConstraints'}},\n",
       "         'PostTrainingConstraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelBiasCheckStep.BaselineUsedForDriftCheckConstraints'}}},\n",
       "        'Explainability': {'Constraints': {'ContentType': 'application/json',\n",
       "          'S3Uri': {'Get': 'Steps.ModelExplainabilityCheckStep.BaselineUsedForDriftCheckConstraints'}},\n",
       "         'ConfigFile': {'S3Uri': 's3://sagemaker-us-east-1-340280328827/model-monitor-clarify/modelexplainabilitycheckstep/analysis_cfg/model-explainability-monitoring-configuration/model-explainability-monitoring-config-2024-09-23-21-05-04-268/c66b6d48-6718-4d38-a2ce-9eb904a92442/analysis_config.json',\n",
       "          'ContentType': 'application/json'}}},\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "          'Environment': {},\n",
       "          'ModelDataUrl': {'Get': 'Steps.TrainAbaloneModel.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "        'SupportedContentTypes': ['text/csv'],\n",
       "        'SupportedResponseMIMETypes': ['text/csv'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium',\n",
       "         'ml.m5.large'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.large']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
       "       'SkipModelValidation': 'None',\n",
       "       'ModelCard': {'ModelCardStatus': 'Draft',\n",
       "        'ModelCardContent': '{\"model_overview\": {\"model_creator\": \"DEMO-Model-Registry-ModelCard-Unification\", \"model_artifact\": []}, \"intended_uses\": {\"purpose_of_model\": \"Test model card.\", \"intended_uses\": \"Not used except this test.\", \"factors_affecting_model_efficiency\": \"No.\", \"risk_rating\": \"Low\", \"explanations_for_risk_rating\": \"Just an example.\"}, \"business_details\": {\"business_problem\": \"The business problem that your model is used to solve.\", \"business_stakeholders\": \"The stakeholders who have the interest in the business that your model is used for.\", \"line_of_business\": \"Services that the business is offering.\"}, \"additional_information\": {\"ethical_considerations\": \"Your model ethical consideration.\", \"caveats_and_recommendations\": \"Your model\\'s caveats and recommendations.\", \"custom_details\": {\"Datazone projects\": \"5rn1teh0tv85rb\"}}}'}}}],\n",
       "    'ElseSteps': []}}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:340280328827:pipeline/ExamplePipeline',\n",
       " 'ResponseMetadata': {'RequestId': '68d144f7-c4d3-45e1-8d5c-b2c44ca535e5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '68d144f7-c4d3-45e1-8d5c-b2c44ca535e5',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '83',\n",
       "   'date': 'Mon, 23 Sep 2024 21:05:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First time executing\n",
    "\n",
    "The first time the pipeline is run the parameters need to be overridden so that the checks are skipped and newly calculated baselines are registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        SkipDataQualityCheck=True,\n",
    "        RegisterNewDataQualityBaseline=True,\n",
    "        SkipDataBiasCheck=True,\n",
    "        RegisterNewDataBiasBaseline=True,\n",
    "        SkipModelQualityCheck=True,\n",
    "        RegisterNewModelQualityBaseline=True,\n",
    "        SkipModelBiasCheck=True,\n",
    "        RegisterNewModelBiasBaseline=True,\n",
    "        SkipModelExplainabilityCheck=True,\n",
    "        RegisterNewModelExplainabilityBaseline=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the pipeline execution to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up resources\n",
    "\n",
    "Users are responsible for cleaning up resources created when running this notebook. Specify the ModelName, ModelPackageName, and ModelPackageGroupName that need to be deleted. The model names are generated by the CreateModel step of the Pipeline and the property values are available only in the Pipeline context. To delete the models created by this pipeline, navigate to the Model Registry and Console to find the models to delete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a SageMaker client\n",
    "# sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# # Delete SageMaker Models\n",
    "# sm_client.delete_model(ModelName=\"...\")\n",
    "\n",
    "# # Delete Model Packages\n",
    "# sm_client.delete_model_package(ModelPackageName=\"...\")\n",
    "\n",
    "# # Delete the Model Package Group\n",
    "# sm_client.delete_model_package_group(ModelPackageGroupName=\"model-monitor-clarify-group\")\n",
    "\n",
    "# # Delete the Pipeline\n",
    "# sm_client.delete_pipeline(PipelineName=\"model-monitor-clarify-pipeline\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
