{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa619cfc",
   "metadata": {},
   "source": [
    "# Compile and Train the GPT2 Model using the Transformers Trainer API with the SST2 Dataset for Single-Node Multi-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f479baf",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Permissions](#Permissions)\n",
    "3. [SageMaker Training Job](#SageMaker-Training-Job)  \n",
    "    1. [Training with Native PyTorch](#Training-with-Native-PyTorch)  \n",
    "    2. [Training with Optimized PyTorch](#Training-with-Optimized-PyTorch)  \n",
    "    3. [Analysis](#Analysis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e922ff",
   "metadata": {},
   "source": [
    "## SageMaker Training Compiler Overview\n",
    "\n",
    "SageMaker Training Compiler is a capability of SageMaker that makes these hard-to-implement optimizations to reduce training time on GPU instances. The compiler optimizes DL models to accelerate training by more efficiently using SageMaker machine learning (ML) GPU instances. SageMaker Training Compiler is available at no additional charge within SageMaker and can help reduce total billable time as it accelerates training. \n",
    "\n",
    "SageMaker Training Compiler is integrated into the AWS Deep Learning Containers (DLCs). Using the SageMaker Training Compiler enabled AWS DLCs, you can compile and optimize training jobs on GPU instances with minimal changes to your code. Bring your deep learning models to SageMaker and enable SageMaker Training Compiler to accelerate the speed of your training job on SageMaker ML instances for accelerated computing. \n",
    "\n",
    "For more information, see [SageMaker Training Compiler](https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html) in the *Amazon SageMaker Developer Guide*.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this demo, you'll use Hugging Face's `transformers` and `datasets` libraries with Amazon SageMaker Training Compiler to train the `gpt-2` model on the `Stanford Sentiment Treebank v2 (SST2)` dataset. Please note that by using this notebook you will be downloading SST2 from https://huggingface.co/datasets/sst2 and can check dataset information and terms there. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "**NOTE:** You can run this demo in SageMaker Studio, SageMaker notebook instances, or your local machine with AWS CLI set up. If using SageMaker Studio or SageMaker notebook instances, make sure you choose one of the PyTorch-based kernels, `Python 3 (PyTorch x.y Python 3.x CPU Optimized)` or `conda_pytorch_p38` respectively.\n",
    "\n",
    "**NOTE:** This notebook uses 2 `ml.g4dn.12xlarge` instances that have multiple GPUs. If you don't have enough quota, see [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2751c",
   "metadata": {},
   "source": [
    "## Development Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945c6f4",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This example notebook requires the **SageMaker Python SDK v2.108.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37613be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.108.0\" botocore boto3 awscli pandas numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"boto3: {boto3.__version__}\")\n",
    "print(f\"botocore: {botocore.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a693fa",
   "metadata": {},
   "source": [
    "Copy and run the following code if you need to upgrade IPython widgets for `datasets` library and restart kernel. This is only needed when preprocessing is done in the notebook.\n",
    "\n",
    "```python\n",
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "# has to restart kernel for the updates to be applied\n",
    "IPython.Application.instance().kernel.do_shutdown(True) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f105f",
   "metadata": {},
   "source": [
    "### SageMaker environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3b0d2",
   "metadata": {},
   "source": [
    "## SageMaker Training Job\n",
    "\n",
    "To create a SageMaker training job, we use an estimator. We use a `HuggingFace` estimator for SageMaker Training Compiler. Using the estimator, you can define which training script should SageMaker use through `entry_point`, which `instance_type` to use for training, which `hyperparameters` to pass, and so on.\n",
    "\n",
    "When a SageMaker training job starts, SageMaker takes care of starting and managing all the required machine learning instances, picks up the appropriate `HuggingFace` Deep Learning Container, uploads your training script, and downloads the data from `sagemaker_session_bucket` into the container at `/opt/ml/input/data`.\n",
    "\n",
    "First, we define some basic parameters common to all estimators.\n",
    "\n",
    "**Note**: We recommend you to turn the SageMaker Debugger's profiling and debugging tools off to avoid additional overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f50795",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_args = dict(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"run_clm.py\",\n",
    "    instance_type=\"ml.g4dn.12xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    py_version=\"py38\",\n",
    "    volume_size=100,\n",
    "    disable_profiler=True,  # Disabling SageMaker Profiler to avoid overheads during benchmarking\n",
    "    debugger_hook_config=False,  # Disabling SageMaker Debugger to avoid overheads during benchmarking\n",
    "    base_job_name=\"trcomp-pt-example\",\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"summary_train_runtime\", \"Regex\": \"'train_runtime': ([0-9.]*)\"},\n",
    "        {\n",
    "            \"Name\": \"summary_train_samples_per_second\",\n",
    "            \"Regex\": \"'train_samples_per_second': ([0-9.]*)\",\n",
    "        },\n",
    "        {\"Name\": \"summary_train_steps_per_second\", \"Regex\": \"'train_steps_per_second': ([0-9.]*)\"},\n",
    "        {\"Name\": \"summary_train_loss\", \"Regex\": \"'train_loss': ([0-9.]*)\"},\n",
    "        {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9.]*)\"},\n",
    "        {\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9.]*)\"},\n",
    "        {\"Name\": \"learning_rate\", \"Regex\": \"'learning_rate': ([0-9.]*)\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Since ml.g4dn.12xlarge instance has 4 GPUs, we set num_gpus_per_instance to 4\n",
    "num_gpus_per_instance = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b1bb3",
   "metadata": {},
   "source": [
    "Next, we define some basic arguments to be passed to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters are passed to the training script as arguments.\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model_type\": \"gpt2\",\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"fp16\": True,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"num_train_epochs\": 100,\n",
    "    \"block_size\": 512,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"dataloader_drop_last\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5204bc",
   "metadata": {},
   "source": [
    "In the following sections, we will create estimators and start training.\n",
    "\n",
    "### Training with Native PyTorch\n",
    "\n",
    "In the following sections, we will create estimators and start training.\n",
    "\n",
    "The `per_device_train_batch_size` below is the largest batch we could fit into the memory of a `ml.g4dn.12xlarge` instance. If you change the model, instance type, sequence length, or other parameters that affect memory consumption, you need to find the corresponding largest batch size.\n",
    "\n",
    "This example uses HuggingFace training script `run_clm.py`, which you can find it inside the `scripts` folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# The original learning rate was set for a batch of 32. Here we scale learning rate linearly with an adjusted batch size\n",
    "per_device_train_batch_size = 10\n",
    "global_batch_size = (\n",
    "    per_device_train_batch_size * num_gpus_per_instance * estimator_args[\"instance_count\"]\n",
    ")\n",
    "learning_rate = float(\"5e-5\") / 32 * global_batch_size\n",
    "\n",
    "# Configure the training job\n",
    "native_estimator = PyTorch(\n",
    "    framework_version=\"1.11\",\n",
    "    hyperparameters=dict(\n",
    "        **hyperparameters,\n",
    "        **{\n",
    "            \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "        },\n",
    "    ),\n",
    "    distribution={\"pytorchddp\": {\"enabled\": True}},\n",
    "    **estimator_args,\n",
    ")\n",
    "\n",
    "# Start the training job\n",
    "native_estimator.fit(wait=False)\n",
    "\n",
    "native_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef182d4",
   "metadata": {},
   "source": [
    "### Training with Optimized PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2011e0",
   "metadata": {},
   "source": [
    "Compilation through Training Compiler changes the memory footprint of the model. Most commonly, this manifests as a reduction in memory utilization and a consequent increase in the largest batch size that can fit on the GPU. Note that when you change the batch size, you must adjust the learning rate appropriately. Below, we have scaled the learning rate linearly with the increase in batch size.\n",
    "\n",
    "**Note:** We are using distribution mechanism `pytorchxla` which is a compiler aware method of distributed training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# The original learning rate was set for a batch of 32. Here we scale learning rate linearly with an adjusted batch size\n",
    "new_per_device_train_batch_size = 20\n",
    "global_batch_size = (\n",
    "    new_per_device_train_batch_size * num_gpus_per_instance * estimator_args[\"instance_count\"]\n",
    ")\n",
    "learning_rate = float(\"5e-5\") / 32 * global_batch_size\n",
    "\n",
    "# Configure the training job\n",
    "optimized_estimator = HuggingFace(\n",
    "    compiler_config=TrainingCompilerConfig(),\n",
    "    transformers_version=\"4.21\",\n",
    "    pytorch_version=\"1.11\",\n",
    "    hyperparameters=dict(\n",
    "        **hyperparameters,\n",
    "        **{\n",
    "            \"per_device_train_batch_size\": new_per_device_train_batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "        },\n",
    "    ),\n",
    "    distribution={\"pytorchxla\": {\"enabled\": True}},\n",
    "    **estimator_args,\n",
    ")\n",
    "\n",
    "# Start the training job\n",
    "optimized_estimator.fit(wait=False)\n",
    "\n",
    "optimized_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc95f44",
   "metadata": {},
   "source": [
    "### Wait for training jobs to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ca2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = native_estimator.sagemaker_session.sagemaker_client.get_waiter(\n",
    "    \"training_job_completed_or_stopped\"\n",
    ")\n",
    "waiter.wait(TrainingJobName=native_estimator.latest_training_job.name)\n",
    "waiter.wait(TrainingJobName=optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f266b9",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df1b04",
   "metadata": {},
   "source": [
    "**Note:** If the estimator object is no longer available due to a kernel break or refresh, you need to directly use the training job name and manually attach the training job to a new HuggingFace estimator. For example:\n",
    "\n",
    "```python\n",
    "native_estimator = PyTorch.attach(\"your_huggingface_training_job_name\")\n",
    "optimized_estimator = HuggingFace.attach(\"your_huggingface_training_job_name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4195d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "native_estimator = PyTorch.attach(native_estimator.latest_training_job.name)\n",
    "optimized_estimator = HuggingFace.attach(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb89b1",
   "metadata": {},
   "source": [
    "### Load logs of the training job *with* SageMaker Training Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14fa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture optimized\n",
    "\n",
    "# access the logs of the optimized training job\n",
    "optimized_estimator.sagemaker_session.logs_for_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14944bde",
   "metadata": {},
   "source": [
    "### Load logs of the training job *without* SageMaker Training Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture native\n",
    "\n",
    "# access the logs of the native training job\n",
    "native_estimator.sagemaker_session.logs_for_job(native_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba586740",
   "metadata": {},
   "source": [
    "### Create helper functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def _summarize(captured):\n",
    "    final = []\n",
    "    for line in captured.stdout.split(\"\\n\"):\n",
    "        cleaned = line.strip()\n",
    "        if \"{\" in cleaned and \"}\" in cleaned:\n",
    "            final.append(cleaned[cleaned.index(\"{\") : cleaned.index(\"}\") + 1])\n",
    "    return final\n",
    "\n",
    "\n",
    "def make_sense(string):\n",
    "    try:\n",
    "        return literal_eval(string)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def summarize(summary):\n",
    "    final = {\"train\": [], \"eval\": [], \"summary\": {}}\n",
    "    for line in summary:\n",
    "        interpretation = make_sense(line)\n",
    "        if interpretation:\n",
    "            if \"loss\" in interpretation:\n",
    "                final[\"train\"].append(interpretation)\n",
    "            elif \"eval_loss\" in interpretation:\n",
    "                final[\"eval\"].append(interpretation)\n",
    "            elif \"train_runtime\" in interpretation:\n",
    "                final[\"summary\"].update(interpretation)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853afbef",
   "metadata": {},
   "source": [
    "### Plot Optimized vs Native Training Throughput\n",
    "\n",
    "Visualize average throughput as reported by HuggingFace and see potential savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average throughput for the native PyTorch training as reported by Trainer\n",
    "n = summarize(_summarize(native))\n",
    "native_throughput = n[\"summary\"][\"train_samples_per_second\"]\n",
    "\n",
    "# Average throughput for the optimized PyTorch training as reported by Trainer\n",
    "o = summarize(_summarize(optimized))\n",
    "optimized_throughput = o[\"summary\"][\"train_samples_per_second\"]\n",
    "\n",
    "# Calculate percentage speedup of optimized PyTorch over native PyTorch\n",
    "avg_speedup = f\"{round((optimized_throughput/native_throughput-1)*100)}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.title(\"Training Throughput \\n (Higher is better)\")\n",
    "plt.ylabel(\"Samples/sec\")\n",
    "\n",
    "plt.bar(x=[1], height=native_throughput, label=\"Baseline PT\", width=0.35)\n",
    "plt.bar(x=[1.5], height=optimized_throughput, label=\"Compiler-enhanced PT\", width=0.35)\n",
    "\n",
    "plt.xlabel(\"  ====> {} Compiler savings <====\".format(avg_speedup))\n",
    "plt.xticks(ticks=[1, 1.5], labels=[\"Baseline PT\", \"Compiler-enhanced PT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d5bbd",
   "metadata": {},
   "source": [
    "### Convergence of Training Loss\n",
    "\n",
    "SageMaker Training Compiler does not affect the model convergence behavior. Here, we see the loss converges faster than the baseline PyTorch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_loss = [i[\"loss\"] for i in n[\"train\"]]\n",
    "vanilla_epochs = [i[\"epoch\"] for i in n[\"train\"]]\n",
    "optimized_loss = [i[\"loss\"] for i in o[\"train\"]]\n",
    "optimized_epochs = [i[\"epoch\"] for i in o[\"train\"]]\n",
    "\n",
    "plt.title(\"Plot of Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.plot(vanilla_epochs, vanilla_loss, label=\"Baseline PT\")\n",
    "plt.plot(optimized_epochs, optimized_loss, label=\"Compiler-enhanced PT\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcad63",
   "metadata": {},
   "source": [
    "### Training Stats\n",
    "\n",
    "Let's compare various training metrics with and without SageMaker Training Compiler. SageMaker Training Compiler provides an increase in training throughput which translates to a decrease in total training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34beb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame([n[\"summary\"], o[\"summary\"]], index=[\"Native\", \"Optimized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage speedup from SageMaker Training Compiler in terms of total training time reported by HF\n",
    "speedup = (\n",
    "    (n[\"summary\"][\"train_runtime\"] - o[\"summary\"][\"train_runtime\"])\n",
    "    * 100\n",
    "    / n[\"summary\"][\"train_runtime\"]\n",
    ")\n",
    "print(\n",
    "    f\"SageMaker Training Compiler integrated PyTorch is about {int(speedup)}% faster in terms of total training time as reported by HF.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468541aa",
   "metadata": {},
   "source": [
    "### Total Billable Time\n",
    "\n",
    "Finally, the decrease in total training time results in a decrease in the billable seconds from SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9192ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BillableTimeInSeconds(name):\n",
    "    describe_training_job = (\n",
    "        optimized_estimator.sagemaker_session.sagemaker_client.describe_training_job\n",
    "    )\n",
    "    details = describe_training_job(TrainingJobName=name)\n",
    "    return details[\"BillableTimeInSeconds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Billable = {}\n",
    "Billable[\"Native\"] = BillableTimeInSeconds(native_estimator.latest_training_job.name)\n",
    "Billable[\"Optimized\"] = BillableTimeInSeconds(optimized_estimator.latest_training_job.name)\n",
    "pd.DataFrame(Billable, index=[\"BillableSecs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = (Billable[\"Native\"] - Billable[\"Optimized\"]) * 100 / Billable[\"Native\"]\n",
    "print(f\"SageMaker Training Compiler integrated PyTorch was {int(speedup)}% faster in summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfc123",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34367f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def stop_training_job(name):\n",
    "    status = sm.describe_training_job(TrainingJobName=name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(native_estimator.latest_training_job.name)\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524e3ba",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
