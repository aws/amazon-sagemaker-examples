{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f82e791",
   "metadata": {
    "papermill": {
     "duration": 0.009226,
     "end_time": "2022-04-20T00:14:03.409609",
     "exception": false,
     "start_time": "2022-04-20T00:14:03.400383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train an MNIST model with PyTorch\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial shows how to train and test an MNIST model on SageMaker using PyTorch. \n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook takes approximately 5 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [PyTorch Estimator](#PyTorch-Estimator)\n",
    "1. [Implement the entry point for training](#Implement-the-entry-point-for-training)\n",
    "1. [Set hyperparameters](#Set-hyperparameters)\n",
    "1. [Set up channels for the training and testing data](#Set-up-channels-for-the-training-and-testing-data)\n",
    "1. [Run the training script on SageMaker](#Run-the-training-script-on-SageMaker)\n",
    "1. [Inspect and store model data](#Inspect-and-store-model-data)\n",
    "1. [Test and debug the entry point before executing the training container](#Test-and-debug-the-entry-point-before-executing-the-training-container)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e2df0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:14:03.448321Z",
     "iopub.status.busy": "2022-04-20T00:14:03.441488Z",
     "iopub.status.idle": "2022-04-20T00:14:04.966645Z",
     "shell.execute_reply": "2022-04-20T00:14:04.966136Z"
    },
    "papermill": {
     "duration": 1.54756,
     "end_time": "2022-04-20T00:14:04.966756",
     "exception": false,
     "start_time": "2022-04-20T00:14:03.419196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "output_path = \"s3://\" + sess.default_bucket() + \"/DEMO-mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289283c",
   "metadata": {
    "papermill": {
     "duration": 0.009481,
     "end_time": "2022-04-20T00:14:04.985765",
     "exception": false,
     "start_time": "2022-04-20T00:14:04.976284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PyTorch Estimator\n",
    "\n",
    "The `PyTorch` class allows you to run your training script on SageMaker\n",
    "infrastracture in a containerized environment. In this notebook, we\n",
    "refer to this container as *training container*. \n",
    "\n",
    "You need to configure\n",
    "it with the following parameters to set up the environment:\n",
    "\n",
    "- `entry_point`: A user-defined Python file used by the training container as the \n",
    "instructions for training. We further discuss this file in the next subsection.\n",
    "\n",
    "- `role`: An IAM role to make AWS service requests\n",
    "\n",
    "- `instance_type`: The type of SageMaker instance to run your training script. \n",
    "Set it to `local` if you want to run the training job on \n",
    "the SageMaker instance you are using to run this notebook\n",
    "\n",
    "- `instance_count`: The number of instances to run your training job on. \n",
    "Multiple instances are needed for distributed training.\n",
    "\n",
    "- `output_path`: \n",
    "S3 bucket URI to save training output (model artifacts and output files)\n",
    "\n",
    "- `framework_version`: The version of PyTorch to use\n",
    "\n",
    "- `py_version`: The Python version to use\n",
    "\n",
    "For more information, see the [EstimatorBase API reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50284c4",
   "metadata": {
    "papermill": {
     "duration": 0.010604,
     "end_time": "2022-04-20T00:14:05.006577",
     "exception": false,
     "start_time": "2022-04-20T00:14:04.995973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implement the entry point for training\n",
    "\n",
    "The entry point for training is a Python script that provides all \n",
    "the code for training a PyTorch model. It is used by the SageMaker \n",
    "PyTorch Estimator (`PyTorch` class above) as the entry point for running the training job.\n",
    "\n",
    "Under the hood, SageMaker PyTorch Estimator creates a docker image\n",
    "with runtime environemnts \n",
    "specified by the parameters you provide to initiate the\n",
    "estimator class, and it injects the training script into the \n",
    "docker image as the entry point to run the container.\n",
    "\n",
    "In the rest of the notebook, we use *training image* to refer to the \n",
    "docker image specified by the PyTorch Estimator and *training container*\n",
    "to refer to the container that runs the training image. \n",
    "\n",
    "This means your training script is very similar to a training script\n",
    "you might run outside Amazon SageMaker, but it can access the useful environment \n",
    "variables provided by the training image. See [the complete list of environment variables](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md) for a complete \n",
    "description of all environment variables your training script\n",
    "can access. \n",
    "\n",
    "In this example, we use the training script `code/train.py`\n",
    "as the entry point for our PyTorch Estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4452d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:14:05.031770Z",
     "iopub.status.busy": "2022-04-20T00:14:05.031156Z",
     "iopub.status.idle": "2022-04-20T00:14:05.906661Z",
     "shell.execute_reply": "2022-04-20T00:14:05.907136Z"
    },
    "papermill": {
     "duration": 0.890643,
     "end_time": "2022-04-20T00:14:05.907284",
     "exception": false,
     "start_time": "2022-04-20T00:14:05.016641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgzip\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader, Dataset\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\u001b[37m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2_drop = nn.Dropout2d()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m320\u001b[39;49;00m, \u001b[34m50\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m50\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv1(x), \u001b[34m2\u001b[39;49;00m))\r\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv2_drop(\u001b[36mself\u001b[39;49;00m.conv2(x)), \u001b[34m2\u001b[39;49;00m))\r\n",
      "        x = x.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m320\u001b[39;49;00m)\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\r\n",
      "        x = F.dropout(x, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Decode binary data from SM_CHANNEL_TRAINING\u001b[39;49;00m\r\n",
      "\u001b[37m# Decode and preprocess data\u001b[39;49;00m\r\n",
      "\u001b[37m# Create map dataset\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mnormalize\u001b[39;49;00m(x, axis):\r\n",
      "    eps = np.finfo(\u001b[36mfloat\u001b[39;49;00m).eps\r\n",
      "    mean = np.mean(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[37m# avoid division by zero\u001b[39;49;00m\r\n",
      "    std = np.std(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m) + eps\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x - mean) / std\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_to_tensor\u001b[39;49;00m(data_dir, images_file, labels_file):\r\n",
      "    \u001b[33m\"\"\"Byte string to torch tensor\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, images_file), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        images = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m16\u001b[39;49;00m).reshape(-\u001b[34m1\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m).astype(np.float32)\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, labels_file), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        labels = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m8\u001b[39;49;00m).astype(np.int64)\r\n",
      "\r\n",
      "    \u001b[37m# normalize the images\u001b[39;49;00m\r\n",
      "    images = normalize(images, axis=(\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m# add channel dimension (depth-major)\u001b[39;49;00m\r\n",
      "    images = np.expand_dims(images, axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# to torch tensor\u001b[39;49;00m\r\n",
      "    images = torch.tensor(images, dtype=torch.float32)\r\n",
      "    labels = torch.tensor(labels, dtype=torch.int64)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m images, labels\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMNIST\u001b[39;49;00m(Dataset):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data_dir, train=\u001b[34mTrue\u001b[39;49;00m):\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m train:\r\n",
      "            images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.images, \u001b[36mself\u001b[39;49;00m.labels = convert_to_tensor(data_dir, images_file, labels_file)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.images[idx], \u001b[36mself\u001b[39;49;00m.labels[idx]\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\r\n",
      "        torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    train_loader = DataLoader(\r\n",
      "        MNIST(args.train, train=\u001b[34mTrue\u001b[39;49;00m), batch_size=args.batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    test_loader = DataLoader(\r\n",
      "        MNIST(args.test, train=\u001b[34mFalse\u001b[39;49;00m), batch_size=args.test_batch_size, shuffle=\u001b[34mFalse\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    net = Net().to(device)\r\n",
      "    loss_fn = nn.CrossEntropyLoss()\r\n",
      "    optimizer = optim.Adam(\r\n",
      "        net.parameters(), betas=(args.beta_1, args.beta_2), weight_decay=args.weight_decay\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStart training ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        net.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (imgs, labels) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\r\n",
      "            imgs, labels = imgs.to(device), labels.to(device)\r\n",
      "            output = net(imgs)\r\n",
      "            loss = loss_fn(output, labels)\r\n",
      "\r\n",
      "            optimizer.zero_grad()\r\n",
      "            loss.backward()\r\n",
      "            optimizer.step()\r\n",
      "\r\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                        epoch,\r\n",
      "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(imgs),\r\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\r\n",
      "                        loss.item(),\r\n",
      "                    )\r\n",
      "                )\r\n",
      "\r\n",
      "        \u001b[37m# test the model\u001b[39;49;00m\r\n",
      "        test(net, test_loader, device)\r\n",
      "\r\n",
      "    \u001b[37m# save model checkpoint\u001b[39;49;00m\r\n",
      "    save_model(net, args.model_dir)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\r\n",
      "    model.eval()\r\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m imgs, labels \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            imgs, labels = imgs.to(device), labels.to(device)\r\n",
      "            output = model(imgs)\r\n",
      "            test_loss += F.cross_entropy(output, labels, reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).item()\r\n",
      "\r\n",
      "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]\r\n",
      "            correct += pred.eq(labels.view_as(pred)).sum().item()\r\n",
      "\r\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\r\n",
      "    logger.info(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset), \u001b[34m100.0\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\r\n",
      "        )\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    torch.save(model.cpu().state_dict(), path)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.001\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--beta_1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mBETA1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbeta1 (default: 0.9)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--beta_2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.999\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mBETA2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbeta2 (default: 0.999)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--weight-decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m1e-4\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mWD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mL2 weight decay (default: 1e-4)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m100\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38436bf",
   "metadata": {
    "papermill": {
     "duration": 0.010515,
     "end_time": "2022-04-20T00:14:05.928597",
     "exception": false,
     "start_time": "2022-04-20T00:14:05.918082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set hyperparameters\n",
    "\n",
    "In addition, the PyTorch estimator allows you to parse command line arguments\n",
    "to your training script via `hyperparameters`.\n",
    "\n",
    "Note: local mode is not supported in SageMaker Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1670729e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:14:05.967344Z",
     "iopub.status.busy": "2022-04-20T00:14:05.966715Z",
     "iopub.status.idle": "2022-04-20T00:14:06.035451Z",
     "shell.execute_reply": "2022-04-20T00:14:06.035951Z"
    },
    "papermill": {
     "duration": 0.096389,
     "end_time": "2022-04-20T00:14:06.036087",
     "exception": false,
     "start_time": "2022-04-20T00:14:05.939698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set local_mode to True to run the training script on the machine that runs this notebook\n",
    "\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = \"local\"\n",
    "else:\n",
    "    instance_type = \"ml.c4.xlarge\"\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",  # directory of your training script\n",
    "    role=role,\n",
    "    framework_version=\"1.5.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    volume_size=250,\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\"batch-size\": 128, \"epochs\": 1, \"learning-rate\": 1e-3, \"log-interval\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05bb23",
   "metadata": {
    "papermill": {
     "duration": 0.010553,
     "end_time": "2022-04-20T00:14:06.057197",
     "exception": false,
     "start_time": "2022-04-20T00:14:06.046644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training container executes your training script like:\n",
    "\n",
    "```\n",
    "python train.py --batch-size 100 --epochs 1 --learning-rate 1e-3 --log-interval 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c7ec6",
   "metadata": {
    "papermill": {
     "duration": 0.010437,
     "end_time": "2022-04-20T00:14:06.078100",
     "exception": false,
     "start_time": "2022-04-20T00:14:06.067663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set up channels for the training and testing data\n",
    "\n",
    "Tell the `PyTorch` estimator where to find the training and \n",
    "testing data. It can be a path to an S3 bucket, or a path\n",
    "in your local file system if you use local mode. In this example,\n",
    "we download the MNIST data from a public S3 bucket and upload it \n",
    "to your default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef3cf6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:14:06.114826Z",
     "iopub.status.busy": "2022-04-20T00:14:06.114007Z",
     "iopub.status.idle": "2022-04-20T00:14:08.411086Z",
     "shell.execute_reply": "2022-04-20T00:14:08.410612Z"
    },
    "papermill": {
     "duration": 2.322656,
     "end_time": "2022-04-20T00:14:08.411228",
     "exception": false,
     "start_time": "2022-04-20T00:14:06.088572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Download training and testing data from a public S3 bucket\n",
    "\n",
    "\n",
    "def download_from_s3(data_dir=\"./data\", train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    # download objects\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = f\"sagemaker-sample-files\"\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3(\"./data\", True)\n",
    "download_from_s3(\"./data\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b484fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:14:08.439921Z",
     "iopub.status.busy": "2022-04-20T00:14:08.436752Z",
     "iopub.status.idle": "2022-04-20T00:14:08.997434Z",
     "shell.execute_reply": "2022-04-20T00:14:08.997880Z"
    },
    "papermill": {
     "duration": 0.575952,
     "end_time": "2022-04-20T00:14:08.998008",
     "exception": false,
     "start_time": "2022-04-20T00:14:08.422056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload to the default bucket\n",
    "\n",
    "prefix = \"DEMO-mnist\"\n",
    "bucket = sess.default_bucket()\n",
    "loc = sess.upload_data(path=\"./data\", bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "channels = {\"training\": loc, \"testing\": loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6d5fe",
   "metadata": {
    "papermill": {
     "duration": 0.010186,
     "end_time": "2022-04-20T00:14:09.018612",
     "exception": false,
     "start_time": "2022-04-20T00:14:09.008426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The keys of the `channels` dictionary are passed to the training image,\n",
    "and it creates the environment variable `SM_CHANNEL_<key name>`. \n",
    "\n",
    "In this example, `SM_CHANNEL_TRAINING` and `SM_CHANNEL_TESTING` are created in the training image (see \n",
    "how `code/train.py` accesses these variables). For more information,\n",
    "see: [SM_CHANNEL_{channel_name}](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name).\n",
    "\n",
    "If you want, you can create a channel for validation:\n",
    "```\n",
    "channels = {\n",
    "    'training': train_data_loc,\n",
    "    'validation': val_data_loc,\n",
    "    'test': test_data_loc\n",
    "}\n",
    "```\n",
    "You can then access this channel within your training script via\n",
    "`SM_CHANNEL_VALIDATION`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705a26d",
   "metadata": {
    "papermill": {
     "duration": 0.010151,
     "end_time": "2022-04-20T00:14:09.039008",
     "exception": false,
     "start_time": "2022-04-20T00:14:09.028857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the training script on SageMaker\n",
    "Now, the training container has everything to execute your training\n",
    "script. Start the container by calling the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e3af70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:14:09.062943Z",
     "iopub.status.busy": "2022-04-20T00:14:09.062361Z",
     "iopub.status.idle": "2022-04-20T00:18:51.894121Z",
     "shell.execute_reply": "2022-04-20T00:18:51.894573Z"
    },
    "papermill": {
     "duration": 282.845615,
     "end_time": "2022-04-20T00:18:51.894704",
     "exception": false,
     "start_time": "2022-04-20T00:14:09.049089",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-20 00:14:09 Starting - Starting the training job."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-04-20 00:14:36 Starting - Preparing the instances for trainingProfilerReport-1650413649: InProgress\n",
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-04-20 00:16:04 Downloading - Downloading input data."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-04-20 00:17:04 Training - Downloading the training image."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-04-20 00:17:24 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,149 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,167 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,180 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,187 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,570 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,588 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,603 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:23,617 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 128,\n",
      "        \"epochs\": 1,\n",
      "        \"learning-rate\": 0.001,\n",
      "        \"log-interval\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-20-00-14-09-077\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-000000000000/pytorch-training-2022-04-20-00-14-09-077/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":128,\"epochs\":1,\"learning-rate\":0.001,\"log-interval\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-000000000000/pytorch-training-2022-04-20-00-14-09-077/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":128,\"epochs\":1,\"learning-rate\":0.001,\"log-interval\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-20-00-14-09-077\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-000000000000/pytorch-training-2022-04-20-00-14-09-077/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"128\",\"--epochs\",\"1\",\"--learning-rate\",\"0.001\",\"--log-interval\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch-size 128 --epochs 1 --learning-rate 0.001 --log-interval 100\u001b[0m\n",
      "\u001b[34mStart training ...\u001b[0m\n",
      "\u001b[34m[2022-04-20 00:17:26.738 algo-1:27 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-04-20 00:17:26.739 algo-1:27 INFO hook.py:192] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-04-20 00:17:26.739 algo-1:27 INFO hook.py:237] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-04-20 00:17:26.739 algo-1:27 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-04-20 00:17:26.739 algo-1:27 INFO hook.py:382] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-04-20 00:17:26.740 algo-1:27 INFO hook.py:443] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.571117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.435707\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.278377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.247071\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1151, Accuracy: 9642/10000, 96.42)\u001b[0m\n",
      "\u001b[34mSaving the model\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1151, Accuracy: 9642/10000, 96.42)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model\u001b[0m\n",
      "\u001b[34m2022-04-20 00:17:43,442 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-04-20 00:18:10 Uploading - Uploading generated training model"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-04-20 00:18:24 Completed - Training job completed\n",
      "ProfilerReport-1650413649: NoIssuesFound\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 136\n",
      "Billable seconds: 136\n"
     ]
    }
   ],
   "source": [
    "est.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37b980",
   "metadata": {
    "papermill": {
     "duration": 0.016054,
     "end_time": "2022-04-20T00:18:51.927150",
     "exception": false,
     "start_time": "2022-04-20T00:18:51.911096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inspect and store model data\n",
    "\n",
    "Now, the training is finished, and the model artifact has been saved in \n",
    "the `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b722e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:18:51.964088Z",
     "iopub.status.busy": "2022-04-20T00:18:51.963576Z",
     "iopub.status.idle": "2022-04-20T00:18:52.012287Z",
     "shell.execute_reply": "2022-04-20T00:18:52.012722Z"
    },
    "papermill": {
     "duration": 0.069725,
     "end_time": "2022-04-20T00:18:52.012853",
     "exception": false,
     "start_time": "2022-04-20T00:18:51.943128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-west-2-000000000000/DEMO-mnist/pytorch-training-2022-04-20-00-14-09-077/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "pt_mnist_model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", pt_mnist_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0d978",
   "metadata": {
    "papermill": {
     "duration": 0.017487,
     "end_time": "2022-04-20T00:18:52.047843",
     "exception": false,
     "start_time": "2022-04-20T00:18:52.030356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We store the variable `pt_mnist_model_data` in the current notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b02f567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:18:52.086443Z",
     "iopub.status.busy": "2022-04-20T00:18:52.085773Z",
     "iopub.status.idle": "2022-04-20T00:18:52.088532Z",
     "shell.execute_reply": "2022-04-20T00:18:52.089010Z"
    },
    "papermill": {
     "duration": 0.02407,
     "end_time": "2022-04-20T00:18:52.089155",
     "exception": false,
     "start_time": "2022-04-20T00:18:52.065085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pt_mnist_model_data' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pt_mnist_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d8666",
   "metadata": {
    "papermill": {
     "duration": 0.017238,
     "end_time": "2022-04-20T00:18:52.123771",
     "exception": false,
     "start_time": "2022-04-20T00:18:52.106533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test and debug the entry point before executing the training container\n",
    "\n",
    "The entry point `code/train.py` can be executed in the training container. \n",
    "When you develop your own training script, it is a good practice to simulate the container environment \n",
    "in the local shell and test it before sending it to SageMaker, because debugging in a containerized environment\n",
    "is rather cumbersome. The following script shows how you can test your training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f02a5b5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-20T00:18:52.168396Z",
     "iopub.status.busy": "2022-04-20T00:18:52.163778Z",
     "iopub.status.idle": "2022-04-20T00:18:53.054539Z",
     "shell.execute_reply": "2022-04-20T00:18:53.055054Z"
    },
    "papermill": {
     "duration": 0.913552,
     "end_time": "2022-04-20T00:18:53.055214",
     "exception": false,
     "start_time": "2022-04-20T00:18:52.141662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtrain\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m parse_args, train\r\n",
      "\r\n",
      "dirname = os.path.dirname(os.path.abspath(\u001b[31m__file__\u001b[39;49;00m))\r\n",
      "\r\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(dirname, \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "    CONFIG = json.load(f)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdownload_from_s3\u001b[39;49;00m(data_dir=\u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[34mTrue\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Download MNIST dataset and convert it to numpy array\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data_dir (str): directory to save the data\u001b[39;49;00m\r\n",
      "\u001b[33m        train (bool): download training set\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        tuple of images and labels as numpy arrays\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(data_dir):\r\n",
      "        os.makedirs(data_dir)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m train:\r\n",
      "        images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# download objects\u001b[39;49;00m\r\n",
      "    s3 = boto3.client(\u001b[33m\"\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    bucket = CONFIG[\u001b[33m\"\u001b[39;49;00m\u001b[33mpublic_bucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "    \u001b[34mfor\u001b[39;49;00m obj \u001b[35min\u001b[39;49;00m [images_file, labels_file]:\r\n",
      "        key = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33mdatasets/image/MNIST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, obj)\r\n",
      "        dest = os.path.join(data_dir, obj)\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(dest):\r\n",
      "            s3.download_file(bucket, key, dest)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mEnv\u001b[39;49;00m:\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[37m# simulate container env\u001b[39;49;00m\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33malgo-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33malgo-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    Env()\r\n",
      "    args = parse_args()\r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/test_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea4312",
   "metadata": {
    "papermill": {
     "duration": 0.018244,
     "end_time": "2022-04-20T00:18:53.091699",
     "exception": false,
     "start_time": "2022-04-20T00:18:53.073455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we trained a PyTorch model on the MNIST dataset by fitting a SageMaker estimator. For next steps on how to deploy the trained model and perform inference, see [Deploy a Trained PyTorch Model](https://sagemaker-examples.readthedocs.io/en/latest/frameworks/pytorch/get_started_mnist_deploy.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 291.09138,
   "end_time": "2022-04-20T00:18:53.728862",
   "environment_variables": {},
   "exception": null,
   "input_path": "get_started_mnist_train.ipynb",
   "output_path": "/opt/ml/processing/output/get_started_mnist_train-2022-04-20-00-09-08.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:000000000000:1234abcd-12ab-34cd-56ef-1234567890ab"
   },
   "start_time": "2022-04-20T00:14:02.637482",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}