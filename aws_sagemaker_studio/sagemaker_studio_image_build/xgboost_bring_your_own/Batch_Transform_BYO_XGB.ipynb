{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Batch Transform using an XgBoost Bring Your Own Container (BYOC)\n",
    "\n",
    "In this notebook, we will walk through an end to end data science workflow demonstrating how to build your own custom XGBoost Container using Amazon SageMaker Studio. We will first process the data using SageMaker Processing, push an XGB algorithm container to ECR, train the model, and use Batch Transform to generate inferences from your model in batch or offline mode. Finally we will use SageMaker Experiments to capture the metadata and lineage associated with the trained model. This is a key differentiator of SageMaker Studio as the metadata captured is visible in the Experiments UI. \n",
    "\n",
    "\n",
    "## The example\n",
    "\n",
    "In this example we show how to package a custom XGBoost container with Amazon SageMaker studio with a Python example which works with the UCI Credit Card dataset. To use a different algorithm or a different dataset, you can easily change the Docker container and the xgboost folder attached with this code.\n",
    "\n",
    "In this example, we use a single image to support training and hosting. This simplifies the procedure because we only need to manage one image for both tasks. Sometimes you may want separate images for training and hosting because they have different requirements. In this case, separate the parts discussed below into separate Dockerfiles and build two images. Choosing whether to use a single image or two images is a matter of what is most convenient for you to develop and manage.\n",
    "\n",
    "If you're only using Amazon SageMaker for training or hosting, but not both, only the functionality used needs to be built into your container.\n",
    "\n",
    "## The workflow\n",
    "\n",
    "This notebook is divided into three parts: *exploring your data and feature engineering*, *building your contianer* and *using your container to train a model and generate inferences*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile\n",
    "\n",
    "The Dockerfile describes the image that we want to build. You can think of it as describing the complete operating system installation of the system that you want to run. A Docker container running is quite a bit lighter than a full operating system, however, because it takes advantage of Linux on the host machine for the basic operations. \n",
    "\n",
    "For the Python science stack, we start from an official TensorFlow docker image and run the normal tools to install TensorFlow Serving. Then we add the code that implements our specific algorithm to the container and set up the right environment for it to run under.\n",
    "\n",
    "For details on how BYOC works with SageMaker Notebook instances, see this example: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb. Unlike SageMaker notebook instances, in SageMaker studio as we will see below, you will not need the build_and_push.sh script anymore. The studio-build CLI will handle pushing the container to ECR for you. \n",
    "\n",
    "Let's look at the Dockerfile for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pre-requisites: Download the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"sagemaker-experiments\"\n",
    "!{sys.executable} -m pip install \"sagemaker-studio-image-build\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites: Check the SageMaker SDK Version.\n",
    "\n",
    "This notebook runs on SageMaker SDK Version < 2. Below we check the version, and if it is 2 or above, we reinstall the older version. If you see the \"Please restart the kernel\" prompt, simply click Kernel above and hit Restart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    !{sys.executable} -m pip install \"sagemaker>=1.71.0,<2.0.0\"\n",
    "    print(\"Installing previous SageMaker Version. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"Version is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Ensure IAM Role has access to necessary services\n",
    "\n",
    "The SageMaker Studio Image Build CLI uses Amazon Elastic Container Registry and AWS CodeBuild so we need to ensure that the role we provide as input to our CLI commands has the necessary policies and permissions attached. \n",
    "\n",
    "Two scenarios are supported including: \n",
    "\n",
    "   * **Add IAM Permissions to SageMaker Execution Role** \n",
    "\n",
    "   This scenario includes updating the Execution Role attached to this notebook instance with the required permissions.  In this scenario, you need to get the current execution role and ensure the trust policy and additional permissions are associated with the role.  \n",
    "       \n",
    "   * **Create/Utilize a secondary role with appropriate permissions attached** \n",
    "\n",
    "  This scenario include using a secondary role setup with the permissions below and identified in the --role argument when invoking the CLI (Example: *sm-docker build .  --role build-cli-role*)\n",
    "    \n",
    "\n",
    "**Ensure the role that will be used has the following**\n",
    "\n",
    "1) Trust policy with CodeBuild\n",
    "\n",
    "      {\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": [\n",
    "              \"codebuild.amazonaws.com\"\n",
    "            ]\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "2) Permissions attached to the execution role to execute a build in AWS CodeBuild, create ECR repository and push images to ECR \n",
    "\n",
    "     {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"codebuild:DeleteProject\",\n",
    "                \"codebuild:CreateProject\",\n",
    "                \"codebuild:BatchGetBuilds\",\n",
    "                \"codebuild:StartBuild\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:codebuild:*:*:project/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogStream\",\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:GetLogEvents\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*:log-stream:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogGroup\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:CreateRepository\",\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:CompleteLayerUpload\",\n",
    "                \"ecr:DescribeImages\",\n",
    "                \"ecr:DescribeRepositories\",\n",
    "                \"ecr:UploadLayerPart\",\n",
    "                \"ecr:ListImages\",\n",
    "                \"ecr:InitiateLayerUpload\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:PutImage\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:ecr:*:*:repository/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"ecr:GetAuthorizationToken\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "              \"s3:GetObject\",\n",
    "              \"s3:DeleteObject\",\n",
    "              \"s3:PutObject\"\n",
    "              ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker-*/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:CreateBucket\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:ListRoles\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::*:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringLikeIfExists\": {\n",
    "                    \"iam:PassedToService\": \"codebuild.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel\n",
    "\n",
    "Once the libraries are installed, restart the kernel by clicking Kernel --> Restart and Running all the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the role we have created for our notebook here:\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Setup: Import libraries and set global definitions.\n",
    "\n",
    "All needed libraries will come pre-installed with this notebook with the Lifecycle configuration scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SageMaker Experiments\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify buckets for storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom bucket here.\n",
    "rawbucket = sess.default_bucket()\n",
    "prefix = \"sagemaker-modelmonitor\"  # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + \"/data\"\n",
    "traindataprefix = prefix + \"/train_data\"\n",
    "testdataprefix = prefix + \"/test_data\"\n",
    "testdatanolabelprefix = prefix + \"/test_data_no_label\"\n",
    "trainheaderprefix = prefix + \"/train_headers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of the data science lifecyle is data exploration, pre-processing and feature engineering. We will demonstrate how to use SM notebooks for data exploration and SM Processing for feature engineering and pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the UCI Machine Learning Archive dataset on payment default for this example [https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+client]. Here we have a number of common features such as payment histories from prior months, payments, bills etc to predict a binary outcome -- whether or not a user will default on their payment in the following month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data.xls\", header=1)\n",
    "data = data.drop(columns=[\"ID\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=[\"Label\"])], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Once you have downloaded the dataset, the next step in the data science lifecycle is to explore the dataset. A correlation plot can indicate whether the features are correlated to one another and the label itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corr plot\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(data.corr(), fignum=f.number)\n",
    "plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title(\"Correlation Matrix\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "SCAT_COLUMNS = [\"BILL_AMT1\", \"BILL_AMT2\", \"PAY_AMT1\", \"PAY_AMT2\"]\n",
    "scatter_matrix(data[SCAT_COLUMNS], figsize=(10, 10), diagonal=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Secure Feature Processing pipeline using SageMaker Processing\n",
    "\n",
    "While you can pre-process small amounts of data directly in a notebook SageMaker Processing offloads the heavy lifting of pre-processing larger datasets by provisioning the underlying infrastructure, downloading the data from an S3 location to the processing container, running the processing scripts, storing the processed data in an output directory in Amazon S3 and deleting the underlying transient resources needed to run the processing job. Once the processing job is complete, the infrastructure used to run the job is wiped, and any temporary data stored on it is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data(\"rawdata\", bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use SageMaker Processing with Sk Learn. -- combine data into train and test at this stage if possible.\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\", role=role, instance_type=\"ml.c4.xlarge\", instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a preprocessing script (same as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--random-split\", type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"rawdata.csv\")\n",
    "\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "\n",
    "    COLS = df.columns\n",
    "    newcolorder = (\n",
    "        [\"PAY_AMT1\", \"BILL_AMT1\"]\n",
    "        + list(COLS[1:])[:11]\n",
    "        + list(COLS[1:])[12:17]\n",
    "        + list(COLS[1:])[18:]\n",
    "    )\n",
    "\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state = args.random_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.drop(\"Label\", axis=1), df[\"Label\"], test_size=split_ratio, random_state=random_state\n",
    "    )\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        ([\"PAY_AMT1\"], StandardScaler()), ([\"BILL_AMT1\"], MinMaxScaler()), remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    print(\"Running preprocessing and feature engineering transformations\")\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns=newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns=newcolorder)\n",
    "\n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat(\n",
    "        [pd.DataFrame(y_train.values, columns=[\"Label\"]), train_features], axis=1\n",
    "    )\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=[\"Label\"]), test_features], axis=1)\n",
    "\n",
    "    print(\"Train data shape after preprocessing: {}\".format(train_features.shape))\n",
    "    print(\"Test data shape after preprocessing: {}\".format(test_features.shape))\n",
    "\n",
    "    train_features_headers_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/train_headers\", \"train_data_with_headers.csv\"\n",
    "    )\n",
    "\n",
    "    train_features_output_path = os.path.join(\"/opt/ml/processing/train\", \"train_data.csv\")\n",
    "\n",
    "    test_features_output_path = os.path.join(\"/opt/ml/processing/test\", \"test_data.csv\")\n",
    "\n",
    "    print(\"Saving training features to {}\".format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "\n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "\n",
    "    print(\"Saving test features to {}\".format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the preprocessing code over to the s3 bucket\n",
    "codeprefix = prefix + \"/code\"\n",
    "codeupload = sess.upload_data(\"preprocessing.py\", bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_location = rawbucket + \"/\" + traindataprefix\n",
    "test_data_location = rawbucket + \"/\" + testdataprefix\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will execute the script above using the managed scikit-learn preprocessing container. This step may take a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=codeupload,\n",
    "    inputs=[ProcessingInput(source=raw_data_location, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\",\n",
    "            source=\"/opt/ml/processing/train\",\n",
    "            destination=\"s3://\" + train_data_location,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test_data\",\n",
    "            source=\"/opt/ml/processing/test\",\n",
    "            destination=\"s3://\" + test_data_location,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data_headers\",\n",
    "            source=\"/opt/ml/processing/train_headers\",\n",
    "            destination=\"s3://\" + rawbucket + \"/\" + prefix + \"/train_headers\",\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"train_data\":\n",
    "        preprocessed_training_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"test_data\":\n",
    "        preprocessed_test_data = output[\"S3Output\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building the Container and Training the model\n",
    "\n",
    "\n",
    "### Step 5: Set up SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we first build the Docker image by providing the Dockerfile discussed before and train a model using that Dockerfile\n",
    "\n",
    "We use SageMaker Experiments for data scientists to track the lineage of the model from the raw data source to the preprocessing steps and the model training pipeline. With SageMaker Experiments, data scientists can compare, track and manage multiple diferent model training jobs, data processing jobs, hyperparameter tuning jobs and retain a lineage from the source data to the training job artifacts to the model hyperparameters and any custom metrics that they may want to monitor as part of the model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker Experiment\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f\"CreditCardDefault-{int(time.time())}\",\n",
    "    description=\"Predict credit card default from payments data\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "print(cc_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to training, we want to track the lineage of the entire machine learing pipeline also including the processing job above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\"train_test_split_ratio\": 0.2, \"random_state\": 0})\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(\n",
    "        name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location\n",
    "    )\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build XgBoost container for training\n",
    "\n",
    "The code for the XGB container is already supplied with this notebook. We simply need to build this container and push it to ECR. The single line of code below will do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sm-docker build ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Model\n",
    "\n",
    "The same security postures we applied previously during SM Processing apply to training jobs. We will also have SageMaker experiments track the training job and store metadata such as model artifact location, training/validation data location, model hyperparameters etc.\n",
    "\n",
    "As shown above, your image URI has the following form:\n",
    "Image URI: {account-id}.dkr.ecr.{region}.amazonaws.com/sagemaker-studio-{studioID}:{username}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "ecr = boto3.client(\"ecr\")\n",
    "domain_id = \"sagemaker-studio-{}\".format(sm.list_apps()[\"Apps\"][0][\"DomainId\"])\n",
    "image_tag = ecr.list_images(repositoryName=domain_id, filter={\"tagStatus\": \"TAGGED\"})[\"imageIds\"][\n",
    "    0\n",
    "][\"imageTag\"]\n",
    "image = \"{}.dkr.ecr.{}.amazonaws.com/{}:{}\".format(account, region, domain_id, image_tag)\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"cc-fraud-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "    trial_name=trial_name, experiment_name=cc_experiment.experiment_name, sagemaker_boto_client=sm\n",
    ")\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m4.xlarge\",\n",
    "    train_max_run=86400,\n",
    "    output_path=\"s3://{}/{}/models\".format(rawbucket, prefix),\n",
    "    sagemaker_session=sess,\n",
    ")  # set to true for distributed training\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    verbosity=0,\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=100,\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    inputs={\"training\": \"s3://\" + train_data_location},\n",
    "    job_name=cc_training_job_name,\n",
    "    experiment_config={\n",
    "        \"TrialName\": cc_trial.trial_name,  # log training job in Trials for lineage\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    wait=True,\n",
    ")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having used SageMaker Experiments to track the training runs, we can now extract model metadata to get the entire lineage of the model from the source data to the model artifacts and the hyperparameters.\n",
    "\n",
    "To do this, simply call the **describe_trial_component** API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Present the Model Lineage as a dataframe\n",
    "from sagemaker.session import Session\n",
    "\n",
    "session = boto3.Session()\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(session, sm),\n",
    "    search_expression={\n",
    "        \"Filters\": [{\"Name\": \"Parents.TrialName\", \"Operator\": \"Equals\", \"Value\": trial_name}]\n",
    "    },\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "lineagedf = lineage_table.dataframe()\n",
    "\n",
    "lineagedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get detailed information about a particular trial\n",
    "sm.describe_trial_component(TrialComponentName=lineagedf.TrialComponentName[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using the trained model for inference\n",
    "\n",
    "### Step 8: Inference using Batch Transform\n",
    "\n",
    "Let's first use Batch Transform to generate inferences for the test dataset you pre-processed before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(rawbucket, testdataprefix + \"/test_data.csv\", \"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcolorder = (\n",
    "    [\"PAY_AMT1\", \"BILL_AMT1\"] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    ")\n",
    "test_full = pd.read_csv(\"test_data.csv\", names=[\"Label\"] + newcolorder)\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_label = test_full.drop(columns=[\"Label\"], axis=1)\n",
    "label = test_full[\"Label\"]\n",
    "test_data_no_label.to_csv(\"test_data_no_label.csv\", index=False, header=False)\n",
    "test_data_no_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "test_data_nohead_location = sess.upload_data(\n",
    "    \"test_data_no_label.csv\", bucket=rawbucket, key_prefix=testdatanolabelprefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, \"ml.m5.xlarge\", accept=\"text/csv\")\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_nohead_location, split_type=\"Line\", content_type=\"text/csv\")\n",
    "sm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    obj = s3.Object(bucket_name, \"{}/{}\".format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_csv_output_from_s3(sm_transformer.output_path, \"test_data_no_label.csv.out\")\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - np.unique(data[\"Label\"], return_counts=True)[1][1] / (len(data[\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Baseline Accuracy = {}\".format(\n",
    "        1 - np.unique(data[\"Label\"], return_counts=True)[1][1] / (len(data[\"Label\"]))\n",
    "    )\n",
    ")\n",
    "print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"Predicted\"] = output_df.values\n",
    "output_df[\"Label\"] = label\n",
    "confusion_matrix = pd.crosstab(\n",
    "    output_df[\"Predicted\"],\n",
    "    output_df[\"Label\"],\n",
    "    rownames=[\"Actual\"],\n",
    "    colnames=[\"Predicted\"],\n",
    "    margins=True,\n",
    ")\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrated an end to end cycle of data exploration, data processing using SageMaker processing, model development using an XGBoost Bring Your Own Container which we pushed to ECR, model training and offline inference using Batch Transform. Finally we logged our training metadata using SageMaker Experiments.\n",
    "\n",
    "You can use this notebook to experiment with end to end data science experimentation using SageMaker Studio. \n",
    "\n",
    "\n",
    "Remember to delete your datasets in the Amazon S3 bucket you used for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
