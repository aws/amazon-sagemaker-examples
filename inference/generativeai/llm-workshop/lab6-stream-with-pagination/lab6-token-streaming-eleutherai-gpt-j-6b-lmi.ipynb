{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872c9a95-4bed-40b5-a76a-2cf5982c1153",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Serve and stream tokens from EleutherAI's `gpt-j-6b` hosted on Amazon SageMaker using LMI (Large Model Inference) DJL-based container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66ada4-d677-4900-a01c-0524090a1ce4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f4e2d-9d88-4038-9129-406edf604197",
   "metadata": {},
   "source": [
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "This notebook focuses on deploying the [`EleutherAI/gpt-j-6b`](https://huggingface.co/EleutherAI/gpt-j-6b) HuggingFace model to an Amazon SageMaker endpoint for a text generation task. In this example, you will use the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) Docker image (Amazon Sagemaker Deep Learning Container - DLC) as inference image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/).\n",
    "\n",
    "Once the model has been deployed, you will submit text generation requests and get a streamed response in return using Amazon SageMaker Runtime's native response streaming capability.\n",
    "\n",
    "**Notice:** The model artifacts (checkpoints, configuration, etc.) are not downloaded from the HuggingFace Hub but from an Amazon S3 bucket managed by AWS.\n",
    "\n",
    "In this notebook, we make an extensive use of the higher-level abstractions provided by the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to which we delegate the management of as many resources and configuration as we can, hence demonstrating that the deployment of LLMs to Amazon SageMaker can be performed with great simplicity and minimal amount of code.\n",
    "\n",
    "You will successively deploy the `EleutherAI/gpt-j-6b` model twice using the HuggingFace Accelerate engine on a `ml.g5.2xlarge` GPU instance (1 device with 24 GiB of device memory):\n",
    "* Once without writing any custom server-side Python handler script and therefore leveraging the fact that the default Python handlers of the LMI DLC natively support streaming for the HuggingFace Accelerate engine (among others).\n",
    "* Once with a custom server-side Python handler script. \n",
    "\n",
    "Notice that when using the default handlers, streaming cannot be disabled once the endpoint has been deployed with `streaming_enabled` set to `True`, i.e. the endpoint can only be invoked using `sagemaker::InvokeWithStreamingResponse` (and not `sagemaker::InvokeEndpoint`). On the other hand, when implementing a custom handler script, we will be able to choose between streaming our responses or not on a per-request basis.\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.g5.2xlarge` instance type is available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.g5.2xlarge for endpoint usage\" Amazon SageMaker service quota allows you to deploy one Endpoint using this instance type.\n",
    "\n",
    "### Additional resources\n",
    "* [AWS Machine Learning blog - Elevating the generative AI experience: Introducing streaming support in Amazon SageMaker hosting](https://aws.amazon.com/fr/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/)\n",
    "* [Amazon SageMaker docs - Invoke real-time endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-test-endpoints.html#test-invoke-endpoint-with-response-stream)\n",
    "\n",
    "### License agreement\n",
    "* This model and the dataset it has been trained on are both under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).\n",
    "* This notebook is a sample notebook and not intended for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa8856-71c9-4333-b734-bd28b0071e08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Execution environment setup\n",
    "### 1.1. Dependencies installation\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#). Since the distribution must support Amazon SageMaker Runtime streaming feature, the minimal `boto3` (resp. `botocore`) version is `1.28.39` (resp. `1.31.39`).\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html). Since we use the 0.23.0 version of the DJL LMI DLC, the minimal SDK version is 2.173.0.\n",
    "\n",
    "Let's install or upgrade these dependencies using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1edbb8-0f0a-4c5c-bcb3-9d3e2be5222e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pip --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b5726-9934-4f38-8b0d-c0461bde40a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet boto3>=1.28.39 botocore>=1.31.39 sagemaker>=2.173.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986aae2-9293-4ac7-b8ca-32b814502529",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2. Imports & global variables assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697d3bb-040b-452e-a79e-9b17c717cca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9db6a2-2510-4844-b73f-fe050f52436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SM_DEFAULT_EXECUTION_ROLE_ARN = sagemaker.get_execution_role()\n",
    "SM_SESSION = sagemaker.session.Session()\n",
    "SM_ARTIFACT_BUCKET_NAME = \"sagemaker-example-files-prod-us-west-2\"\n",
    "\n",
    "REGION_NAME = SM_SESSION._region_name\n",
    "S3_CLIENT = boto3.client(\"s3\", region_name=REGION_NAME)\n",
    "SAGEMAKER_RUNTIME_CLIENT = boto3.client(\"sagemaker-runtime\", region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b34d1c-cf7a-4f7e-8091-215c52c3aba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference code local storage\n",
    "SOURCE_DIR = Path.cwd() / \"code\"\n",
    "SOURCE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model related constants\n",
    "PROMPT_TEMPLATE = \"{prompt}\"  # Other LLMs may be prompted using a different pattern, for example, lmsys/vicuna-7b-v1.3 prompt pattern would be \"USER: {prompt}\\nAssistant:\"\n",
    "MODEL_DATA_URL = f\"s3://{SM_ARTIFACT_BUCKET_NAME}/models/gpt-j-6b-model\"\n",
    "\n",
    "# Other global constants\n",
    "DJL_VERSION = \"0.23.0\"  # requires sagemaker>=2.173.0\n",
    "INSTANCE_TYPE = \"ml.g5.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0df51-4444-4869-b597-927b20fd97d7",
   "metadata": {},
   "source": [
    "### 1.3. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057583f-ae9b-40aa-b18b-8fd30e4269c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StreamScanner:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream.\n",
    "\n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "\n",
    "    While usually each PayloadPart event from the event stream will contain a byte array\n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "\n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read\n",
    "    position to ensure that previous bytes are not exposed again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def write(self, content: bytes) -> None:\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    def readlines(self) -> bytes:\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b\"\\n\":\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e5eec-1e13-4b43-b7d0-a2c885c0b335",
   "metadata": {},
   "source": [
    "## 2. Deployment to a SageMaker Endpoint using a SageMaker LMI Docker image and the HuggingFace Accelerate engine\n",
    "Start up of LLM inference containers can last longer than for smaller models mainly because of longer model downloading and loading times. Timeout values need to be increased accordingly from their default values. Each endpoint deployment takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b29b72-24cb-41dd-a773-c7a472d39efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ARTIFACTS_DOWNLOAD_TIMEOUT_IN_SECS = 12 * 60\n",
    "CONTAINER_STARTUP_TIMEOUT_IN_SECS = 12 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7509050-876c-41df-972e-51917df18c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTAINER_STARTUP_CONFIGURATION = {\n",
    "    \"model_data_download_timeout\": MODEL_ARTIFACTS_DOWNLOAD_TIMEOUT_IN_SECS,\n",
    "    \"container_startup_health_check_timeout\": CONTAINER_STARTUP_TIMEOUT_IN_SECS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b7179-6b6c-459c-9d5a-d40c1275dda5",
   "metadata": {},
   "source": [
    "### 2.1. Inference using the default HuggingFace Accelerate handler\n",
    "In this section, you deploy the `EleutherAI/gpt-j-6b` model to a SageMaker endpoint consisting of a single `ml.g5.2xlarge` instance. The inference engine used by the DJL Serving stack is HuggingFace Accelerate. Chosen precision is FP16 (native precision). and using the HuggingFace Accelerate handler as inference engine (referred as the `Python` engine in the [DJL Serving general settings](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)).\n",
    "\n",
    "To each engine corresponds a dedicated `sagemaker.model.Model` class. In the present case, you will use the `sagemaker.djl_inference.HuggingFaceAccelerateModel` class. The model server configuration is generated by the `HuggingFaceAccelerateModel` class from the arguments we pass to its constructor and from an optional and already-existing `serving.properties` file.\n",
    "\n",
    "Since the HuggingFace Accelerate [default handler script](https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/huggingface.py) natively supports response streaming, we do not implement any custom server-side handler script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3187f-edb7-4047-b555-87b7ca887cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference import HuggingFaceAccelerateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22a9bb-4a35-4730-bb35-201a5a1976af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOURCE_DIR_ACCELERATE = Path(\"code-accelerate\")\n",
    "SOURCE_DIR_ACCELERATE.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f368bc-389b-4912-a2dd-c8c03fde4ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/serving.properties\n",
    "option.enable_streaming = huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ce82f-787e-4017-b27a-91665afa7c44",
   "metadata": {},
   "source": [
    "Notice that the HuggingFace Accelerate (and DeepSpeed) default DJL handler support two streaming modes:\n",
    "* A legacy streaming mode (`option.enable_streaming=true`)\n",
    "* An recommended alternative based on [HuggingFace's streamers](https://huggingface.co/docs/transformers/v4.31.0/en/internal/generation_utils#transformers.TextStreamer) (`option.enable_streaming=huggingface`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f39dbe-53b7-4bd1-a1b3-103e307564af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/requirements.txt\n",
    "protobuf<3.20\n",
    "transformers>=4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102aa8e-a5fc-4850-bd24-8d1a2baf10a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_model = HuggingFaceAccelerateModel(\n",
    "    djl_version=DJL_VERSION,\n",
    "    model_id=MODEL_DATA_URL,\n",
    "    source_dir=SOURCE_DIR_ACCELERATE.as_posix(),\n",
    "    role=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    task=\"text-generation\",\n",
    "    # HF Accelerate configuration arguments\n",
    "    dtype=\"fp16\",\n",
    "    number_of_partitions=1,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7c7bd-fbf9-49d8-83be-75828c51f5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_predictor = hf_accelerate_model.deploy(\n",
    "    instance_type=INSTANCE_TYPE, initial_instance_count=1, **CONTAINER_STARTUP_CONFIGURATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856b4a4-73f0-4b64-a185-262821b913a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Notices:***\n",
    "* Requests with response streaming currently do not support multiple input prompts\n",
    "* The `Predictor` object returned by the `deploy` method is currently not capable of invoking the endpoint it is tied to with response streaming. We therefore use the lower-level `boto3` client to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186f8a5-df3e-492f-b5ce-53f5cd71dc94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = hf_accelerate_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9a4ab-d60e-415c-b812-12d958d56b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompts = [\"What is Amazon? Be concise.\"]\n",
    "request_content_type = \"application/json\"\n",
    "response_content_type = \"application/jsonlines\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputs\": [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts],\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 1.1,\n",
    "        \"top_p\": 0.85,\n",
    "    },\n",
    "}\n",
    "\n",
    "response = SAGEMAKER_RUNTIME_CLIENT.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(request_body),\n",
    "    ContentType=request_content_type,\n",
    "    Accept=response_content_type,\n",
    ")\n",
    "\n",
    "event_stream = response[\"Body\"]\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    scanner.write(event[\"PayloadPart\"][\"Bytes\"])\n",
    "    for line in scanner.readlines():\n",
    "        deserialized_line = json.loads(line)\n",
    "        print(deserialized_line.get(\"outputs\")[0], end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139fa422-1336-4c5d-8200-2b8f4f459556",
   "metadata": {},
   "source": [
    "Now let's delete the endpoint to redeploy the model with a custom server-side handler script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621ac53-3e95-4033-81e6-984a8d22c50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up\n",
    "hf_accelerate_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hf_accelerate_model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d7a2a-5554-4edb-970a-43070ac11369",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Inference using a custom server-side handler script\n",
    "In this section, you will redeploy the same model but first, you will add a custom Python server-side handler script to the code artifacts that are to be deployed to the container (gathered in the `source_dir` / `SOURCE_DIR_ACCELERATE` directory).\n",
    "\n",
    "The custom handler script below allows to enable or disable streaming on a per-request basis. Default behavior is set using the `option.enable_streaming` field to `true` in the model server's configuration file `serving.properties`.\n",
    "\n",
    "The custom handler script allows to showcase the main differences when enabling streaming in the LMI container compared to sending the full generated sequences once:\n",
    "* We use a streamer object which implements the interface defined by [`transformers.TextStreamer`](https://huggingface.co/docs/transformers/v4.31.0/en/internal/generation_utils#transformers.TextStreamer) like `djl_python.streaming_utils.HFStreamer` or `transformers.generation.streamers.TextIteratorStreamer`. The streamer object uses the model's tokenizer to decode the generated token Ids before pushing them to the streamer's internal queue. The `transformers.generation.streamers.TextIteratorStreamer` streamer can be used as an alternative.\n",
    "* Instead of adding the result to the `djl_python.Output` object, we attach the streamer object using its `add_stream_content`.\n",
    "* Generation is executed in a background thread. The streamer object is passed to the [`generate` method](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generation#transformers.GenerationMixin.generate) together with the `GenerationConfig`. The generation logic then uses the streamer to post tokens to the streamer's queue. On the other side, since the `Output` object has access to the streamer object, it is able to retrieve the tokens from its queue to dispatch them to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6434a3-1fe3-4687-8068-9d625a6b57fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/serving.properties\n",
    "option.enable_streaming = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2c97b-ee41-463a-a2dd-d5f4cc320be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/handler.py\n",
    "from threading import Thread\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    GenerationConfig,\n",
    "    TextGenerationPipeline,\n",
    ")\n",
    "from transformers.generation.streamers import BaseStreamer, TextIteratorStreamer\n",
    "\n",
    "\n",
    "def get_torch_dtype_from_str(dtype: str) -> torch.dtype:\n",
    "    if dtype == \"fp32\":\n",
    "        return torch.float32\n",
    "    if dtype == \"fp16\":\n",
    "        return torch.float16\n",
    "    if dtype == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if dtype == \"int8\":\n",
    "        return torch.int8\n",
    "    if dtype is None:\n",
    "        return None\n",
    "    raise ValueError(f\"Data type cannot be parsed as valid Torch data type: {dtype}\")\n",
    "\n",
    "\n",
    "def start_generation_thread(\n",
    "    model: TextGenerationPipeline,\n",
    "    streamer: BaseStreamer,\n",
    "    input_sequences: List[str],\n",
    "    generation_config: GenerationConfig,\n",
    ") -> None:\n",
    "    def run_generation_with_streaming(\n",
    "        model: TextGenerationPipeline,\n",
    "        streamer: BaseStreamer,\n",
    "        input_sequences: List[str],\n",
    "        generation_config: GenerationConfig,\n",
    "    ) -> None:\n",
    "        try:\n",
    "            model.generate(input_sequences, streamer=streamer, generation_config=generation_config)\n",
    "        except Exception as e:\n",
    "            streamer.put_text(str(e))\n",
    "        finally:\n",
    "            streamer.end()\n",
    "\n",
    "    thread = Thread(\n",
    "        target=run_generation_with_streaming,\n",
    "        args=[model, streamer, input_sequences, generation_config],\n",
    "    )\n",
    "    thread.start()\n",
    "\n",
    "\n",
    "class ConfigFactory:\n",
    "    def __init__(self, properties: Dict[str, Any]) -> None:\n",
    "        self._properties = properties\n",
    "        self._dtype = get_torch_dtype_from_str(properties.get(\"dtype\", \"fp16\"))\n",
    "\n",
    "    def build_model_loading_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"low_cpu_mem_usage\": (\n",
    "                self._properties.get(\"low_cpu_mem_usage\", \"true\").lower() == \"true\"\n",
    "            ),\n",
    "            \"trust_remote_code\": (\n",
    "                self._properties.get(\"trust_remote_code\", \"false\").lower() == \"true\"\n",
    "            ),\n",
    "            \"local_files_only\": (\n",
    "                self._properties.get(\"local_files_only\", \"false\").lower() == \"true\"\n",
    "            ),\n",
    "            \"torch_dtype\": self._dtype,\n",
    "            \"revision\": self._properties.get(\"revision\", \"main\"),\n",
    "            \"device_map\": self._properties.get(\"device_map\", \"auto\"),\n",
    "        }\n",
    "\n",
    "    def build_tokenizer_loading_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"trust_remote_code\": (\n",
    "                self._properties.get(\"trust_remote_code\", \"false\").lower() == \"true\"\n",
    "            ),\n",
    "            \"revision\": self._properties.get(\"revision\", \"main\"),\n",
    "            \"legacy\": (\n",
    "                self._properties.get(\"tokenizer_legacy_behavior\", \"false\").lower() == \"true\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def build_tokenizer_encoding_config(self) -> Dict[str, Any]:\n",
    "        return {\"padding\": True, \"return_tensors\": \"pt\"}\n",
    "\n",
    "    def build_tokenizer_decoding_config(self) -> Dict[str, Any]:\n",
    "        return {\"skip_special_tokens\": self._properties.get(\"skip_special_tokens\", True)}\n",
    "\n",
    "\n",
    "class HuggingFaceAccelerateInferenceService:\n",
    "    def __init__(self) -> None:\n",
    "        self.model_location = None\n",
    "        self._config_factory = None\n",
    "        self._tokenizer = None\n",
    "        self._model = None\n",
    "        self.initialized = False\n",
    "        self.default_is_streaming_enabled = None\n",
    "        self._default_generation_parameters = {}\n",
    "        self.device = None\n",
    "\n",
    "    def _load_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        tokenizer_loading_config = self._config_factory.build_tokenizer_loading_config()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_location, **tokenizer_loading_config)\n",
    "        if not tokenizer.pad_token:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "            self._default_generation_parameters.update({\"pad_token_id\": tokenizer.pad_token_id})\n",
    "        return tokenizer\n",
    "\n",
    "    def initialize(self, properties: Dict[str, str]) -> None:\n",
    "        self._config_factory = ConfigFactory(properties=properties)\n",
    "        # model_id can point to huggingface model_id or local directory.\n",
    "        # If option.model_id points to a s3 bucket, the DJL model server downloads it and set option.model_id to the local download directory.\n",
    "        # If option.model_id is not available, it is assumed model artifacts are in the option.model_dir (by default set to /opt/ml/model, which is also the cwd)\n",
    "        self.model_location = properties.get(\"model_id\") or properties.get(\"model_dir\")\n",
    "        self.default_is_streaming_enabled = (\n",
    "            properties.get(\"enable_streaming\", \"true\").lower() == \"true\"\n",
    "        )\n",
    "        device_id = properties.get(\"device_id\", None)\n",
    "        self.device = f\"cuda:{device_id}\" if device_id is not None else \"cuda:0\"\n",
    "\n",
    "        self._tokenizer = self._load_tokenizer()\n",
    "        model_loading_config = self._config_factory.build_model_loading_config()\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_location, **model_loading_config\n",
    "        ).to(self.device)\n",
    "        self.initialized = True\n",
    "\n",
    "    def handle_generation_request(self, inputs: Input) -> Output:\n",
    "        request_payload = inputs.get_as_json()\n",
    "        input_sequences = request_payload[\"inputs\"]\n",
    "        request_parameters = request_payload[\"parameters\"]\n",
    "        is_streaming_enabled = request_parameters.pop(\n",
    "            \"stream_response\", self.default_is_streaming_enabled\n",
    "        )\n",
    "        generation_parameters = self._default_generation_parameters.copy()\n",
    "        generation_parameters.update(request_parameters)\n",
    "        generation_config = GenerationConfig(**generation_parameters)\n",
    "        outputs = Output()\n",
    "        encoding_config = self._config_factory.build_tokenizer_encoding_config()\n",
    "        decoding_config = self._config_factory.build_tokenizer_decoding_config()\n",
    "        inputs = self._tokenizer(input_sequences, **encoding_config).input_ids.to(self.device)\n",
    "        if is_streaming_enabled:\n",
    "            assert (\n",
    "                len(input_sequences) == 1\n",
    "            ), \"Only one sequence can be processed at a time when stream_response=True\"\n",
    "            streamer = TextIteratorStreamer(\n",
    "                tokenizer=self._tokenizer, skip_prompt=True, **decoding_config\n",
    "            )\n",
    "            start_generation_thread(\n",
    "                model=self._model,\n",
    "                streamer=streamer,\n",
    "                input_sequences=inputs,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "            outputs.add_stream_content(streamer)\n",
    "        else:\n",
    "            output_ids = self._model.generate(inputs, generation_config=generation_config)\n",
    "            output_sequences = self._tokenizer.batch_decode(output_ids, **decoding_config)\n",
    "            output_sequences = [\n",
    "                output_seq[len(input_seq) :]\n",
    "                for input_seq, output_seq in zip(input_sequences, output_sequences)\n",
    "            ]\n",
    "            outputs.add_property(\"content-type\", \"application/json\")\n",
    "            outputs.add(output_sequences)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "_service = HuggingFaceAccelerateInferenceService()\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> Optional[Output]:\n",
    "    if not _service.initialized:\n",
    "        print(\"Initializing inference service\")\n",
    "        _service.initialize(properties=inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    return _service.handle_generation_request(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231a588-c50c-4ccf-9a9e-2b99a89f8426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_model = HuggingFaceAccelerateModel(\n",
    "    djl_version=DJL_VERSION,\n",
    "    model_id=MODEL_DATA_URL,\n",
    "    source_dir=SOURCE_DIR_ACCELERATE.as_posix(),\n",
    "    entry_point=\"handler.py\",\n",
    "    role=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    task=\"text-generation\",\n",
    "    # HF Accelerate configuration arguments\n",
    "    dtype=\"fp16\",\n",
    "    device_id=0,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa47093-bb8a-4d45-adad-0be16744f5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_predictor = hf_accelerate_model.deploy(\n",
    "    instance_type=INSTANCE_TYPE, initial_instance_count=1, **CONTAINER_STARTUP_CONFIGURATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9e333-8639-4808-866d-45619ba2289b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = hf_accelerate_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171bfb9-5e88-4938-b638-50e047648cd3",
   "metadata": {},
   "source": [
    "The custom handler script allow to either stream the response tokens (default behavior), i.e. invoke the endpoint using `sagemaker:InvokeEndpointWithResponseStreaming` or to disable streaming at the request level, i.e. invoke the endpoint using `sagemaker:InvokeEndpoint`. Let's first invoke the endpoint with the streaming feature enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4d9ee-4ed0-4a81-990a-82928aa6f500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompts = [\"What is Amazon? Be concise.\"]\n",
    "request_content_type = \"application/json\"\n",
    "response_content_type = \"application/jsonlines\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputs\": [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts],\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 1.1,\n",
    "        \"top_p\": 0.85,\n",
    "    },\n",
    "}\n",
    "\n",
    "response = SAGEMAKER_RUNTIME_CLIENT.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(request_body),\n",
    "    ContentType=request_content_type,\n",
    "    Accept=response_content_type,\n",
    ")\n",
    "\n",
    "event_stream = response[\"Body\"]\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    scanner.write(event[\"PayloadPart\"][\"Bytes\"])\n",
    "    for line in scanner.readlines():\n",
    "        deserialized_line = json.loads(line)\n",
    "        print(deserialized_line.get(\"outputs\"), end=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d20228-233e-4922-b16d-e3c28a5eac1d",
   "metadata": {},
   "source": [
    "Now let's add a `stream_response: False` entry to our request parameters to allow our endpoint to be invoked using the `sagemaker:InvokeEndpoint` API call and let's use the `Predictor` object returned by `Model.deploy` to perform this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6af75-21ae-4631-a383-3114fbc0a564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompts = [\"What is Amazon? Be concise.\"]\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1.1,\n",
    "    \"top_p\": 0.85,\n",
    "    \"stream_response\": False,\n",
    "}\n",
    "\n",
    "hf_accelerate_predictor.predict(\n",
    "    data={\n",
    "        \"inputs\": [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts],\n",
    "        \"parameters\": generation_config,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91643a60-d85a-4f71-8990-64cca6d20dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up\n",
    "hf_accelerate_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hf_accelerate_model.delete_model()\n",
    "shutil.rmtree(SOURCE_DIR_ACCELERATE.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd11c6-a056-42ed-8b78-6d6f015bceda",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-eleutherai-gpt-j-6b-lmi.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
