resources:
  instance_count: 4
  instance_type: ml.trn1.32xlarge 
  volume_size: 200
git:
  repo_url: "https://github.com/aws-neuron/neuronx-distributed.git"
  branch: main
  commit: 3aa65c661c45f133a99052746ed70a0121d0e695
pre_script: 
  - pip3 install --upgrade pip
  - SCRIPT_DIR=$GIT_CLONE_DIR/examples/training/llama/tp_zero1_llama_hf_pretrain
  - cp $GIT_CLONE_DIR/examples/training/llama/requirements.txt $SCRIPT_DIR
  - cp $GIT_CLONE_DIR/examples/training/llama/*.py $SCRIPT_DIR
  - cd $SCRIPT_DIR
  - pip3 install -r requirements.txt
  - LOGS_DIR=$LOG_ROOT/$PET_NODE_RANK
  - 'if [ -d $LOGS_DIR ]; then rm -rf $LOGS_DIR; fi'
  - mkdir -p $LOGS_DIR 
  - OUTPUT_LOG=$LOGS_DIR/compile.log
  - CACHE_DIR=$CACHE_ROOT/$PET_NODE_RANK
  - 'if [ -d $CACHE_DIR ]; then rm -rf $CACHE_DIR; fi'
  - mkdir -p $CACHE_DIR
  - TMP_CACHE_DIR=$SM_OUTPUT_DATA_DIR/cache
  - mkdir -p $CKPT_ROOT
  - DATA_PATH="$DATA_ROOT/examples_datasets/wikicorpus_llama3_tokenized_8k"
  - DISTRIBUTED_ARGS="--nproc_per_node $PET_NPROC_PER_NODE --nnodes $PET_NNODES --node_rank $PET_NODE_RANK --master_addr $PET_MASTER_ADDR --master_port $PET_MASTER_PORT"
  - TP_DEGREE=32
  - GBS=1024
  - MBS=1
  - TOTAL_STEPS=10000
  - WARMUP_STEPS=100
  - LR=1.5e-4
  - SEQ_LEN=8192
  - DP=$(($PET_NPROC_PER_NODE * $PET_NNODES / $TP_DEGREE))
  - ACC_STEPS=$(($GBS / $MBS / $DP)) 
  - STEPS_THIS_RUN=2
  - LLAMA_ARGS="--model_path $SCRIPT_DIR/8B_config_llama3.1
    --data_dir $DATA_PATH 
    --tensor_parallel_size $TP_DEGREE 
    --batch_size $MBS 
    --steps_this_run $STEPS_THIS_RUN
    --max_steps $TOTAL_STEPS 
    --warmup_steps $WARMUP_STEPS 
    --lr $LR 
    --grad_accum_usteps $ACC_STEPS 
    --seq_len $SEQ_LEN 
    --sequence_parallel_enabled 
    --selective_checkpoint_enabled 
    --logging_interval 10
    --qkv_linear
    --kv_replicator 4
    --use_flash_attention 1
    --use_gpu_compatible_precision 1
    --use_mix_precision
    --use_zero_1"
  - echo DISTRIBUTED_ARGS=$DISTRIBUTED_ARGS
  - echo "#!/bin/bash" >  compile.sh
  - echo "export TPU_NUM_DEVICES=$PET_NPROC_PER_NODE" >> ./compile.sh
  - echo "export TPU_CHIPS_PER_HOST_BOUNDS=$PET_NPROC_PER_NODE" >> ./compile.sh
  - echo export NEURON_CC_FLAGS=\"--model-type transformer --distribution-strategy=llm-training --cache_dir=$TMP_CACHE_DIR\" >> ./compile.sh
  - echo "torchrun  $DISTRIBUTED_ARGS tp_zero1_llama_hf_pretrain.py $LLAMA_ARGS --checkpoint_dir $CKPT_ROOT 2>&1 | tee $OUTPUT_LOG"  >> compile.sh 
  - chmod u+x ./compile.sh
post_script:
  - cp -r $TMP_CACHE_DIR/* $CACHE_DIR
train:
  distribution: "torch"
  env:
    - name: HOME
      value: $SM_OUTPUT_DATA_DIR
    - name: LOG_ROOT
      value: $SM_CHANNEL_EFS/home/$RELEASE_NAME/logs
    - name: DATA_ROOT
      value: $SM_CHANNEL_FSX/home/$RELEASE_NAME
    - name: CACHE_ROOT
      value: $SM_CHANNEL_EFS/home/$RELEASE_NAME/cache
    - name: CKPT_ROOT
      value: "$SM_OUTPUT_DATA_DIR/checkpoints"
    - name: CCOM_SOCKET_IFNAME
      value: "eth0"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_EFA_FORK_SAFE
      value: "1"
    - name:  NEURON_FUSE_SOFTMAX
      value: "1"
    - name: NEURON_RT_STOCHASTIC_ROUNDING_EN
      value: "1"
    - name: NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS
      value: "3"
    - name: MALLOC_ARENA_MAX
      value: "64"
  command:
    -  neuron_parallel_compile
  args:
    - './compile.sh'
    - '2>&1 | tee $LOGS_DIR/neuron_parallel_compile.log'
