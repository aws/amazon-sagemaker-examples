{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Batch Transform custom TensorFlow inference.py (CSV & TFRecord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This notebook trains a simple classifier on the Iris dataset. Training is completed locally on the machine where this notebook is executed. A custom `inference.py` script for `CSV` and `TFRecord` is used for hosting our model in a Batch Transform Job. The TFRecord data is generated from the CSV data.\n",
    "\n",
    "**Table of Contents:**\n",
    "* [Prerequisites](#Prerequisites)\n",
    "* [Training the Network Locally](#training-the-network-locally)\n",
    "* [Set the model up for hosting ](#hosting)\n",
    "* [Write a custom inference.py script](#inference-script)\n",
    "* [ Create Batch Transform Job](#transform)\n",
    "\n",
    "\n",
    "## Prerequisites <a class=\"anchor\" id=\"prequisites\"></a>\n",
    "### Packages and Permissions\n",
    "\n",
    "Here, we set up the specific TensorFlow version that is used to train locally. We specify the same version when we host our model. The SageMaker SDK uses the SageMaker default S3 bucket when needed. If the `get_execution_role` does not return a role with the appropriate permissions, you'll need to specify an IAM role ARN that does. Please make sure the `SageMakerFullAccess` policy is attached to the execution role you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.1\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from datetime import datetime\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we use a very simple network architecture with three densely-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_mlp(metrics):\n",
    "    ### Setup loss and output node activation\n",
    "    output_activation = \"softmax\"\n",
    "    loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "    input = Input(shape=(4,), name=\"input\")\n",
    "\n",
    "    x = Dense(\n",
    "        units=10,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_layer1\",\n",
    "    )(input)\n",
    "\n",
    "    x = Dense(\n",
    "        units=20,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_layer2\",\n",
    "    )(x)\n",
    "\n",
    "    x = Dense(\n",
    "        units=10,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        name=\"dense_layer3\",\n",
    "    )(x)\n",
    "\n",
    "    output = Dense(units=3, activation=output_activation)(x)\n",
    "\n",
    "    ### Compile the model\n",
    "    model = tf.keras.Model(input, output)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the pre-processed Iris training and test data stored in the `sagemaker-sample-files` public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Iris test and train data sets from S3\n",
    "SOURCE_DATA_BUCKET = \"sagemaker-sample-files\"\n",
    "SOURCE_DATA_PREFIX = \"datasets/tabular/iris\"\n",
    "sm_session.download_data(\".\", bucket=SOURCE_DATA_BUCKET, key_prefix=SOURCE_DATA_PREFIX)\n",
    "\n",
    "# Load the training and test data from .csv to a Pandas data frame.\n",
    "train_df = pd.read_csv(\n",
    "    \"iris_train.csv\",\n",
    "    header=0,\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "test_df = pd.read_csv(\n",
    "    \"iris_test.csv\",\n",
    "    header=0,\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "\n",
    "# Pop the record labels into N x 1 Numpy arrays\n",
    "train_labels = np.array(train_df.pop(\"class\"))\n",
    "test_labels = np.array(test_df.pop(\"class\"))\n",
    "\n",
    "# Save the remaining features as Numpy arrays\n",
    "train_np = np.array(train_df)\n",
    "test_np = np.array(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network Locally <a class=\"anchor\" id=\"training-the-network-locally\"></a> \n",
    "Here, we train the network using the TensorFlow `.fit` method. This should only take a few seconds because the model is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"auto\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Instantiate classifier\n",
    "classifier = iris_mlp(metrics=[\"accuracy\", \"binary_accuracy\"])\n",
    "\n",
    "# Fit classifier\n",
    "history = classifier.fit(\n",
    "    x=train_np,\n",
    "    y=train_labels,\n",
    "    validation_data=(test_np, test_labels),\n",
    "    callbacks=[EARLY_STOPPING],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model up for hosting <a class=\"anchor\" id=\"hosting\"></a>\n",
    "\n",
    "### Export the model from TensorFlow\n",
    "The SageMaker TensorFlow Serving container expects the model artifacts are organized in the following format:\n",
    "\n",
    "```\n",
    "1\n",
    "├── keras_metadata.pb\n",
    "├── saved_model.pb\n",
    "└── variables\n",
    "    ├── variables.data-00000-of-00001\n",
    "    └── variables.index\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"1\")\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SageMaker session to upload the model on to the default SageMaker S3 bucket. We use the ``sagemaker.Session.upload_data`` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=\"model\")\n",
    "s3_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model input tensor shape\n",
    "We use the `saved_model_cli` to view the model's input tensors which help us in building our custom inference.py script.\n",
    "\n",
    "As we can see our model expects input in the shape of (-1, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir {\"1\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Example Batch Data\n",
    "Below we view the sample CSV data that will be used as input to our Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head Data/batch-iris-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_csv_data = \"s3://{}/datasets/batch-iris-data.csv\".format(bucket_name)\n",
    "s3_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp Data/batch-iris-data.csv $s3_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Example Batch Data\n",
    "We use the CSV data to generate TFRecord data that will also be used for inference.\n",
    "The CSV and TFRecord use identical data, they just express it in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"Data/batch-iris-data.csv\", header=None).values\n",
    "\n",
    "with tf.io.TFRecordWriter(\"Data/batch-iris-data.tfrecords\") as writer:\n",
    "    for row in csv:\n",
    "        features = row[:]\n",
    "        example = tf.train.Example()\n",
    "        example.features.feature[\"features\"].float_list.value.extend(features)\n",
    "\n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"Data/batch-iris-data.tfrecords\")\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_tf_record_data = \"s3://{}/datasets/batch-iris-data.tfrecords\".format(bucket_name)\n",
    "s3_tf_record_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload TFRecord input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp Data/batch-iris-data.tfrecords $s3_tf_record_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a custom inference.py script <a class=\"anchor\" id=\"inference-script\"></a>\n",
    "\n",
    "Our model accepts a tensor of (-1, 4). Hence, we create an input handler for each file type (text/csv, application/x-tfrecord).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.system(\"pip install numpy tensorflow crcmod\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "import crcmod\n",
    "\n",
    "\n",
    "def _masked_crc32c(value):\n",
    "    crc = crcmod.predefined.mkPredefinedCrcFun(\"crc-32c\")(value)\n",
    "    return (((crc >> 15) | (crc << 17)) + 0xA282EAD8) & 0xFFFFFFFF\n",
    "\n",
    "\n",
    "def read_tfrecords(tfrecords):\n",
    "    import io\n",
    "    import struct\n",
    "\n",
    "    tfrecords_bytes = io.BytesIO(tfrecords)\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    while True:\n",
    "        length_header = 12\n",
    "        buf = tfrecords_bytes.read(length_header)\n",
    "        if not buf:\n",
    "            # reached end of tfrecord buffer, return examples\n",
    "            return examples\n",
    "\n",
    "        if len(buf) != length_header:\n",
    "            raise ValueError(\"TFrecord is fewer than %d bytes\" % length_header)\n",
    "        length, length_mask = struct.unpack(\"<QI\", buf)\n",
    "        length_mask_actual = _masked_crc32c(buf[:8])\n",
    "        if length_mask_actual != length_mask:\n",
    "            raise ValueError(\"TFRecord does not contain a valid length mask\")\n",
    "\n",
    "        length_data = length + 4\n",
    "        buf = tfrecords_bytes.read(length_data)\n",
    "        if len(buf) != length_data:\n",
    "            raise ValueError(\"TFRecord data payload has fewer bytes than specified in header\")\n",
    "        data, data_mask_expected = struct.unpack(\"<%dsI\" % length, buf)\n",
    "        data_mask_actual = _masked_crc32c(data)\n",
    "        if data_mask_actual != data_mask_expected:\n",
    "            raise ValueError(\"TFRecord has an invalid data crc32c\")\n",
    "\n",
    "        # Deserialize the tf.Example proto\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(data)\n",
    "        example_features = MessageToDict(example)[\"features\"][\"feature\"][\"features\"][\"floatList\"][\n",
    "            \"value\"\n",
    "        ]\n",
    "        # Extract a feature map from the example object\n",
    "        examples.append(example_features)\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def read_csv(csv):\n",
    "    return np.array([[float(j) for j in i.split(\",\")] for i in csv.splitlines()])\n",
    "\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\"Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data stream\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "\n",
    "    if context.request_content_type == \"text/csv\":\n",
    "\n",
    "        payload = data.read().decode(\"utf-8\")\n",
    "        inputs = read_csv(payload)\n",
    "\n",
    "        input = {\"inputs\": inputs.tolist()}\n",
    "\n",
    "        return json.dumps(input)\n",
    "\n",
    "    if context.request_content_type == \"application/x-tfrecord\":\n",
    "\n",
    "        payload = data.read()\n",
    "        examples = read_tfrecords(payload)\n",
    "\n",
    "        input = {\"inputs\": examples}\n",
    "\n",
    "        return json.dumps(input)\n",
    "\n",
    "    raise ValueError(\n",
    "        '{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "            context.request_content_type or \"unknown\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode(\"utf-8\"))\n",
    "\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the input_handler for TFRecord and CSV return the same output\n",
    "Due to the fact that the TFRecord and CSV input data are the same, the input_handler should have identical output for each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference\n",
    "\n",
    "\n",
    "class Context:\n",
    "    def __init__(self, request_content_type):\n",
    "        self.request_content_type = request_content_type\n",
    "\n",
    "\n",
    "tfrecord_bytes = open(\"Data/batch-iris-data.tfrecords\", \"rb\")\n",
    "tfrecord_input = inference.input_handler(tfrecord_bytes, Context(\"application/x-tfrecord\"))\n",
    "\n",
    "csv_file_bytes = open(\"Data/batch-iris-data.csv\", \"rb\")\n",
    "csv_input = inference.input_handler(csv_file_bytes, Context(\"text/csv\"))\n",
    "\n",
    "assert csv_input == tfrecord_input, \"CSV and TFRecord output do not match!\"\n",
    "print(\"CSV and TFRecord output match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Transform Job <a class=\"anchor\" id=\"transform\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SageMaker TensorFlow Model\n",
    "First we create a `TensorFlowModel` which specifies the custom `inference.py`, TensorFlow version and points to our model tar ball in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "tensorflow_serving_model_batch = TensorFlowModel(\n",
    "    model_data=f\"s3://{bucket_name}/model/model.tar.gz\",\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    framework_version=\"2.3.1\",\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Input Transform Job \n",
    "Create the Transform Job by specifying the S3 CSV input data location and `content_type` as `text/csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d-%H-%m-%S\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path_batch = \"s3://{}/output/batch_iris/\".format(bucket_name)\n",
    "output_data_path = output_data_path_batch\n",
    "batch_instance_count = 1\n",
    "batch_instance_type = \"ml.m5.4xlarge\"\n",
    "concurrency = 5\n",
    "max_payload_in_mb = 1\n",
    "split_type = \"Line\"\n",
    "batch_strategy = \"MultiRecord\"\n",
    "CSV_job_name = \"tensorflow-inference-CSV-{}\".format(date)\n",
    "\n",
    "\n",
    "transformer = tensorflow_serving_model_batch.transformer(\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    strategy=batch_strategy,\n",
    "    output_path=output_data_path,\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=s3_csv_data,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=split_type,\n",
    "    wait=False,\n",
    "    job_name=CSV_job_name,\n",
    ")\n",
    "print(CSV_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Input Transform Job \n",
    "Create the Transform Job by specifying the S3 TFRecord input data location and `content_type` as `application/x-tfrecord`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path_batch = \"s3://{}/output/batch_iris/\".format(bucket_name)\n",
    "\n",
    "output_data_path = output_data_path_batch\n",
    "batch_instance_count = 1\n",
    "batch_instance_type = \"ml.m5.4xlarge\"\n",
    "concurrency = 5\n",
    "max_payload_in_mb = 1\n",
    "split_type = \"TFRecord\"\n",
    "batch_strategy = \"MultiRecord\"\n",
    "TFRecord_job_name = \"tensorflow-inference-TFRecord-{}\".format(date)\n",
    "\n",
    "transformer = tensorflow_serving_model_batch.transformer(\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    strategy=batch_strategy,\n",
    "    output_path=output_data_path,\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=s3_tf_record_data,\n",
    "    content_type=\"application/x-tfrecord\",\n",
    "    split_type=split_type,\n",
    "    wait=False,\n",
    "    job_name=TFRecord_job_name,\n",
    ")\n",
    "print(TFRecord_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Transform Jobs Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "\n",
    "while True:\n",
    "    TFRecord_response = client.describe_transform_job(TransformJobName=TFRecord_job_name)\n",
    "    CSV_response = client.describe_transform_job(TransformJobName=CSV_job_name)\n",
    "\n",
    "    if (CSV_response[\"TransformJobStatus\"] == \"InProgress\") & (\n",
    "        TFRecord_response[\"TransformJobStatus\"] == \"InProgress\"\n",
    "    ):\n",
    "        print(\"CSV and TFRecord Transform Job is inProgress...\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    elif (CSV_response[\"TransformJobStatus\"] == \"Completed\") & (\n",
    "        TFRecord_response[\"TransformJobStatus\"] == \"Completed\"\n",
    "    ):\n",
    "        print(\"CSV and TFRecord Transform Job is Completed\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"CSV Transform Job status: {}\".format(CSV_response[\"TransformJobStatus\"]))\n",
    "        print(\"TFRecord Transform Job status: {}\".format(TFRecord_response[\"TransformJobStatus\"]))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
